{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q learning with Doom üïπÔ∏è\n",
    "In this notebook we'll implement an agent <b>that plays Doom by using a Deep Q learning architecture.</b> <br>\n",
    "Our agent playing Doom:\n",
    "\n",
    "<img src=\"assets/doom.gif\" style=\"max-width: 600px;\" alt=\"Deep Q learning with Doom\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a notebook from [Deep Reinforcement Learning Course with Tensorflow](https://simoninithomas.github.io/Deep_reinforcement_learning_Course/)\n",
    "<img src=\"https://raw.githubusercontent.com/simoninithomas/Deep_reinforcement_learning_Course/master/docs/assets/img/DRLC%20Environments.png\" alt=\"Deep Reinforcement Course\"/>\n",
    "<br>\n",
    "<p>  Deep Reinforcement Learning Course is a free series of articles and videos tutorials üÜï about Deep Reinforcement Learning, where **we'll learn the main algorithms (Q-learning, Deep Q Nets, Dueling Deep Q Nets, Policy Gradients, A2C, Proximal Policy Gradients‚Ä¶), and how to implement them with Tensorflow.**\n",
    "<br><br>\n",
    "    \n",
    "üìúThe articles explain the architectures from the big picture to the mathematical details behind them.\n",
    "<br>\n",
    "üìπ The videos explain how to build the agents with Tensorflow </b></p>\n",
    "<br>\n",
    "This course will give you a **solid foundation for understanding and implementing the future state of the art algorithms**. And, you'll build a strong professional portfolio by creating **agents that learn to play awesome environments**: Doom¬© üëπ, Space invaders üëæ, Outrun, Sonic the Hedgehog¬©, Michael Jackson‚Äôs Moonwalker, agents that will be able to navigate in 3D environments with DeepMindLab (Quake) and able to walk with Mujoco. \n",
    "<br><br>\n",
    "</p> \n",
    "\n",
    "## üìö The complete [Syllabus HERE](https://simoninithomas.github.io/Deep_reinforcement_learning_Course/)\n",
    "\n",
    "\n",
    "## Any questions üë®‚Äçüíª\n",
    "<p> If you have any questions, feel free to ask me: </p>\n",
    "<p> üìß: <a href=\"mailto:hello@simoninithomas.com\">hello@simoninithomas.com</a>  </p>\n",
    "<p> Github: https://github.com/simoninithomas/Deep_reinforcement_learning_Course </p>\n",
    "<p> üåê : https://simoninithomas.github.io/Deep_reinforcement_learning_Course/ </p>\n",
    "<p> Twitter: <a href=\"https://twitter.com/ThomasSimonini\">@ThomasSimonini</a> </p>\n",
    "<p> Don't forget to <b> follow me on <a href=\"https://twitter.com/ThomasSimonini\">twitter</a>, <a href=\"https://github.com/simoninithomas/Deep_reinforcement_learning_Course\">github</a> and <a href=\"https://medium.com/@thomassimonini\">Medium</a> to be alerted of the new articles that I publish </b></p>\n",
    "    \n",
    "## How to help  üôå\n",
    "3 ways:\n",
    "- **Clap our articles and like our videos a lot**:Clapping in Medium means that you really like our articles. And the more claps we have, the more our article is shared Liking our videos help them to be much more visible to the deep learning community.\n",
    "- **Share and speak about our articles and videos**: By sharing our articles and videos you help us to spread the word. \n",
    "- **Improve our notebooks**: if you found a bug or **a better implementation** you can send a pull request.\n",
    "<br>\n",
    "\n",
    "## Important note ü§î\n",
    "<b> You can run it on your computer but it's better to run it on GPU based services</b>, personally I use Microsoft Azure and their Deep Learning Virtual Machine (they offer 170$)\n",
    "https://azuremarketplace.microsoft.com/en-us/marketplace/apps/microsoft-ads.dsvm-deep-learning\n",
    "<br>\n",
    "‚ö†Ô∏è I don't have any business relations with them. I just loved their excellent customer service.\n",
    "\n",
    "If you have some troubles to use Microsoft Azure follow the explainations of this excellent article here (without last the part fast.ai): https://medium.com/@manikantayadunanda/setting-up-deeplearning-machine-and-fast-ai-on-azure-a22eb6bd6429"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites üèóÔ∏è\n",
    "Before diving on the notebook **you need to understand**:\n",
    "- The foundations of Reinforcement learning (MC, TD, Rewards hypothesis...) [Article](https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419)\n",
    "- Q-learning [Article](https://medium.freecodecamp.org/diving-deeper-into-reinforcement-learning-with-q-learning-c18d0db58efe)\n",
    "- Deep Q-Learning [Article](https://medium.freecodecamp.org/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8)\n",
    "- In the [video version](https://www.youtube.com/watch?v=gCJyVX98KJ4)  we implemented a Deep Q-learning agent with Tensorflow that learns to play Atari Space Invaders üïπÔ∏èüëæ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T15:14:53.526432Z",
     "start_time": "2021-03-03T15:14:53.517633Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/explore/miniconda3/envs/drl_simonini/lib/python3.7/site-packages/IPython/core/display.py:717: UserWarning: Consider using IPython.display.IFrame instead\n",
      "  warnings.warn(\"Consider using IPython.display.IFrame instead\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/gCJyVX98KJ4?showinfo=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/gCJyVX98KJ4?showinfo=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import the libraries üìö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T15:14:54.544539Z",
     "start_time": "2021-03-03T15:14:53.527651Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf      # Deep Learning library\n",
    "import numpy as np           # Handle matrices\n",
    "from vizdoom import *        # Doom Environment\n",
    "\n",
    "import random                # Handling random number generation\n",
    "import time                  # Handling time calculation\n",
    "from skimage import transform# Help us to preprocess the frames\n",
    "\n",
    "from collections import deque# Ordered collection with ends\n",
    "import matplotlib.pyplot as plt # Display graphs\n",
    "\n",
    "import warnings # This ignore all the warning messages that are normally printed during the training because of skiimage\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create our environment üéÆ\n",
    "- Now that we imported the libraries/dependencies, we will create our environment.\n",
    "- Doom environment takes:\n",
    "    - A `configuration file` that **handle all the options** (size of the frame, possible actions...)\n",
    "    - A `scenario file`: that **generates the correct scenario** (in our case basic **but you're invited to try other scenarios**).\n",
    "- Note: We have 3 possible actions `[[0,0,1], [1,0,0], [0,1,0]]` so we don't need to do one hot encoding (thanks to < a href=\"https://stackoverflow.com/users/2237916/silgon\">silgon</a> for figuring out. \n",
    "\n",
    "### Our environment\n",
    "<img src=\"assets/doom.png\" style=\"max-width:500px;\" alt=\"Doom\"/>\n",
    "                                    \n",
    "- A monster is spawned **randomly somewhere along the opposite wall**. \n",
    "- Player can only go **left/right and shoot**. \n",
    "- 1 hit is enough **to kill the monster**. \n",
    "- Episode finishes when **monster is killed or on timeout (300)**.\n",
    "<br><br>\n",
    "REWARDS:\n",
    "\n",
    "- +101 for killing the monster \n",
    "- -5 for missing \n",
    "- Episode ends after killing the monster or on timeout.\n",
    "- living reward = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T15:14:54.550319Z",
     "start_time": "2021-03-03T15:14:54.545628Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here we create our environment\n",
    "\"\"\"\n",
    "def create_environment():\n",
    "    game = DoomGame()\n",
    "    \n",
    "    # Load the correct configuration\n",
    "    game.load_config(\"basic.cfg\")\n",
    "    \n",
    "    # Load the correct scenario (in our case basic scenario)\n",
    "    game.set_doom_scenario_path(\"basic.wad\")\n",
    "    \n",
    "    game.init()\n",
    "    \n",
    "    # Here our possible actions\n",
    "    left = [1, 0, 0]\n",
    "    right = [0, 1, 0]\n",
    "    shoot = [0, 0, 1]\n",
    "    possible_actions = [left, right, shoot]\n",
    "    \n",
    "    return game, possible_actions\n",
    "       \n",
    "\"\"\"\n",
    "Here we performing random action to test the environment\n",
    "\"\"\"\n",
    "def test_environment():\n",
    "    game = DoomGame()\n",
    "    game.load_config(\"basic.cfg\")\n",
    "    game.set_doom_scenario_path(\"basic.wad\")\n",
    "    game.init()\n",
    "    shoot = [0, 0, 1]\n",
    "    left = [1, 0, 0]\n",
    "    right = [0, 1, 0]\n",
    "    actions = [shoot, left, right]\n",
    "\n",
    "    episodes = 10\n",
    "    for i in range(episodes):\n",
    "        game.new_episode()\n",
    "        while not game.is_episode_finished():\n",
    "            state = game.get_state()\n",
    "            img = state.screen_buffer\n",
    "            misc = state.game_variables\n",
    "            action = random.choice(actions)\n",
    "            print(action)\n",
    "            reward = game.make_action(action)\n",
    "            print (\"\\treward:\", reward)\n",
    "            time.sleep(0.02)\n",
    "        print (\"Result:\", game.get_total_reward())\n",
    "        time.sleep(2)\n",
    "    game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T15:14:55.336496Z",
     "start_time": "2021-03-03T15:14:54.551506Z"
    }
   },
   "outputs": [],
   "source": [
    "game, possible_actions = create_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define the preprocessing functions ‚öôÔ∏è\n",
    "### preprocess_frame\n",
    "Preprocessing is an important step, <b>because we want to reduce the complexity of our states to reduce the computation time needed for training.</b>\n",
    "<br><br>\n",
    "Our steps:\n",
    "- Grayscale each of our frames (because <b> color does not add important information </b>). But this is already done by the config file.\n",
    "- Crop the screen (in our case we remove the roof because it contains no information)\n",
    "- We normalize pixel values\n",
    "- Finally we resize the preprocessed frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T15:14:55.340759Z",
     "start_time": "2021-03-03T15:14:55.337871Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    preprocess_frame:\n",
    "    Take a frame.\n",
    "    Resize it.\n",
    "        __________________\n",
    "        |                 |\n",
    "        |                 |\n",
    "        |                 |\n",
    "        |                 |\n",
    "        |_________________|\n",
    "        \n",
    "        to\n",
    "        _____________\n",
    "        |            |\n",
    "        |            |\n",
    "        |            |\n",
    "        |____________|\n",
    "    Normalize it.\n",
    "    \n",
    "    return preprocessed_frame\n",
    "    \n",
    "    \"\"\"\n",
    "def preprocess_frame(frame):\n",
    "    # Greyscale frame already done in our vizdoom config\n",
    "    # x = np.mean(frame,-1)\n",
    "    \n",
    "    # Crop the screen (remove the roof because it contains no information)\n",
    "    cropped_frame = frame[30:-10,30:-30]\n",
    "    \n",
    "    # Normalize Pixel Values\n",
    "    normalized_frame = cropped_frame/255.0\n",
    "    \n",
    "    # Resize\n",
    "    preprocessed_frame = transform.resize(normalized_frame, [84,84])\n",
    "    \n",
    "    return preprocessed_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stack_frames\n",
    "üëè This part was made possible thanks to help of <a href=\"https://github.com/Miffyli\">Anssi</a><br>\n",
    "\n",
    "As explained in this really <a href=\"https://danieltakeshi.github.io/2016/11/25/frame-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games/\">  good article </a> we stack frames.\n",
    "\n",
    "Stacking frames is really important because it helps us to **give have a sense of motion to our Neural Network.**\n",
    "\n",
    "- First we preprocess frame\n",
    "- Then we append the frame to the deque that automatically **removes the oldest frame**\n",
    "- Finally we **build the stacked state**\n",
    "\n",
    "This is how work stack:\n",
    "- For the first frame, we feed 4 frames\n",
    "- At each timestep, **we add the new frame to deque and then we stack them to form a new stacked frame**\n",
    "- And so on\n",
    "<img src=\"https://raw.githubusercontent.com/simoninithomas/Deep_reinforcement_learning_Course/master/DQN/Space%20Invaders/assets/stack_frames.png\" alt=\"stack\">\n",
    "- If we're done, **we create a new stack with 4 new frames (because we are in a new episode)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T15:14:55.351953Z",
     "start_time": "2021-03-03T15:14:55.341995Z"
    }
   },
   "outputs": [],
   "source": [
    "stack_size = 4 # We stack 4 frames\n",
    "\n",
    "# Initialize deque with zero-images one array for each image\n",
    "stacked_frames  =  deque([np.zeros((84,84), dtype=np.int) for i in range(stack_size)], maxlen=4) \n",
    "\n",
    "def stack_frames(stacked_frames, state, is_new_episode):\n",
    "    # Preprocess frame\n",
    "    frame = preprocess_frame(state)\n",
    "    \n",
    "    if is_new_episode:\n",
    "        # Clear our stacked_frames\n",
    "        stacked_frames = deque([np.zeros((84,84), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "        \n",
    "        # Because we're in a new episode, copy the same frame 4x\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        \n",
    "        # Stack the frames\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "        \n",
    "    else:\n",
    "        # Append frame to deque, automatically removes the oldest frame\n",
    "        stacked_frames.append(frame)\n",
    "\n",
    "        # Build the stacked state (first dimension specifies different frames)\n",
    "        stacked_state = np.stack(stacked_frames, axis=2) \n",
    "    \n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Set up our hyperparameters ‚öóÔ∏è\n",
    "In this part we'll set up our different hyperparameters. But when you implement a Neural Network by yourself you will **not implement hyperparamaters at once but progressively**.\n",
    "\n",
    "- First, you begin by defining the neural networks hyperparameters when you implement the model.\n",
    "- Then, you'll add the training hyperparameters when you implement the training algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T15:14:55.357541Z",
     "start_time": "2021-03-03T15:14:55.352986Z"
    }
   },
   "outputs": [],
   "source": [
    "### MODEL HYPERPARAMETERS\n",
    "state_size = [84,84,4]      # Our input is a stack of 4 frames hence 84x84x4 (Width, height, channels) \n",
    "action_size = game.get_available_buttons_size()              # 3 possible actions: left, right, shoot\n",
    "learning_rate =  0.0002      # Alpha (aka learning rate)\n",
    "\n",
    "### TRAINING HYPERPARAMETERS\n",
    "total_episodes = 500        # Total episodes for training\n",
    "max_steps = 100              # Max possible steps in an episode\n",
    "batch_size = 64             \n",
    "\n",
    "# Exploration parameters for epsilon greedy strategy\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Q learning hyperparameters\n",
    "gamma = 0.95               # Discounting rate\n",
    "\n",
    "### MEMORY HYPERPARAMETERS\n",
    "pretrain_length = batch_size   # Number of experiences stored in the Memory when initialized for the first time\n",
    "memory_size = 1000000          # Number of experiences the Memory can keep\n",
    "\n",
    "### MODIFY THIS TO FALSE IF YOU JUST WANT TO SEE THE TRAINED AGENT\n",
    "training = True\n",
    "\n",
    "## TURN THIS TO TRUE IF YOU WANT TO RENDER THE ENVIRONMENT\n",
    "episode_render = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create our Deep Q-learning Neural Network model üß†\n",
    "<img src=\"assets/model.png\" alt=\"Model\" />\n",
    "This is our Deep Q-learning model:\n",
    "- We take a stack of 4 frames as input\n",
    "- It passes through 3 convnets\n",
    "- Then it is flatened\n",
    "- Finally it passes through 2 FC layers\n",
    "- It outputs a Q value for each actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T15:14:55.368234Z",
     "start_time": "2021-03-03T15:14:55.359144Z"
    }
   },
   "outputs": [],
   "source": [
    "class DQNetwork:\n",
    "    def __init__(self, state_size, action_size, learning_rate, name='DQNetwork'):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        with tf.variable_scope(name):\n",
    "            # We create the placeholders\n",
    "            # *state_size means that we take each elements of state_size in tuple hence is like if we wrote\n",
    "            # [None, 84, 84, 4]\n",
    "            self.inputs_ = tf.placeholder(tf.float32, [None, *state_size], name=\"inputs\")\n",
    "            self.actions_ = tf.placeholder(tf.float32, [None, 3], name=\"actions_\")\n",
    "            \n",
    "            # Remember that target_Q is the R(s,a) + ymax Qhat(s', a')\n",
    "            self.target_Q = tf.placeholder(tf.float32, [None], name=\"target\")\n",
    "            \n",
    "            \"\"\"\n",
    "            First convnet:\n",
    "            CNN\n",
    "            BatchNormalization\n",
    "            ELU\n",
    "            \"\"\"\n",
    "            # Input is 84x84x4\n",
    "            self.conv1 = tf.layers.conv2d(inputs = self.inputs_,\n",
    "                                         filters = 32,\n",
    "                                         kernel_size = [8,8],\n",
    "                                         strides = [4,4],\n",
    "                                         padding = \"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                         name = \"conv1\")\n",
    "            \n",
    "            self.conv1_batchnorm = tf.layers.batch_normalization(self.conv1,\n",
    "                                                   training = True,\n",
    "                                                   epsilon = 1e-5,\n",
    "                                                     name = 'batch_norm1')\n",
    "            \n",
    "            self.conv1_out = tf.nn.elu(self.conv1_batchnorm, name=\"conv1_out\")\n",
    "            ## --> [20, 20, 32]\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Second convnet:\n",
    "            CNN\n",
    "            BatchNormalization\n",
    "            ELU\n",
    "            \"\"\"\n",
    "            self.conv2 = tf.layers.conv2d(inputs = self.conv1_out,\n",
    "                                 filters = 64,\n",
    "                                 kernel_size = [4,4],\n",
    "                                 strides = [2,2],\n",
    "                                 padding = \"VALID\",\n",
    "                                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                 name = \"conv2\")\n",
    "        \n",
    "            self.conv2_batchnorm = tf.layers.batch_normalization(self.conv2,\n",
    "                                                   training = True,\n",
    "                                                   epsilon = 1e-5,\n",
    "                                                     name = 'batch_norm2')\n",
    "\n",
    "            self.conv2_out = tf.nn.elu(self.conv2_batchnorm, name=\"conv2_out\")\n",
    "            ## --> [9, 9, 64]\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Third convnet:\n",
    "            CNN\n",
    "            BatchNormalization\n",
    "            ELU\n",
    "            \"\"\"\n",
    "            self.conv3 = tf.layers.conv2d(inputs = self.conv2_out,\n",
    "                                 filters = 128,\n",
    "                                 kernel_size = [4,4],\n",
    "                                 strides = [2,2],\n",
    "                                 padding = \"VALID\",\n",
    "                                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                 name = \"conv3\")\n",
    "        \n",
    "            self.conv3_batchnorm = tf.layers.batch_normalization(self.conv3,\n",
    "                                                   training = True,\n",
    "                                                   epsilon = 1e-5,\n",
    "                                                     name = 'batch_norm3')\n",
    "\n",
    "            self.conv3_out = tf.nn.elu(self.conv3_batchnorm, name=\"conv3_out\")\n",
    "            ## --> [3, 3, 128]\n",
    "            \n",
    "            \n",
    "            self.flatten = tf.layers.flatten(self.conv3_out)\n",
    "            ## --> [1152]\n",
    "            \n",
    "            \n",
    "            self.fc = tf.layers.dense(inputs = self.flatten,\n",
    "                                  units = 512,\n",
    "                                  activation = tf.nn.elu,\n",
    "                                       kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                name=\"fc1\")\n",
    "            \n",
    "            \n",
    "            self.output = tf.layers.dense(inputs = self.fc, \n",
    "                                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                          units = 3, \n",
    "                                        activation=None)\n",
    "\n",
    "  \n",
    "            # Q is our predicted Q value.\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, self.actions_), axis=1)\n",
    "            \n",
    "            \n",
    "            # The loss is the difference between our predicted Q_values and the Q_target\n",
    "            # Sum(Qtarget - Q)^2\n",
    "            self.loss = tf.reduce_mean(tf.square(self.target_Q - self.Q))\n",
    "            \n",
    "            self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T15:14:55.970033Z",
     "start_time": "2021-03-03T15:14:55.369263Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-8-511c694463c0>:30: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "WARNING:tensorflow:From /home/explore/miniconda3/envs/drl_simonini/lib/python3.7/site-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From <ipython-input-8-511c694463c0>:35: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
      "WARNING:tensorflow:From <ipython-input-8-511c694463c0>:87: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From <ipython-input-8-511c694463c0>:95: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /home/explore/miniconda3/envs/drl_simonini/lib/python3.7/site-packages/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# Reset the graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Instantiate the DQNetwork\n",
    "DQNetwork = DQNetwork(state_size, action_size, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Experience Replay üîÅ\n",
    "Now that we create our Neural Network, **we need to implement the Experience Replay method.** <br><br>\n",
    "Here we'll create the Memory object that creates a deque.A deque (double ended queue) is a data type that **removes the oldest element each time that you add a new element.**\n",
    "\n",
    "This part was taken from Udacity : <a href=\"https://github.com/udacity/deep-learning/blob/master/reinforcement/Q-learning-cart.ipynb\" Cartpole DQN</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T15:14:55.973821Z",
     "start_time": "2021-03-03T15:14:55.970993Z"
    }
   },
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen = max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        buffer_size = len(self.buffer)\n",
    "        index = np.random.choice(np.arange(buffer_size),\n",
    "                                size = batch_size,\n",
    "                                replace = False)\n",
    "        \n",
    "        return [self.buffer[i] for i in index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll **deal with the empty memory problem**: we pre-populate our memory by taking random actions and storing the experience (state, action, reward, new_state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T15:14:56.104887Z",
     "start_time": "2021-03-03T15:14:55.975099Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate memory\n",
    "memory = Memory(max_size = memory_size)\n",
    "\n",
    "# Render the environment\n",
    "game.new_episode()\n",
    "\n",
    "for i in range(pretrain_length):\n",
    "    # If it's the first step\n",
    "    if i == 0:\n",
    "        # First we need a state\n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "    \n",
    "    # Random action\n",
    "    action = random.choice(possible_actions)\n",
    "    \n",
    "    # Get the rewards\n",
    "    reward = game.make_action(action)\n",
    "    \n",
    "    # Look if the episode is finished\n",
    "    done = game.is_episode_finished()\n",
    "    \n",
    "    # If we're dead\n",
    "    if done:\n",
    "        # We finished the episode\n",
    "        next_state = np.zeros(state.shape)\n",
    "        \n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        \n",
    "        # Start a new episode\n",
    "        game.new_episode()\n",
    "        \n",
    "        # First we need a state\n",
    "        state = game.get_state().screen_buffer\n",
    "        \n",
    "        # Stack the frames\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "        \n",
    "    else:\n",
    "        # Get the next state\n",
    "        next_state = game.get_state().screen_buffer\n",
    "        next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "        \n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        \n",
    "        # Our state is now the next_state\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Set up Tensorboard üìä\n",
    "For more information about tensorboard, please watch this <a href=\"https://www.youtube.com/embed/eBbEDRsCmv4\">excellent 30min tutorial</a> <br><br>\n",
    "To launch tensorboard : `tensorboard --logdir=./tensorboard/dqn/1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T15:14:56.110581Z",
     "start_time": "2021-03-03T15:14:56.105908Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setup TensorBoard Writer\n",
    "writer = tf.summary.FileWriter(\"./tensorboard/dqn/1\")\n",
    "\n",
    "## Losses\n",
    "tf.summary.scalar(\"Loss\", DQNetwork.loss)\n",
    "\n",
    "write_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Train our Agent üèÉ‚Äç‚ôÇÔ∏è\n",
    "\n",
    "Our algorithm:\n",
    "<br>\n",
    "* Initialize the weights\n",
    "* Init the environment\n",
    "* Initialize the decay rate (that will use to reduce epsilon) \n",
    "<br><br>\n",
    "* **For** episode to max_episode **do** \n",
    "    * Make new episode\n",
    "    * Set step to 0\n",
    "    * Observe the first state $s_0$\n",
    "    <br><br>\n",
    "    * **While** step < max_steps **do**:\n",
    "        * Increase decay_rate\n",
    "        * With $\\epsilon$ select a random action $a_t$, otherwise select $a_t = \\mathrm{argmax}_a Q(s_t,a)$\n",
    "        * Execute action $a_t$ in simulator and observe reward $r_{t+1}$ and new state $s_{t+1}$\n",
    "        * Store transition $<s_t, a_t, r_{t+1}, s_{t+1}>$ in memory $D$\n",
    "        * Sample random mini-batch from $D$: $<s, a, r, s'>$\n",
    "        * Set $\\hat{Q} = r$ if the episode ends at $+1$, otherwise set $\\hat{Q} = r + \\gamma \\max_{a'}{Q(s', a')}$\n",
    "        * Make a gradient descent step with loss $(\\hat{Q} - Q(s, a))^2$\n",
    "    * **endfor**\n",
    "    <br><br>\n",
    "* **endfor**\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T15:14:56.117434Z",
     "start_time": "2021-03-03T15:14:56.111473Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function will do the part\n",
    "With œµ select a random action atat, otherwise select at=argmaxaQ(st,a)\n",
    "\"\"\"\n",
    "def predict_action(explore_start, explore_stop, decay_rate, decay_step, state, actions):\n",
    "    ## EPSILON GREEDY STRATEGY\n",
    "    # Choose action a from state s using epsilon greedy.\n",
    "    ## First we randomize a number\n",
    "    exp_exp_tradeoff = np.random.rand()\n",
    "\n",
    "    # Here we'll use an improved version of our epsilon greedy strategy used in Q-learning notebook\n",
    "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "    \n",
    "    if (explore_probability > exp_exp_tradeoff):\n",
    "        # Make a random action (exploration)\n",
    "        action = random.choice(possible_actions)\n",
    "        \n",
    "    else:\n",
    "        # Get action from Q-network (exploitation)\n",
    "        # Estimate the Qs values state\n",
    "        Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "        \n",
    "        # Take the biggest Q value (= the best action)\n",
    "        choice = np.argmax(Qs)\n",
    "        action = possible_actions[int(choice)]\n",
    "                \n",
    "    return action, explore_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T15:14:56.229056Z",
     "start_time": "2021-03-03T15:14:56.118417Z"
    }
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "tf.keras.backend.set_session(tf.Session(config=config));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T15:22:01.978001Z",
     "start_time": "2021-03-03T15:14:56.230110Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n",
      "Episode: 4 Total reward: 95.0 Training loss: 0.8955 Explore P: 0.9606\n",
      "Model Saved\n",
      "Model Saved\n",
      "Episode: 11 Total reward: 94.0 Training loss: 0.4188 Explore P: 0.9046\n",
      "Episode: 12 Total reward: 76.0 Training loss: 3.7190 Explore P: 0.9028\n",
      "Model Saved\n",
      "Episode: 17 Total reward: -21.0 Training loss: 3.7790 Explore P: 0.8595\n",
      "Model Saved\n",
      "Episode: 21 Total reward: 93.0 Training loss: 1.8547 Explore P: 0.8338\n",
      "Episode: 22 Total reward: 70.0 Training loss: 1.9218 Explore P: 0.8316\n",
      "Episode: 23 Total reward: 13.0 Training loss: 56.0187 Explore P: 0.8261\n",
      "Episode: 25 Total reward: 57.0 Training loss: 2.6823 Explore P: 0.8152\n",
      "Model Saved\n",
      "Episode: 28 Total reward: 95.0 Training loss: 9.2574 Explore P: 0.7988\n",
      "Model Saved\n",
      "Episode: 34 Total reward: 74.0 Training loss: 2.7551 Explore P: 0.7587\n",
      "Episode: 35 Total reward: 93.0 Training loss: 140.5170 Explore P: 0.7581\n",
      "Model Saved\n",
      "Episode: 36 Total reward: 16.0 Training loss: 2.8286 Explore P: 0.7529\n",
      "Episode: 38 Total reward: 45.0 Training loss: 6.5435 Explore P: 0.7421\n",
      "Model Saved\n",
      "Episode: 41 Total reward: 74.0 Training loss: 10.7408 Explore P: 0.7260\n",
      "Episode: 42 Total reward: 94.0 Training loss: 17.4907 Explore P: 0.7255\n",
      "Episode: 43 Total reward: 85.0 Training loss: 6.5500 Explore P: 0.7244\n",
      "Episode: 45 Total reward: 95.0 Training loss: 14.6052 Explore P: 0.7168\n",
      "Model Saved\n",
      "Episode: 46 Total reward: 52.0 Training loss: 8.3879 Explore P: 0.7141\n",
      "Episode: 47 Total reward: 95.0 Training loss: 8.7369 Explore P: 0.7137\n",
      "Episode: 49 Total reward: 67.0 Training loss: 5.4434 Explore P: 0.7046\n",
      "Model Saved\n",
      "Episode: 52 Total reward: -3.0 Training loss: 9.6030 Explore P: 0.6852\n",
      "Episode: 53 Total reward: 75.0 Training loss: 2.4332 Explore P: 0.6838\n",
      "Model Saved\n",
      "Episode: 57 Total reward: 94.0 Training loss: 5.9650 Explore P: 0.6634\n",
      "Model Saved\n",
      "Episode: 61 Total reward: 89.0 Training loss: 4.4800 Explore P: 0.6433\n",
      "Model Saved\n",
      "Episode: 68 Total reward: 94.0 Training loss: 4.1854 Explore P: 0.6060\n",
      "Episode: 69 Total reward: -12.0 Training loss: 7.3158 Explore P: 0.6008\n",
      "Episode: 70 Total reward: 95.0 Training loss: 6.4641 Explore P: 0.6005\n",
      "Model Saved\n",
      "Episode: 73 Total reward: 62.0 Training loss: 7.3462 Explore P: 0.5868\n",
      "Episode: 74 Total reward: 85.0 Training loss: 14.1086 Explore P: 0.5859\n",
      "Episode: 75 Total reward: 76.0 Training loss: 1.0814 Explore P: 0.5847\n",
      "Model Saved\n",
      "Episode: 77 Total reward: 54.0 Training loss: 5.7408 Explore P: 0.5769\n",
      "Episode: 78 Total reward: 62.0 Training loss: 4.3905 Explore P: 0.5750\n",
      "Episode: 79 Total reward: -17.0 Training loss: 7.3470 Explore P: 0.5698\n",
      "Episode: 80 Total reward: 74.0 Training loss: 19.9984 Explore P: 0.5685\n",
      "Model Saved\n",
      "Episode: 81 Total reward: 41.0 Training loss: 6.4751 Explore P: 0.5657\n",
      "Episode: 83 Total reward: 75.0 Training loss: 7.3078 Explore P: 0.5591\n",
      "Episode: 84 Total reward: 32.0 Training loss: 3.0870 Explore P: 0.5561\n",
      "Episode: 85 Total reward: 94.0 Training loss: 6.3626 Explore P: 0.5557\n",
      "Model Saved\n",
      "Episode: 86 Total reward: 38.0 Training loss: 2.7302 Explore P: 0.5531\n",
      "Episode: 87 Total reward: 93.0 Training loss: 11.7706 Explore P: 0.5527\n",
      "Episode: 88 Total reward: 5.0 Training loss: 9.4503 Explore P: 0.5486\n",
      "Episode: 89 Total reward: 94.0 Training loss: 13.6191 Explore P: 0.5482\n",
      "Episode: 90 Total reward: 56.0 Training loss: 3.3207 Explore P: 0.5463\n",
      "Model Saved\n",
      "Episode: 93 Total reward: 23.0 Training loss: 3.7837 Explore P: 0.5324\n",
      "Episode: 94 Total reward: 62.0 Training loss: 4.2298 Explore P: 0.5306\n",
      "Model Saved\n",
      "Episode: 97 Total reward: 27.0 Training loss: 5.2808 Explore P: 0.5173\n",
      "Episode: 98 Total reward: 90.0 Training loss: 7.3426 Explore P: 0.5167\n",
      "Episode: 99 Total reward: 93.0 Training loss: 6.9904 Explore P: 0.5163\n",
      "Episode: 100 Total reward: 30.0 Training loss: 3.2385 Explore P: 0.5135\n",
      "Model Saved\n",
      "Episode: 101 Total reward: 8.0 Training loss: 4.0277 Explore P: 0.5098\n",
      "Episode: 102 Total reward: 75.0 Training loss: 63.0756 Explore P: 0.5088\n",
      "Episode: 103 Total reward: -3.0 Training loss: 6.0179 Explore P: 0.5046\n",
      "Episode: 104 Total reward: 50.0 Training loss: 14.4388 Explore P: 0.5026\n",
      "Episode: 105 Total reward: 75.0 Training loss: 9.9547 Explore P: 0.5016\n",
      "Model Saved\n",
      "Episode: 106 Total reward: -14.0 Training loss: 6.1677 Explore P: 0.4969\n",
      "Episode: 107 Total reward: 19.0 Training loss: 4.9886 Explore P: 0.4937\n",
      "Episode: 108 Total reward: 94.0 Training loss: 6.5826 Explore P: 0.4933\n",
      "Episode: 109 Total reward: 68.0 Training loss: 10.7332 Explore P: 0.4920\n",
      "Episode: 110 Total reward: 54.0 Training loss: 5.4758 Explore P: 0.4902\n",
      "Model Saved\n",
      "Episode: 111 Total reward: 66.0 Training loss: 4.0245 Explore P: 0.4888\n",
      "Episode: 112 Total reward: 65.0 Training loss: 11.0493 Explore P: 0.4873\n",
      "Episode: 113 Total reward: 63.0 Training loss: 4.9409 Explore P: 0.4857\n",
      "Episode: 114 Total reward: 67.0 Training loss: 11.5482 Explore P: 0.4843\n",
      "Episode: 115 Total reward: 26.0 Training loss: 8.0023 Explore P: 0.4815\n",
      "Model Saved\n",
      "Episode: 116 Total reward: 51.0 Training loss: 5.9006 Explore P: 0.4796\n",
      "Episode: 117 Total reward: 52.0 Training loss: 3.5831 Explore P: 0.4778\n",
      "Episode: 118 Total reward: 93.0 Training loss: 7.7429 Explore P: 0.4774\n",
      "Episode: 119 Total reward: 68.0 Training loss: 8.0987 Explore P: 0.4761\n",
      "Episode: 120 Total reward: 11.0 Training loss: 12.8472 Explore P: 0.4729\n",
      "Model Saved\n",
      "Episode: 121 Total reward: 61.0 Training loss: 6.3400 Explore P: 0.4712\n",
      "Episode: 122 Total reward: 95.0 Training loss: 10.8965 Explore P: 0.4710\n",
      "Episode: 123 Total reward: 94.0 Training loss: 5.9799 Explore P: 0.4706\n",
      "Episode: 125 Total reward: 95.0 Training loss: 9.0965 Explore P: 0.4658\n",
      "Model Saved\n",
      "Episode: 126 Total reward: 61.0 Training loss: 15.7155 Explore P: 0.4642\n",
      "Episode: 127 Total reward: 74.0 Training loss: 8.0344 Explore P: 0.4632\n",
      "Episode: 128 Total reward: 71.0 Training loss: 7.4998 Explore P: 0.4621\n",
      "Episode: 129 Total reward: 92.0 Training loss: 4.4848 Explore P: 0.4616\n",
      "Episode: 130 Total reward: 46.0 Training loss: 13.0796 Explore P: 0.4596\n",
      "Model Saved\n",
      "Episode: 131 Total reward: 68.0 Training loss: 15.8841 Explore P: 0.4584\n",
      "Episode: 132 Total reward: 37.0 Training loss: 6.1987 Explore P: 0.4562\n",
      "Episode: 133 Total reward: 30.0 Training loss: 15.5796 Explore P: 0.4537\n",
      "Episode: 134 Total reward: 86.0 Training loss: 7.5259 Explore P: 0.4530\n",
      "Episode: 135 Total reward: 15.0 Training loss: 4.7917 Explore P: 0.4499\n",
      "Model Saved\n",
      "Episode: 136 Total reward: 25.0 Training loss: 10.1018 Explore P: 0.4472\n",
      "Episode: 137 Total reward: 65.0 Training loss: 7.9116 Explore P: 0.4459\n",
      "Episode: 138 Total reward: 92.0 Training loss: 17.0677 Explore P: 0.4455\n",
      "Episode: 139 Total reward: 50.0 Training loss: 7.3227 Explore P: 0.4437\n",
      "Episode: 140 Total reward: 95.0 Training loss: 7.1511 Explore P: 0.4434\n",
      "Model Saved\n",
      "Episode: 141 Total reward: 59.0 Training loss: 7.2699 Explore P: 0.4418\n",
      "Episode: 142 Total reward: 62.0 Training loss: 11.0224 Explore P: 0.4404\n",
      "Episode: 143 Total reward: 94.0 Training loss: 8.3385 Explore P: 0.4401\n",
      "Episode: 144 Total reward: 25.0 Training loss: 6.5388 Explore P: 0.4374\n",
      "Episode: 145 Total reward: 1.0 Training loss: 6.1370 Explore P: 0.4340\n",
      "Model Saved\n",
      "Episode: 146 Total reward: 69.0 Training loss: 9.6849 Explore P: 0.4329\n",
      "Episode: 147 Total reward: 93.0 Training loss: 13.9076 Explore P: 0.4325\n",
      "Episode: 148 Total reward: 54.0 Training loss: 6.2916 Explore P: 0.4310\n",
      "Episode: 149 Total reward: 63.0 Training loss: 14.3407 Explore P: 0.4296\n",
      "Episode: 150 Total reward: 39.0 Training loss: 10.7789 Explore P: 0.4274\n",
      "Model Saved\n",
      "Episode: 151 Total reward: 60.0 Training loss: 6.2066 Explore P: 0.4259\n",
      "Episode: 152 Total reward: 61.0 Training loss: 8.1717 Explore P: 0.4245\n",
      "Episode: 153 Total reward: 76.0 Training loss: 12.4490 Explore P: 0.4236\n",
      "Episode: 154 Total reward: 68.0 Training loss: 13.7802 Explore P: 0.4225\n",
      "Model Saved\n",
      "Episode: 156 Total reward: 90.0 Training loss: 9.9449 Explore P: 0.4179\n",
      "Episode: 157 Total reward: 21.0 Training loss: 7.3437 Explore P: 0.4153\n",
      "Episode: 159 Total reward: 73.0 Training loss: 6.1107 Explore P: 0.4103\n",
      "Episode: 160 Total reward: 95.0 Training loss: 7.9242 Explore P: 0.4101\n",
      "Model Saved\n",
      "Episode: 161 Total reward: 95.0 Training loss: 5.4359 Explore P: 0.4099\n",
      "Episode: 162 Total reward: 91.0 Training loss: 9.7519 Explore P: 0.4095\n",
      "Episode: 163 Total reward: 53.0 Training loss: 8.4565 Explore P: 0.4079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 164 Total reward: 22.0 Training loss: 15.9124 Explore P: 0.4052\n",
      "Episode: 165 Total reward: 66.0 Training loss: 16.7729 Explore P: 0.4040\n",
      "Model Saved\n",
      "Episode: 166 Total reward: 94.0 Training loss: 7.0903 Explore P: 0.4037\n",
      "Episode: 167 Total reward: 93.0 Training loss: 6.4919 Explore P: 0.4034\n",
      "Episode: 168 Total reward: 27.0 Training loss: 4.8197 Explore P: 0.4011\n",
      "Episode: 169 Total reward: 52.0 Training loss: 3.8667 Explore P: 0.3996\n",
      "Episode: 170 Total reward: 94.0 Training loss: 7.7751 Explore P: 0.3993\n",
      "Model Saved\n",
      "Episode: 171 Total reward: 31.0 Training loss: 7.3933 Explore P: 0.3972\n",
      "Episode: 172 Total reward: 68.0 Training loss: 9.0641 Explore P: 0.3961\n",
      "Episode: 173 Total reward: 94.0 Training loss: 3.3227 Explore P: 0.3958\n",
      "Episode: 174 Total reward: 43.0 Training loss: 15.6840 Explore P: 0.3940\n",
      "Episode: 175 Total reward: 90.0 Training loss: 14.8641 Explore P: 0.3936\n",
      "Model Saved\n",
      "Episode: 176 Total reward: 91.0 Training loss: 6.9231 Explore P: 0.3932\n",
      "Episode: 177 Total reward: 95.0 Training loss: 9.8308 Explore P: 0.3930\n",
      "Episode: 178 Total reward: 13.0 Training loss: 9.0857 Explore P: 0.3902\n",
      "Episode: 179 Total reward: 25.0 Training loss: 20.3810 Explore P: 0.3879\n",
      "Episode: 180 Total reward: 75.0 Training loss: 9.0563 Explore P: 0.3871\n",
      "Model Saved\n",
      "Episode: 181 Total reward: 61.0 Training loss: 12.3350 Explore P: 0.3857\n",
      "Episode: 182 Total reward: 94.0 Training loss: 10.6335 Explore P: 0.3855\n",
      "Episode: 183 Total reward: 26.0 Training loss: 4.0628 Explore P: 0.3832\n",
      "Episode: 184 Total reward: 27.0 Training loss: 10.9842 Explore P: 0.3810\n",
      "Episode: 185 Total reward: 72.0 Training loss: 7.6714 Explore P: 0.3801\n",
      "Model Saved\n",
      "Episode: 187 Total reward: 92.0 Training loss: 7.4014 Explore P: 0.3761\n",
      "Episode: 188 Total reward: 66.0 Training loss: 6.2651 Explore P: 0.3750\n",
      "Episode: 189 Total reward: 73.0 Training loss: 14.4143 Explore P: 0.3742\n",
      "Episode: 190 Total reward: 67.0 Training loss: 12.3531 Explore P: 0.3731\n",
      "Model Saved\n",
      "Episode: 191 Total reward: 49.0 Training loss: 12.1760 Explore P: 0.3716\n",
      "Episode: 192 Total reward: 57.0 Training loss: 7.1494 Explore P: 0.3704\n",
      "Episode: 193 Total reward: 43.0 Training loss: 7.6617 Explore P: 0.3687\n",
      "Episode: 194 Total reward: 15.0 Training loss: 9.7933 Explore P: 0.3661\n",
      "Episode: 195 Total reward: 72.0 Training loss: 8.6187 Explore P: 0.3651\n",
      "Model Saved\n",
      "Episode: 196 Total reward: 55.0 Training loss: 8.9555 Explore P: 0.3638\n",
      "Episode: 197 Total reward: 35.0 Training loss: 16.6679 Explore P: 0.3618\n",
      "Episode: 198 Total reward: 95.0 Training loss: 9.0377 Explore P: 0.3616\n",
      "Episode: 199 Total reward: 63.0 Training loss: 18.8058 Explore P: 0.3605\n",
      "Episode: 200 Total reward: 74.0 Training loss: 5.9410 Explore P: 0.3597\n",
      "Model Saved\n",
      "Episode: 201 Total reward: 65.0 Training loss: 19.8010 Explore P: 0.3586\n",
      "Episode: 202 Total reward: 57.0 Training loss: 6.2830 Explore P: 0.3574\n",
      "Episode: 203 Total reward: 35.0 Training loss: 23.0046 Explore P: 0.3555\n",
      "Episode: 204 Total reward: 53.0 Training loss: 6.6945 Explore P: 0.3542\n",
      "Episode: 205 Total reward: 56.0 Training loss: 14.8061 Explore P: 0.3530\n",
      "Model Saved\n",
      "Episode: 206 Total reward: -4.0 Training loss: 21.9252 Explore P: 0.3501\n",
      "Episode: 207 Total reward: 10.0 Training loss: 4.7555 Explore P: 0.3477\n",
      "Episode: 208 Total reward: 58.0 Training loss: 10.6679 Explore P: 0.3464\n",
      "Episode: 209 Total reward: 42.0 Training loss: 8.4083 Explore P: 0.3448\n",
      "Episode: 210 Total reward: 46.0 Training loss: 6.7043 Explore P: 0.3433\n",
      "Model Saved\n",
      "Episode: 211 Total reward: 35.0 Training loss: 5.0626 Explore P: 0.3414\n",
      "Episode: 212 Total reward: 50.0 Training loss: 8.9660 Explore P: 0.3400\n",
      "Episode: 213 Total reward: 49.0 Training loss: 8.2119 Explore P: 0.3387\n",
      "Episode: 214 Total reward: -8.0 Training loss: 6.3216 Explore P: 0.3356\n",
      "Model Saved\n",
      "Episode: 216 Total reward: 30.0 Training loss: 6.7519 Explore P: 0.3304\n",
      "Episode: 217 Total reward: 39.0 Training loss: 14.0760 Explore P: 0.3287\n",
      "Episode: 218 Total reward: 76.0 Training loss: 6.9257 Explore P: 0.3281\n",
      "Episode: 219 Total reward: 94.0 Training loss: 8.4725 Explore P: 0.3279\n",
      "Episode: 220 Total reward: -3.0 Training loss: 12.9459 Explore P: 0.3252\n",
      "Model Saved\n",
      "Episode: 221 Total reward: 93.0 Training loss: 10.2748 Explore P: 0.3249\n",
      "Episode: 222 Total reward: 50.0 Training loss: 8.9192 Explore P: 0.3237\n",
      "Episode: 223 Total reward: 73.0 Training loss: 5.8525 Explore P: 0.3229\n",
      "Episode: 224 Total reward: 92.0 Training loss: 6.4917 Explore P: 0.3227\n",
      "Episode: 225 Total reward: 67.0 Training loss: 8.8848 Explore P: 0.3218\n",
      "Model Saved\n",
      "Episode: 226 Total reward: 95.0 Training loss: 24.2075 Explore P: 0.3216\n",
      "Episode: 228 Total reward: 48.0 Training loss: 6.8882 Explore P: 0.3171\n",
      "Episode: 230 Total reward: 93.0 Training loss: 8.8023 Explore P: 0.3138\n",
      "Model Saved\n",
      "Episode: 231 Total reward: 47.0 Training loss: 4.3373 Explore P: 0.3125\n",
      "Episode: 232 Total reward: 18.0 Training loss: 12.3681 Explore P: 0.3105\n",
      "Episode: 233 Total reward: 90.0 Training loss: 4.4464 Explore P: 0.3101\n",
      "Episode: 234 Total reward: 92.0 Training loss: 10.7472 Explore P: 0.3099\n",
      "Episode: 235 Total reward: 68.0 Training loss: 8.0735 Explore P: 0.3090\n",
      "Model Saved\n",
      "Episode: 237 Total reward: 91.0 Training loss: 9.1962 Explore P: 0.3057\n",
      "Episode: 238 Total reward: 57.0 Training loss: 7.8850 Explore P: 0.3047\n",
      "Episode: 239 Total reward: 95.0 Training loss: 5.9501 Explore P: 0.3046\n",
      "Episode: 240 Total reward: 47.0 Training loss: 5.0955 Explore P: 0.3033\n",
      "Model Saved\n",
      "Episode: 241 Total reward: 69.0 Training loss: 9.0784 Explore P: 0.3025\n",
      "Episode: 242 Total reward: 33.0 Training loss: 6.7942 Explore P: 0.3008\n",
      "Episode: 243 Total reward: 95.0 Training loss: 6.8413 Explore P: 0.3006\n",
      "Episode: 244 Total reward: 67.0 Training loss: 6.4708 Explore P: 0.2998\n",
      "Model Saved\n",
      "Episode: 246 Total reward: 45.0 Training loss: 6.1809 Explore P: 0.2956\n",
      "Episode: 247 Total reward: 53.0 Training loss: 5.1264 Explore P: 0.2945\n",
      "Episode: 248 Total reward: 55.0 Training loss: 39.0467 Explore P: 0.2933\n",
      "Episode: 249 Total reward: 45.0 Training loss: 8.3467 Explore P: 0.2919\n",
      "Episode: 250 Total reward: 93.0 Training loss: 8.0321 Explore P: 0.2917\n",
      "Model Saved\n",
      "Episode: 251 Total reward: 45.0 Training loss: 5.2521 Explore P: 0.2904\n",
      "Episode: 252 Total reward: 53.0 Training loss: 5.6301 Explore P: 0.2892\n",
      "Episode: 253 Total reward: 93.0 Training loss: 6.5448 Explore P: 0.2889\n",
      "Episode: 254 Total reward: 71.0 Training loss: 8.8620 Explore P: 0.2882\n",
      "Episode: 255 Total reward: 49.0 Training loss: 6.4151 Explore P: 0.2871\n",
      "Model Saved\n",
      "Episode: 256 Total reward: 36.0 Training loss: 8.9087 Explore P: 0.2856\n",
      "Episode: 257 Total reward: 33.0 Training loss: 7.6437 Explore P: 0.2840\n",
      "Episode: 258 Total reward: 52.0 Training loss: 10.5327 Explore P: 0.2829\n",
      "Episode: 260 Total reward: 57.0 Training loss: 6.2223 Explore P: 0.2793\n",
      "Model Saved\n",
      "Episode: 262 Total reward: 74.0 Training loss: 16.4397 Explore P: 0.2760\n",
      "Episode: 263 Total reward: 90.0 Training loss: 6.6702 Explore P: 0.2757\n",
      "Episode: 265 Total reward: 76.0 Training loss: 4.1412 Explore P: 0.2725\n",
      "Model Saved\n",
      "Episode: 266 Total reward: 68.0 Training loss: 6.3497 Explore P: 0.2718\n",
      "Episode: 267 Total reward: 57.0 Training loss: 10.1009 Explore P: 0.2709\n",
      "Episode: 268 Total reward: 20.0 Training loss: 4.3500 Explore P: 0.2692\n",
      "Episode: 269 Total reward: 86.0 Training loss: 8.5630 Explore P: 0.2688\n",
      "Episode: 270 Total reward: 26.0 Training loss: 5.0460 Explore P: 0.2671\n",
      "Model Saved\n",
      "Episode: 271 Total reward: 89.0 Training loss: 11.0176 Explore P: 0.2668\n",
      "Episode: 272 Total reward: 73.0 Training loss: 11.3724 Explore P: 0.2661\n",
      "Episode: 273 Total reward: 66.0 Training loss: 6.7686 Explore P: 0.2653\n",
      "Episode: 274 Total reward: 42.0 Training loss: 12.1091 Explore P: 0.2641\n",
      "Episode: 275 Total reward: 47.0 Training loss: 10.6334 Explore P: 0.2630\n",
      "Model Saved\n",
      "Episode: 276 Total reward: 66.0 Training loss: 10.6710 Explore P: 0.2622\n",
      "Episode: 277 Total reward: 37.0 Training loss: 11.4634 Explore P: 0.2610\n",
      "Episode: 279 Total reward: 58.0 Training loss: 12.5415 Explore P: 0.2575\n",
      "Episode: 280 Total reward: 56.0 Training loss: 4.5770 Explore P: 0.2567\n",
      "Model Saved\n",
      "Episode: 281 Total reward: 14.0 Training loss: 7.6874 Explore P: 0.2549\n",
      "Episode: 282 Total reward: 43.0 Training loss: 11.5939 Explore P: 0.2537\n",
      "Episode: 283 Total reward: 52.0 Training loss: 7.4336 Explore P: 0.2527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 284 Total reward: 42.0 Training loss: 16.0244 Explore P: 0.2515\n",
      "Episode: 285 Total reward: 94.0 Training loss: 5.3804 Explore P: 0.2513\n",
      "Model Saved\n",
      "Episode: 286 Total reward: 65.0 Training loss: 5.2179 Explore P: 0.2506\n",
      "Episode: 288 Total reward: 86.0 Training loss: 4.1106 Explore P: 0.2478\n",
      "Episode: 289 Total reward: 94.0 Training loss: 19.9549 Explore P: 0.2477\n",
      "Episode: 290 Total reward: 95.0 Training loss: 2.9923 Explore P: 0.2475\n",
      "Model Saved\n",
      "Episode: 291 Total reward: 40.0 Training loss: 4.3114 Explore P: 0.2463\n",
      "Episode: 294 Total reward: 66.0 Training loss: 5.4314 Explore P: 0.2409\n",
      "Episode: 295 Total reward: 34.0 Training loss: 8.6180 Explore P: 0.2396\n",
      "Model Saved\n",
      "Episode: 296 Total reward: 88.0 Training loss: 5.0098 Explore P: 0.2393\n",
      "Episode: 297 Total reward: 44.0 Training loss: 5.2067 Explore P: 0.2382\n",
      "Episode: 298 Total reward: 40.0 Training loss: 6.1198 Explore P: 0.2371\n",
      "Episode: 299 Total reward: 29.0 Training loss: 3.8866 Explore P: 0.2357\n",
      "Model Saved\n",
      "Episode: 301 Total reward: 93.0 Training loss: 3.6845 Explore P: 0.2333\n",
      "Episode: 302 Total reward: 95.0 Training loss: 4.2576 Explore P: 0.2331\n",
      "Episode: 303 Total reward: 64.0 Training loss: 3.8652 Explore P: 0.2324\n",
      "Episode: 304 Total reward: 25.0 Training loss: 11.3808 Explore P: 0.2311\n",
      "Episode: 305 Total reward: 66.0 Training loss: 5.3734 Explore P: 0.2304\n",
      "Model Saved\n",
      "Episode: 306 Total reward: 50.0 Training loss: 7.8173 Explore P: 0.2295\n",
      "Episode: 307 Total reward: 93.0 Training loss: 5.2398 Explore P: 0.2293\n",
      "Episode: 308 Total reward: 67.0 Training loss: 6.1916 Explore P: 0.2287\n",
      "Episode: 309 Total reward: 67.0 Training loss: 6.1852 Explore P: 0.2280\n",
      "Episode: 310 Total reward: 32.0 Training loss: 12.4650 Explore P: 0.2268\n",
      "Model Saved\n",
      "Episode: 311 Total reward: 58.0 Training loss: 17.5830 Explore P: 0.2259\n",
      "Episode: 312 Total reward: 14.0 Training loss: 4.6425 Explore P: 0.2244\n",
      "Episode: 314 Total reward: 93.0 Training loss: 4.9662 Explore P: 0.2221\n",
      "Episode: 315 Total reward: 95.0 Training loss: 5.0095 Explore P: 0.2220\n",
      "Model Saved\n",
      "Episode: 316 Total reward: 85.0 Training loss: 4.6407 Explore P: 0.2216\n",
      "Episode: 317 Total reward: 93.0 Training loss: 7.0176 Explore P: 0.2215\n",
      "Episode: 318 Total reward: 93.0 Training loss: 24.9810 Explore P: 0.2213\n",
      "Episode: 319 Total reward: 75.0 Training loss: 16.6978 Explore P: 0.2208\n",
      "Episode: 320 Total reward: 68.0 Training loss: 5.2454 Explore P: 0.2203\n",
      "Model Saved\n",
      "Episode: 321 Total reward: 57.0 Training loss: 4.0661 Explore P: 0.2194\n",
      "Episode: 322 Total reward: 46.0 Training loss: 10.1367 Explore P: 0.2185\n",
      "Episode: 323 Total reward: 16.0 Training loss: 5.0615 Explore P: 0.2170\n",
      "Episode: 324 Total reward: 76.0 Training loss: 16.2005 Explore P: 0.2166\n",
      "Episode: 325 Total reward: 69.0 Training loss: 6.2425 Explore P: 0.2161\n",
      "Model Saved\n",
      "Episode: 326 Total reward: 59.0 Training loss: 5.8927 Explore P: 0.2153\n",
      "Episode: 327 Total reward: 93.0 Training loss: 6.2094 Explore P: 0.2151\n",
      "Episode: 328 Total reward: 44.0 Training loss: 3.2602 Explore P: 0.2142\n",
      "Episode: 329 Total reward: 71.0 Training loss: 10.2560 Explore P: 0.2137\n",
      "Episode: 330 Total reward: 83.0 Training loss: 6.1192 Explore P: 0.2133\n",
      "Model Saved\n",
      "Episode: 331 Total reward: 93.0 Training loss: 28.0486 Explore P: 0.2131\n",
      "Episode: 333 Total reward: 35.0 Training loss: 6.8221 Explore P: 0.2100\n",
      "Episode: 334 Total reward: 52.0 Training loss: 2.8514 Explore P: 0.2092\n",
      "Episode: 335 Total reward: 88.0 Training loss: 5.8944 Explore P: 0.2090\n",
      "Model Saved\n",
      "Episode: 336 Total reward: 68.0 Training loss: 6.5352 Explore P: 0.2084\n",
      "Episode: 337 Total reward: 17.0 Training loss: 14.1938 Explore P: 0.2070\n",
      "Episode: 338 Total reward: 95.0 Training loss: 7.0119 Explore P: 0.2069\n",
      "Episode: 339 Total reward: 67.0 Training loss: 8.6484 Explore P: 0.2064\n",
      "Episode: 340 Total reward: 46.0 Training loss: 6.6254 Explore P: 0.2055\n",
      "Model Saved\n",
      "Episode: 341 Total reward: 92.0 Training loss: 10.1833 Explore P: 0.2053\n",
      "Episode: 342 Total reward: 49.0 Training loss: 5.4368 Explore P: 0.2045\n",
      "Episode: 343 Total reward: 84.0 Training loss: 8.0057 Explore P: 0.2041\n",
      "Episode: 344 Total reward: 59.0 Training loss: 5.4685 Explore P: 0.2034\n",
      "Episode: 345 Total reward: 61.0 Training loss: 5.1961 Explore P: 0.2028\n",
      "Model Saved\n",
      "Episode: 346 Total reward: 86.0 Training loss: 3.8776 Explore P: 0.2025\n",
      "Episode: 347 Total reward: 66.0 Training loss: 16.6918 Explore P: 0.2019\n",
      "Episode: 348 Total reward: 31.0 Training loss: 16.0451 Explore P: 0.2007\n",
      "Episode: 349 Total reward: 43.0 Training loss: 14.2689 Explore P: 0.1998\n",
      "Episode: 350 Total reward: 38.0 Training loss: 8.7803 Explore P: 0.1989\n",
      "Model Saved\n",
      "Episode: 352 Total reward: 59.0 Training loss: 5.1959 Explore P: 0.1963\n",
      "Episode: 353 Total reward: 93.0 Training loss: 7.0258 Explore P: 0.1962\n",
      "Episode: 354 Total reward: 93.0 Training loss: 4.5174 Explore P: 0.1960\n",
      "Episode: 355 Total reward: 64.0 Training loss: 8.5993 Explore P: 0.1955\n",
      "Model Saved\n",
      "Episode: 356 Total reward: 95.0 Training loss: 4.6296 Explore P: 0.1953\n",
      "Episode: 357 Total reward: 71.0 Training loss: 6.2062 Explore P: 0.1948\n",
      "Episode: 358 Total reward: 95.0 Training loss: 8.1703 Explore P: 0.1947\n",
      "Episode: 359 Total reward: 66.0 Training loss: 5.3097 Explore P: 0.1941\n",
      "Episode: 360 Total reward: 66.0 Training loss: 9.9018 Explore P: 0.1936\n",
      "Model Saved\n",
      "Episode: 361 Total reward: 95.0 Training loss: 6.5549 Explore P: 0.1935\n",
      "Episode: 362 Total reward: 8.0 Training loss: 5.8933 Explore P: 0.1920\n",
      "Episode: 363 Total reward: 60.0 Training loss: 5.5553 Explore P: 0.1914\n",
      "Episode: 364 Total reward: 50.0 Training loss: 6.8845 Explore P: 0.1906\n",
      "Episode: 365 Total reward: 63.0 Training loss: 4.3269 Explore P: 0.1900\n",
      "Model Saved\n",
      "Episode: 366 Total reward: 71.0 Training loss: 9.4780 Explore P: 0.1896\n",
      "Episode: 367 Total reward: 95.0 Training loss: 11.5088 Explore P: 0.1895\n",
      "Episode: 368 Total reward: -22.0 Training loss: 6.9752 Explore P: 0.1877\n",
      "Episode: 369 Total reward: 59.0 Training loss: 4.5890 Explore P: 0.1871\n",
      "Episode: 370 Total reward: 92.0 Training loss: 3.4918 Explore P: 0.1869\n",
      "Model Saved\n",
      "Episode: 371 Total reward: 28.0 Training loss: 5.0271 Explore P: 0.1859\n",
      "Episode: 372 Total reward: 51.0 Training loss: 8.4172 Explore P: 0.1852\n",
      "Episode: 373 Total reward: 65.0 Training loss: 6.8672 Explore P: 0.1847\n",
      "Episode: 374 Total reward: 56.0 Training loss: 7.2842 Explore P: 0.1840\n",
      "Episode: 375 Total reward: 56.0 Training loss: 4.9818 Explore P: 0.1833\n",
      "Model Saved\n",
      "Episode: 376 Total reward: 72.0 Training loss: 6.2481 Explore P: 0.1828\n",
      "Episode: 379 Total reward: 56.0 Training loss: 4.9125 Explore P: 0.1787\n",
      "Episode: 380 Total reward: 70.0 Training loss: 9.1129 Explore P: 0.1782\n",
      "Model Saved\n",
      "Episode: 381 Total reward: 72.0 Training loss: 10.8099 Explore P: 0.1778\n",
      "Episode: 382 Total reward: 85.0 Training loss: 4.3456 Explore P: 0.1776\n",
      "Episode: 383 Total reward: 67.0 Training loss: 3.1121 Explore P: 0.1771\n",
      "Episode: 384 Total reward: 22.0 Training loss: 42.2044 Explore P: 0.1760\n",
      "Episode: 385 Total reward: 45.0 Training loss: 6.7093 Explore P: 0.1752\n",
      "Model Saved\n",
      "Episode: 386 Total reward: 58.0 Training loss: 9.2911 Explore P: 0.1746\n",
      "Episode: 388 Total reward: 39.0 Training loss: 5.9133 Explore P: 0.1721\n",
      "Episode: 390 Total reward: 81.0 Training loss: 5.4113 Explore P: 0.1702\n",
      "Model Saved\n",
      "Episode: 391 Total reward: 58.0 Training loss: 4.7741 Explore P: 0.1696\n",
      "Episode: 392 Total reward: 56.0 Training loss: 4.5701 Explore P: 0.1690\n",
      "Episode: 393 Total reward: 75.0 Training loss: 10.0787 Explore P: 0.1686\n",
      "Episode: 394 Total reward: 68.0 Training loss: 9.9652 Explore P: 0.1682\n",
      "Episode: 395 Total reward: 43.0 Training loss: 4.5665 Explore P: 0.1674\n",
      "Model Saved\n",
      "Episode: 396 Total reward: 82.0 Training loss: 5.5412 Explore P: 0.1671\n",
      "Episode: 397 Total reward: 90.0 Training loss: 8.4633 Explore P: 0.1669\n",
      "Episode: 398 Total reward: 71.0 Training loss: 5.8326 Explore P: 0.1666\n",
      "Episode: 399 Total reward: 77.0 Training loss: 10.2828 Explore P: 0.1662\n",
      "Episode: 400 Total reward: 34.0 Training loss: 8.0057 Explore P: 0.1653\n",
      "Model Saved\n",
      "Episode: 403 Total reward: 66.0 Training loss: 4.5878 Explore P: 0.1618\n",
      "Episode: 404 Total reward: 87.0 Training loss: 16.6705 Explore P: 0.1615\n",
      "Episode: 405 Total reward: 68.0 Training loss: 9.0729 Explore P: 0.1611\n",
      "Model Saved\n",
      "Episode: 406 Total reward: 62.0 Training loss: 11.4065 Explore P: 0.1606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 407 Total reward: 68.0 Training loss: 8.7565 Explore P: 0.1602\n",
      "Episode: 408 Total reward: 95.0 Training loss: 5.5548 Explore P: 0.1601\n",
      "Episode: 409 Total reward: 61.0 Training loss: 5.3606 Explore P: 0.1596\n",
      "Episode: 410 Total reward: 94.0 Training loss: 12.0223 Explore P: 0.1595\n",
      "Model Saved\n",
      "Episode: 411 Total reward: 68.0 Training loss: 7.8564 Explore P: 0.1591\n",
      "Episode: 412 Total reward: 81.0 Training loss: 4.6955 Explore P: 0.1588\n",
      "Episode: 413 Total reward: 38.0 Training loss: 5.0971 Explore P: 0.1580\n",
      "Episode: 414 Total reward: 78.0 Training loss: 7.2489 Explore P: 0.1576\n",
      "Episode: 415 Total reward: 65.0 Training loss: 4.1180 Explore P: 0.1572\n",
      "Model Saved\n",
      "Episode: 416 Total reward: 90.0 Training loss: 6.3198 Explore P: 0.1570\n",
      "Episode: 417 Total reward: 72.0 Training loss: 6.4250 Explore P: 0.1567\n",
      "Episode: 418 Total reward: 73.0 Training loss: 4.1749 Explore P: 0.1563\n",
      "Episode: 419 Total reward: 63.0 Training loss: 8.9317 Explore P: 0.1558\n",
      "Episode: 420 Total reward: 69.0 Training loss: 6.7037 Explore P: 0.1554\n",
      "Model Saved\n",
      "Episode: 421 Total reward: 50.0 Training loss: 8.8782 Explore P: 0.1549\n",
      "Episode: 422 Total reward: 8.0 Training loss: 5.9122 Explore P: 0.1537\n",
      "Episode: 423 Total reward: 39.0 Training loss: 2.8140 Explore P: 0.1530\n",
      "Episode: 424 Total reward: 38.0 Training loss: 7.8705 Explore P: 0.1522\n",
      "Episode: 425 Total reward: 62.0 Training loss: 7.5732 Explore P: 0.1517\n",
      "Model Saved\n",
      "Episode: 426 Total reward: 95.0 Training loss: 4.4804 Explore P: 0.1517\n",
      "Episode: 427 Total reward: 63.0 Training loss: 6.8661 Explore P: 0.1512\n",
      "Episode: 428 Total reward: 76.0 Training loss: 13.9503 Explore P: 0.1509\n",
      "Episode: 429 Total reward: 95.0 Training loss: 9.9323 Explore P: 0.1508\n",
      "Model Saved\n",
      "Episode: 431 Total reward: 37.0 Training loss: 7.7207 Explore P: 0.1487\n",
      "Episode: 432 Total reward: 71.0 Training loss: 7.2306 Explore P: 0.1483\n",
      "Episode: 433 Total reward: 42.0 Training loss: 4.1633 Explore P: 0.1476\n",
      "Episode: 434 Total reward: 87.0 Training loss: 5.7491 Explore P: 0.1475\n",
      "Episode: 435 Total reward: 76.0 Training loss: 8.3541 Explore P: 0.1472\n",
      "Model Saved\n",
      "Episode: 436 Total reward: 75.0 Training loss: 4.5638 Explore P: 0.1468\n",
      "Episode: 437 Total reward: 48.0 Training loss: 6.7532 Explore P: 0.1462\n",
      "Episode: 438 Total reward: 43.0 Training loss: 5.6754 Explore P: 0.1455\n",
      "Episode: 439 Total reward: 78.0 Training loss: 5.2724 Explore P: 0.1452\n",
      "Episode: 440 Total reward: 66.0 Training loss: 6.1741 Explore P: 0.1448\n",
      "Model Saved\n",
      "Episode: 441 Total reward: 86.0 Training loss: 6.7106 Explore P: 0.1446\n",
      "Episode: 442 Total reward: 57.0 Training loss: 4.7099 Explore P: 0.1441\n",
      "Episode: 443 Total reward: 48.0 Training loss: 6.0892 Explore P: 0.1435\n",
      "Episode: 444 Total reward: 75.0 Training loss: 16.4501 Explore P: 0.1432\n",
      "Episode: 445 Total reward: 94.0 Training loss: 10.3597 Explore P: 0.1431\n",
      "Model Saved\n",
      "Episode: 446 Total reward: 32.0 Training loss: 8.7218 Explore P: 0.1423\n",
      "Episode: 447 Total reward: 29.0 Training loss: 5.7532 Explore P: 0.1415\n",
      "Episode: 448 Total reward: 95.0 Training loss: 9.4912 Explore P: 0.1414\n",
      "Episode: 450 Total reward: 47.0 Training loss: 9.4974 Explore P: 0.1396\n",
      "Model Saved\n",
      "Episode: 451 Total reward: 71.0 Training loss: 5.4947 Explore P: 0.1392\n",
      "Episode: 452 Total reward: 87.0 Training loss: 4.7344 Explore P: 0.1390\n",
      "Episode: 453 Total reward: 76.0 Training loss: 5.8622 Explore P: 0.1387\n",
      "Episode: 454 Total reward: 56.0 Training loss: 8.4874 Explore P: 0.1382\n",
      "Episode: 455 Total reward: 86.0 Training loss: 11.3288 Explore P: 0.1380\n",
      "Model Saved\n",
      "Episode: 456 Total reward: 85.0 Training loss: 6.3962 Explore P: 0.1378\n",
      "Episode: 457 Total reward: 61.0 Training loss: 8.4691 Explore P: 0.1374\n",
      "Episode: 458 Total reward: 74.0 Training loss: 5.6934 Explore P: 0.1371\n",
      "Episode: 459 Total reward: 64.0 Training loss: 5.1455 Explore P: 0.1367\n",
      "Episode: 460 Total reward: 50.0 Training loss: 2.9439 Explore P: 0.1361\n",
      "Model Saved\n",
      "Episode: 461 Total reward: 94.0 Training loss: 3.1670 Explore P: 0.1360\n",
      "Episode: 462 Total reward: 59.0 Training loss: 6.2372 Explore P: 0.1356\n",
      "Episode: 463 Total reward: 80.0 Training loss: 6.0375 Explore P: 0.1353\n",
      "Episode: 464 Total reward: 42.0 Training loss: 7.8792 Explore P: 0.1347\n",
      "Episode: 465 Total reward: 49.0 Training loss: 7.0544 Explore P: 0.1342\n",
      "Model Saved\n",
      "Episode: 466 Total reward: 75.0 Training loss: 5.6253 Explore P: 0.1338\n",
      "Episode: 467 Total reward: 65.0 Training loss: 6.2405 Explore P: 0.1335\n",
      "Episode: 468 Total reward: 94.0 Training loss: 12.6991 Explore P: 0.1334\n",
      "Episode: 469 Total reward: 57.0 Training loss: 11.2309 Explore P: 0.1329\n",
      "Episode: 470 Total reward: 36.0 Training loss: 8.0421 Explore P: 0.1322\n",
      "Model Saved\n",
      "Episode: 471 Total reward: 94.0 Training loss: 6.3715 Explore P: 0.1321\n",
      "Episode: 472 Total reward: 86.0 Training loss: 6.9192 Explore P: 0.1319\n",
      "Episode: 473 Total reward: 95.0 Training loss: 6.0307 Explore P: 0.1319\n",
      "Episode: 474 Total reward: 95.0 Training loss: 4.4046 Explore P: 0.1318\n",
      "Episode: 475 Total reward: 62.0 Training loss: 10.6996 Explore P: 0.1314\n",
      "Model Saved\n",
      "Episode: 476 Total reward: 95.0 Training loss: 5.6881 Explore P: 0.1313\n",
      "Episode: 477 Total reward: 67.0 Training loss: 15.0666 Explore P: 0.1310\n",
      "Episode: 478 Total reward: 85.0 Training loss: 5.7237 Explore P: 0.1308\n",
      "Episode: 479 Total reward: 21.0 Training loss: 9.9742 Explore P: 0.1300\n",
      "Episode: 480 Total reward: 39.0 Training loss: 12.5672 Explore P: 0.1294\n",
      "Model Saved\n",
      "Episode: 481 Total reward: 94.0 Training loss: 3.9774 Explore P: 0.1293\n",
      "Episode: 482 Total reward: 67.0 Training loss: 7.1780 Explore P: 0.1289\n",
      "Episode: 483 Total reward: 55.0 Training loss: 14.1818 Explore P: 0.1284\n",
      "Episode: 484 Total reward: 90.0 Training loss: 6.7346 Explore P: 0.1283\n",
      "Episode: 485 Total reward: 45.0 Training loss: 5.1664 Explore P: 0.1277\n",
      "Model Saved\n",
      "Episode: 486 Total reward: 94.0 Training loss: 6.6384 Explore P: 0.1276\n",
      "Episode: 487 Total reward: 94.0 Training loss: 3.9072 Explore P: 0.1275\n",
      "Episode: 488 Total reward: 65.0 Training loss: 6.2866 Explore P: 0.1271\n",
      "Episode: 489 Total reward: 87.0 Training loss: 9.4599 Explore P: 0.1270\n",
      "Episode: 490 Total reward: 50.0 Training loss: 4.7983 Explore P: 0.1265\n",
      "Model Saved\n",
      "Episode: 491 Total reward: 66.0 Training loss: 5.5463 Explore P: 0.1261\n",
      "Episode: 492 Total reward: 78.0 Training loss: 8.3337 Explore P: 0.1259\n",
      "Episode: 493 Total reward: 46.0 Training loss: 6.3409 Explore P: 0.1254\n",
      "Episode: 494 Total reward: 70.0 Training loss: 5.2546 Explore P: 0.1251\n",
      "Episode: 495 Total reward: 66.0 Training loss: 4.7850 Explore P: 0.1247\n",
      "Model Saved\n",
      "Episode: 496 Total reward: 95.0 Training loss: 3.8787 Explore P: 0.1246\n",
      "Episode: 497 Total reward: 30.0 Training loss: 9.0979 Explore P: 0.1239\n",
      "Episode: 498 Total reward: 72.0 Training loss: 4.2544 Explore P: 0.1237\n",
      "Episode: 499 Total reward: 57.0 Training loss: 4.2083 Explore P: 0.1232\n"
     ]
    }
   ],
   "source": [
    "# Saver will help us to save our model\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "if training == True:\n",
    "    with tf.Session() as sess:\n",
    "        # Initialize the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Initialize the decay rate (that will use to reduce epsilon) \n",
    "        decay_step = 0\n",
    "\n",
    "        # Init the game\n",
    "        game.init()\n",
    "\n",
    "        for episode in range(total_episodes):\n",
    "            # Set step to 0\n",
    "            step = 0\n",
    "            \n",
    "            # Initialize the rewards of the episode\n",
    "            episode_rewards = []\n",
    "            \n",
    "            # Make a new episode and observe the first state\n",
    "            game.new_episode()\n",
    "            state = game.get_state().screen_buffer\n",
    "            \n",
    "            # Remember that stack frame function also call our preprocess function.\n",
    "            state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "\n",
    "            while step < max_steps:\n",
    "                step += 1\n",
    "                \n",
    "                # Increase decay_step\n",
    "                decay_step +=1\n",
    "                \n",
    "                # Predict the action to take and take it\n",
    "                action, explore_probability = predict_action(explore_start, explore_stop, decay_rate, decay_step, state, possible_actions)\n",
    "\n",
    "                # Do the action\n",
    "                reward = game.make_action(action)\n",
    "\n",
    "                # Look if the episode is finished\n",
    "                done = game.is_episode_finished()\n",
    "                \n",
    "                # Add the reward to total reward\n",
    "                episode_rewards.append(reward)\n",
    "\n",
    "                # If the game is finished\n",
    "                if done:\n",
    "                    # the episode ends so no next state\n",
    "                    next_state = np.zeros((84,84), dtype=np.int)\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "\n",
    "                    # Set step = max_steps to end the episode\n",
    "                    step = max_steps\n",
    "\n",
    "                    # Get the total reward of the episode\n",
    "                    total_reward = np.sum(episode_rewards)\n",
    "\n",
    "                    print('Episode: {}'.format(episode),\n",
    "                              'Total reward: {}'.format(total_reward),\n",
    "                              'Training loss: {:.4f}'.format(loss),\n",
    "                              'Explore P: {:.4f}'.format(explore_probability))\n",
    "\n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "\n",
    "                else:\n",
    "                    # Get the next state\n",
    "                    next_state = game.get_state().screen_buffer\n",
    "                    \n",
    "                    # Stack the frame of the next_state\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                    \n",
    "\n",
    "                    # Add experience to memory\n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "                    \n",
    "                    # st+1 is now our current state\n",
    "                    state = next_state\n",
    "\n",
    "\n",
    "                ### LEARNING PART            \n",
    "                # Obtain random mini-batch from memory\n",
    "                batch = memory.sample(batch_size)\n",
    "                states_mb = np.array([each[0] for each in batch], ndmin=3)\n",
    "                actions_mb = np.array([each[1] for each in batch])\n",
    "                rewards_mb = np.array([each[2] for each in batch]) \n",
    "                next_states_mb = np.array([each[3] for each in batch], ndmin=3)\n",
    "                dones_mb = np.array([each[4] for each in batch])\n",
    "\n",
    "                target_Qs_batch = []\n",
    "\n",
    "                 # Get Q values for next_state \n",
    "                Qs_next_state = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: next_states_mb})\n",
    "                \n",
    "                # Set Q_target = r if the episode ends at s+1, otherwise set Q_target = r + gamma*maxQ(s', a')\n",
    "                for i in range(0, len(batch)):\n",
    "                    terminal = dones_mb[i]\n",
    "\n",
    "                    # If we are in a terminal state, only equals reward\n",
    "                    if terminal:\n",
    "                        target_Qs_batch.append(rewards_mb[i])\n",
    "                        \n",
    "                    else:\n",
    "                        target = rewards_mb[i] + gamma * np.max(Qs_next_state[i])\n",
    "                        target_Qs_batch.append(target)\n",
    "                        \n",
    "\n",
    "                targets_mb = np.array([each for each in target_Qs_batch])\n",
    "\n",
    "                loss, _ = sess.run([DQNetwork.loss, DQNetwork.optimizer],\n",
    "                                    feed_dict={DQNetwork.inputs_: states_mb,\n",
    "                                               DQNetwork.target_Q: targets_mb,\n",
    "                                               DQNetwork.actions_: actions_mb})\n",
    "\n",
    "                # Write TF Summaries\n",
    "                summary = sess.run(write_op, feed_dict={DQNetwork.inputs_: states_mb,\n",
    "                                                   DQNetwork.target_Q: targets_mb,\n",
    "                                                   DQNetwork.actions_: actions_mb})\n",
    "                writer.add_summary(summary, episode)\n",
    "                writer.flush()\n",
    "\n",
    "            # Save model every 5 episodes\n",
    "            if episode % 5 == 0:\n",
    "                save_path = saver.save(sess, \"./models/model.ckpt\")\n",
    "                print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Watch our Agent play üëÄ\n",
    "Now that we trained our agent, we can test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T15:23:06.631414Z",
     "start_time": "2021-03-03T15:23:04.808459Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/model.ckpt\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "Score:  86.0\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "Score:  44.0\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "Score:  61.0\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "Score:  58.0\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "Score:  52.0\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "Score:  92.0\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "Score:  44.0\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "Score:  67.0\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "Score:  95.0\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "Score:  64.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    game, possible_actions = create_environment()\n",
    "    \n",
    "    totalScore = 0\n",
    "    \n",
    "    # Load the model\n",
    "    saver.restore(sess, \"./models/model.ckpt\")\n",
    "    game.init()\n",
    "    for i in range(10):\n",
    "        \n",
    "        done = False\n",
    "        \n",
    "        game.new_episode()\n",
    "        \n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "            \n",
    "        while not game.is_episode_finished():\n",
    "            # Take the biggest Q value (= the best action)\n",
    "            Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "            \n",
    "            # Take the biggest Q value (= the best action)\n",
    "            choice = np.argmax(Qs)\n",
    "            action = possible_actions[int(choice)]\n",
    "            \n",
    "            game.make_action(action)\n",
    "            done = game.is_episode_finished()\n",
    "            score = game.get_total_reward()\n",
    "            \n",
    "            if done:\n",
    "                break  \n",
    "                \n",
    "            else:\n",
    "                print(\"else\")\n",
    "                next_state = game.get_state().screen_buffer\n",
    "                next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                state = next_state\n",
    "                \n",
    "        score = game.get_total_reward()\n",
    "        print(\"Score: \", score)\n",
    "    game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
