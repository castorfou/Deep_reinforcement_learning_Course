{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cartpole: REINFORCE Monte Carlo Policy Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we'll implement an agent <b>that plays Cartpole </b>\n",
    "\n",
    "<img src=\"http://neuro-educator.com/wp-content/uploads/2017/09/DQN.gif\" alt=\"Cartpole gif\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a notebook from [Deep Reinforcement Learning Course with Tensorflow](https://simoninithomas.github.io/Deep_reinforcement_learning_Course/)\n",
    "<img src=\"https://raw.githubusercontent.com/simoninithomas/Deep_reinforcement_learning_Course/master/docs/assets/img/DRLC%20Environments.png\" alt=\"Deep Reinforcement Course\"/>\n",
    "<br>\n",
    "<p>  Deep Reinforcement Learning Course is a free series of articles and videos tutorials üÜï about Deep Reinforcement Learning, where **we'll learn the main algorithms (Q-learning, Deep Q Nets, Dueling Deep Q Nets, Policy Gradients, A2C, Proximal Policy Gradients‚Ä¶), and how to implement them with Tensorflow.**\n",
    "<br><br>\n",
    "    \n",
    "üìúThe articles explain the architectures from the big picture to the mathematical details behind them.\n",
    "<br>\n",
    "üìπ The videos explain how to build the agents with Tensorflow </b></p>\n",
    "<br>\n",
    "This course will give you a **solid foundation for understanding and implementing the future state of the art algorithms**. And, you'll build a strong professional portfolio by creating **agents that learn to play awesome environments**: Doom¬© üëπ, Space invaders üëæ, Outrun, Sonic the Hedgehog¬©, Michael Jackson‚Äôs Moonwalker, agents that will be able to navigate in 3D environments with DeepMindLab (Quake) and able to walk with Mujoco. \n",
    "<br><br>\n",
    "</p> \n",
    "\n",
    "## üìö The complete [Syllabus HERE](https://simoninithomas.github.io/Deep_reinforcement_learning_Course/)\n",
    "\n",
    "\n",
    "## Any questions üë®‚Äçüíª\n",
    "<p> If you have any questions, feel free to ask me: </p>\n",
    "<p> üìß: <a href=\"mailto:hello@simoninithomas.com\">hello@simoninithomas.com</a>  </p>\n",
    "<p> Github: https://github.com/simoninithomas/Deep_reinforcement_learning_Course </p>\n",
    "<p> üåê : https://simoninithomas.github.io/Deep_reinforcement_learning_Course/ </p>\n",
    "<p> Twitter: <a href=\"https://twitter.com/ThomasSimonini\">@ThomasSimonini</a> </p>\n",
    "<p> Don't forget to <b> follow me on <a href=\"https://twitter.com/ThomasSimonini\">twitter</a>, <a href=\"https://github.com/simoninithomas/Deep_reinforcement_learning_Course\">github</a> and <a href=\"https://medium.com/@thomassimonini\">Medium</a> to be alerted of the new articles that I publish </b></p>\n",
    "    \n",
    "## How to help  üôå\n",
    "3 ways:\n",
    "- **Clap our articles and like our videos a lot**:Clapping in Medium means that you really like our articles. And the more claps we have, the more our article is shared Liking our videos help them to be much more visible to the deep learning community.\n",
    "- **Share and speak about our articles and videos**: By sharing our articles and videos you help us to spread the word. \n",
    "- **Improve our notebooks**: if you found a bug or **a better implementation** you can send a pull request.\n",
    "<br>\n",
    "\n",
    "## Important note ü§î\n",
    "<b> You can run it on your computer but it's better to run it on GPU based services</b>, personally I use Microsoft Azure and their Deep Learning Virtual Machine (they offer 170$)\n",
    "https://azuremarketplace.microsoft.com/en-us/marketplace/apps/microsoft-ads.dsvm-deep-learning\n",
    "<br>\n",
    "‚ö†Ô∏è I don't have any business relations with them. I just loved their excellent customer service.\n",
    "\n",
    "If you have some troubles to use Microsoft Azure follow the explainations of this excellent article here (without last the part fast.ai): https://medium.com/@manikantayadunanda/setting-up-deeplearning-machine-and-fast-ai-on-azure-a22eb6bd6429"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites üèóÔ∏è\n",
    "Before diving on the notebook **you need to understand**:\n",
    "- The foundations of Reinforcement learning (MC, TD, Rewards hypothesis...) [Article](https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419)\n",
    "- Policy gradients [Article](https://medium.freecodecamp.org/an-introduction-to-policy-gradients-with-cartpole-and-doom-495b5ef2207f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import the libraries üìö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T07:38:14.201033Z",
     "start_time": "2021-03-18T07:38:13.498095Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create our environment üéÆ\n",
    "This time we use <a href=\"https://gym.openai.com/\">OpenAI Gym</a> which has a lot of great environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T07:38:18.202175Z",
     "start_time": "2021-03-18T07:38:18.174228Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env = env.unwrapped\n",
    "# Policy gradient has high variance, seed for reproducability\n",
    "env.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Set up our hyperparameters ‚öóÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T07:38:34.663465Z",
     "start_time": "2021-03-18T07:38:34.658020Z"
    }
   },
   "outputs": [],
   "source": [
    "## ENVIRONMENT Hyperparameters\n",
    "state_size = 4\n",
    "action_size = env.action_space.n\n",
    "\n",
    "## TRAINING Hyperparameters\n",
    "max_episodes = 300\n",
    "learning_rate = 0.01\n",
    "gamma = 0.95 # Discount rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 : Define the preprocessing functions ‚öôÔ∏è\n",
    "This function takes <b>the rewards and perform discounting.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T07:38:43.379066Z",
     "start_time": "2021-03-18T07:38:43.373372Z"
    }
   },
   "outputs": [],
   "source": [
    "def discount_and_normalize_rewards(episode_rewards):\n",
    "    discounted_episode_rewards = np.zeros_like(episode_rewards)\n",
    "    cumulative = 0.0\n",
    "    for i in reversed(range(len(episode_rewards))):\n",
    "        cumulative = cumulative * gamma + episode_rewards[i]\n",
    "        discounted_episode_rewards[i] = cumulative\n",
    "    \n",
    "    mean = np.mean(discounted_episode_rewards)\n",
    "    std = np.std(discounted_episode_rewards)\n",
    "    discounted_episode_rewards = (discounted_episode_rewards - mean) / (std)\n",
    "    \n",
    "    return discounted_episode_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create our Policy Gradient Neural Network model üß†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/simoninithomas/Deep_reinforcement_learning_Course/master/Policy%20Gradients/Cartpole/assets/catpole.png\">\n",
    "\n",
    "The idea is simple:\n",
    "- Our state which is an array of 4 values will be used as an input.\n",
    "- Our NN is 3 fully connected layers.\n",
    "- Our output activation function is softmax that squashes the outputs to a probability distribution (for instance if we have 4, 2, 6 --> softmax --> (0.4, 0.2, 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T07:40:25.149444Z",
     "start_time": "2021-03-18T07:40:24.755446Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/explore/miniconda3/envs/drl_simonini/lib/python3.7/site-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1866: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"inputs\"):\n",
    "    input_ = tf.placeholder(tf.float32, [None, state_size], name=\"input_\")\n",
    "    actions = tf.placeholder(tf.int32, [None, action_size], name=\"actions\")\n",
    "    discounted_episode_rewards_ = tf.placeholder(tf.float32, [None,], name=\"discounted_episode_rewards\")\n",
    "    \n",
    "    # Add this placeholder for having this variable in tensorboard\n",
    "    mean_reward_ = tf.placeholder(tf.float32 , name=\"mean_reward\")\n",
    "\n",
    "    with tf.name_scope(\"fc1\"):\n",
    "        fc1 = tf.contrib.layers.fully_connected(inputs = input_,\n",
    "                                                num_outputs = 10,\n",
    "                                                activation_fn=tf.nn.relu,\n",
    "                                                weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "    with tf.name_scope(\"fc2\"):\n",
    "        fc2 = tf.contrib.layers.fully_connected(inputs = fc1,\n",
    "                                                num_outputs = action_size,\n",
    "                                                activation_fn= tf.nn.relu,\n",
    "                                                weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    with tf.name_scope(\"fc3\"):\n",
    "        fc3 = tf.contrib.layers.fully_connected(inputs = fc2,\n",
    "                                                num_outputs = action_size,\n",
    "                                                activation_fn= None,\n",
    "                                                weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "    with tf.name_scope(\"softmax\"):\n",
    "        action_distribution = tf.nn.softmax(fc3)\n",
    "\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        # tf.nn.softmax_cross_entropy_with_logits computes the cross entropy of the result after applying the softmax function\n",
    "        # If you have single-class labels, where an object can only belong to one class, you might now consider using \n",
    "        # tf.nn.sparse_softmax_cross_entropy_with_logits so that you don't have to convert your labels to a dense one-hot array. \n",
    "        neg_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(logits = fc3, labels = actions)\n",
    "        loss = tf.reduce_mean(neg_log_prob * discounted_episode_rewards_) \n",
    "        \n",
    "    \n",
    "    with tf.name_scope(\"train\"):\n",
    "        train_opt = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Set up Tensorboard üìä\n",
    "For more information about tensorboard, please watch this <a href=\"https://www.youtube.com/embed/eBbEDRsCmv4\">excellent 30min tutorial</a> <br><br>\n",
    "To launch tensorboard : `tensorboard --logdir=./tensorboard/pg/1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T07:40:45.147664Z",
     "start_time": "2021-03-18T07:40:45.138335Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setup TensorBoard Writer\n",
    "writer = tf.summary.FileWriter(\"./tensorboard/pg/1\")\n",
    "\n",
    "## Losses\n",
    "tf.summary.scalar(\"Loss\", loss)\n",
    "\n",
    "## Reward mean\n",
    "tf.summary.scalar(\"Reward_mean\", mean_reward_)\n",
    "\n",
    "write_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Train our Agent üèÉ‚Äç‚ôÇÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the NN\n",
    "maxReward = 0 # Keep track of maximum reward\n",
    "For episode in range(max_episodes):\n",
    "    episode + 1\n",
    "    reset environment\n",
    "    reset stores (states, actions, rewards)\n",
    "    \n",
    "    For each step:\n",
    "        Choose action a\n",
    "        Perform action a\n",
    "        Store s, a, r\n",
    "        If done:\n",
    "            Calculate sum reward\n",
    "            Calculate gamma Gt\n",
    "            Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T07:41:41.316494Z",
     "start_time": "2021-03-18T07:41:29.804966Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Episode:  0\n",
      "Reward:  63.0\n",
      "Mean Reward 63.0\n",
      "Max reward so far:  63.0\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  1\n",
      "Reward:  37.0\n",
      "Mean Reward 50.0\n",
      "Max reward so far:  63.0\n",
      "==========================================\n",
      "Episode:  2\n",
      "Reward:  9.0\n",
      "Mean Reward 36.333333333333336\n",
      "Max reward so far:  63.0\n",
      "==========================================\n",
      "Episode:  3\n",
      "Reward:  15.0\n",
      "Mean Reward 31.0\n",
      "Max reward so far:  63.0\n",
      "==========================================\n",
      "Episode:  4\n",
      "Reward:  49.0\n",
      "Mean Reward 34.6\n",
      "Max reward so far:  63.0\n",
      "==========================================\n",
      "Episode:  5\n",
      "Reward:  64.0\n",
      "Mean Reward 39.5\n",
      "Max reward so far:  64.0\n",
      "==========================================\n",
      "Episode:  6\n",
      "Reward:  46.0\n",
      "Mean Reward 40.42857142857143\n",
      "Max reward so far:  64.0\n",
      "==========================================\n",
      "Episode:  7\n",
      "Reward:  18.0\n",
      "Mean Reward 37.625\n",
      "Max reward so far:  64.0\n",
      "==========================================\n",
      "Episode:  8\n",
      "Reward:  13.0\n",
      "Mean Reward 34.888888888888886\n",
      "Max reward so far:  64.0\n",
      "==========================================\n",
      "Episode:  9\n",
      "Reward:  20.0\n",
      "Mean Reward 33.4\n",
      "Max reward so far:  64.0\n",
      "==========================================\n",
      "Episode:  10\n",
      "Reward:  33.0\n",
      "Mean Reward 33.36363636363637\n",
      "Max reward so far:  64.0\n",
      "==========================================\n",
      "Episode:  11\n",
      "Reward:  12.0\n",
      "Mean Reward 31.583333333333332\n",
      "Max reward so far:  64.0\n",
      "==========================================\n",
      "Episode:  12\n",
      "Reward:  12.0\n",
      "Mean Reward 30.076923076923077\n",
      "Max reward so far:  64.0\n",
      "==========================================\n",
      "Episode:  13\n",
      "Reward:  10.0\n",
      "Mean Reward 28.642857142857142\n",
      "Max reward so far:  64.0\n",
      "==========================================\n",
      "Episode:  14\n",
      "Reward:  21.0\n",
      "Mean Reward 28.133333333333333\n",
      "Max reward so far:  64.0\n",
      "==========================================\n",
      "Episode:  15\n",
      "Reward:  24.0\n",
      "Mean Reward 27.875\n",
      "Max reward so far:  64.0\n",
      "==========================================\n",
      "Episode:  16\n",
      "Reward:  32.0\n",
      "Mean Reward 28.11764705882353\n",
      "Max reward so far:  64.0\n",
      "==========================================\n",
      "Episode:  17\n",
      "Reward:  18.0\n",
      "Mean Reward 27.555555555555557\n",
      "Max reward so far:  64.0\n",
      "==========================================\n",
      "Episode:  18\n",
      "Reward:  17.0\n",
      "Mean Reward 27.0\n",
      "Max reward so far:  64.0\n",
      "==========================================\n",
      "Episode:  19\n",
      "Reward:  24.0\n",
      "Mean Reward 26.85\n",
      "Max reward so far:  64.0\n",
      "==========================================\n",
      "Episode:  20\n",
      "Reward:  26.0\n",
      "Mean Reward 26.80952380952381\n",
      "Max reward so far:  64.0\n",
      "==========================================\n",
      "Episode:  21\n",
      "Reward:  20.0\n",
      "Mean Reward 26.5\n",
      "Max reward so far:  64.0\n",
      "==========================================\n",
      "Episode:  22\n",
      "Reward:  11.0\n",
      "Mean Reward 25.82608695652174\n",
      "Max reward so far:  64.0\n",
      "==========================================\n",
      "Episode:  23\n",
      "Reward:  12.0\n",
      "Mean Reward 25.25\n",
      "Max reward so far:  64.0\n",
      "==========================================\n",
      "Episode:  24\n",
      "Reward:  24.0\n",
      "Mean Reward 25.2\n",
      "Max reward so far:  64.0\n",
      "==========================================\n",
      "Episode:  25\n",
      "Reward:  72.0\n",
      "Mean Reward 27.0\n",
      "Max reward so far:  72.0\n",
      "==========================================\n",
      "Episode:  26\n",
      "Reward:  36.0\n",
      "Mean Reward 27.333333333333332\n",
      "Max reward so far:  72.0\n",
      "==========================================\n",
      "Episode:  27\n",
      "Reward:  17.0\n",
      "Mean Reward 26.964285714285715\n",
      "Max reward so far:  72.0\n",
      "==========================================\n",
      "Episode:  28\n",
      "Reward:  21.0\n",
      "Mean Reward 26.75862068965517\n",
      "Max reward so far:  72.0\n",
      "==========================================\n",
      "Episode:  29\n",
      "Reward:  20.0\n",
      "Mean Reward 26.533333333333335\n",
      "Max reward so far:  72.0\n",
      "==========================================\n",
      "Episode:  30\n",
      "Reward:  29.0\n",
      "Mean Reward 26.612903225806452\n",
      "Max reward so far:  72.0\n",
      "==========================================\n",
      "Episode:  31\n",
      "Reward:  31.0\n",
      "Mean Reward 26.75\n",
      "Max reward so far:  72.0\n",
      "==========================================\n",
      "Episode:  32\n",
      "Reward:  15.0\n",
      "Mean Reward 26.393939393939394\n",
      "Max reward so far:  72.0\n",
      "==========================================\n",
      "Episode:  33\n",
      "Reward:  12.0\n",
      "Mean Reward 25.970588235294116\n",
      "Max reward so far:  72.0\n",
      "==========================================\n",
      "Episode:  34\n",
      "Reward:  13.0\n",
      "Mean Reward 25.6\n",
      "Max reward so far:  72.0\n",
      "==========================================\n",
      "Episode:  35\n",
      "Reward:  29.0\n",
      "Mean Reward 25.694444444444443\n",
      "Max reward so far:  72.0\n",
      "==========================================\n",
      "Episode:  36\n",
      "Reward:  53.0\n",
      "Mean Reward 26.43243243243243\n",
      "Max reward so far:  72.0\n",
      "==========================================\n",
      "Episode:  37\n",
      "Reward:  28.0\n",
      "Mean Reward 26.473684210526315\n",
      "Max reward so far:  72.0\n",
      "==========================================\n",
      "Episode:  38\n",
      "Reward:  31.0\n",
      "Mean Reward 26.58974358974359\n",
      "Max reward so far:  72.0\n",
      "==========================================\n",
      "Episode:  39\n",
      "Reward:  43.0\n",
      "Mean Reward 27.0\n",
      "Max reward so far:  72.0\n",
      "==========================================\n",
      "Episode:  40\n",
      "Reward:  48.0\n",
      "Mean Reward 27.51219512195122\n",
      "Max reward so far:  72.0\n",
      "==========================================\n",
      "Episode:  41\n",
      "Reward:  22.0\n",
      "Mean Reward 27.38095238095238\n",
      "Max reward so far:  72.0\n",
      "==========================================\n",
      "Episode:  42\n",
      "Reward:  17.0\n",
      "Mean Reward 27.13953488372093\n",
      "Max reward so far:  72.0\n",
      "==========================================\n",
      "Episode:  43\n",
      "Reward:  34.0\n",
      "Mean Reward 27.295454545454547\n",
      "Max reward so far:  72.0\n",
      "==========================================\n",
      "Episode:  44\n",
      "Reward:  14.0\n",
      "Mean Reward 27.0\n",
      "Max reward so far:  72.0\n",
      "==========================================\n",
      "Episode:  45\n",
      "Reward:  50.0\n",
      "Mean Reward 27.5\n",
      "Max reward so far:  72.0\n",
      "==========================================\n",
      "Episode:  46\n",
      "Reward:  13.0\n",
      "Mean Reward 27.19148936170213\n",
      "Max reward so far:  72.0\n",
      "==========================================\n",
      "Episode:  47\n",
      "Reward:  21.0\n",
      "Mean Reward 27.0625\n",
      "Max reward so far:  72.0\n",
      "==========================================\n",
      "Episode:  48\n",
      "Reward:  22.0\n",
      "Mean Reward 26.959183673469386\n",
      "Max reward so far:  72.0\n",
      "==========================================\n",
      "Episode:  49\n",
      "Reward:  11.0\n",
      "Mean Reward 26.64\n",
      "Max reward so far:  72.0\n",
      "==========================================\n",
      "Episode:  50\n",
      "Reward:  20.0\n",
      "Mean Reward 26.50980392156863\n",
      "Max reward so far:  72.0\n",
      "==========================================\n",
      "Episode:  51\n",
      "Reward:  27.0\n",
      "Mean Reward 26.51923076923077\n",
      "Max reward so far:  72.0\n",
      "==========================================\n",
      "Episode:  52\n",
      "Reward:  32.0\n",
      "Mean Reward 26.62264150943396\n",
      "Max reward so far:  72.0\n",
      "==========================================\n",
      "Episode:  53\n",
      "Reward:  57.0\n",
      "Mean Reward 27.185185185185187\n",
      "Max reward so far:  72.0\n",
      "==========================================\n",
      "Episode:  54\n",
      "Reward:  25.0\n",
      "Mean Reward 27.145454545454545\n",
      "Max reward so far:  72.0\n",
      "==========================================\n",
      "Episode:  55\n",
      "Reward:  14.0\n",
      "Mean Reward 26.910714285714285\n",
      "Max reward so far:  72.0\n",
      "==========================================\n",
      "Episode:  56\n",
      "Reward:  23.0\n",
      "Mean Reward 26.842105263157894\n",
      "Max reward so far:  72.0\n",
      "==========================================\n",
      "Episode:  57\n",
      "Reward:  16.0\n",
      "Mean Reward 26.655172413793103\n",
      "Max reward so far:  72.0\n",
      "==========================================\n",
      "Episode:  58\n",
      "Reward:  25.0\n",
      "Mean Reward 26.627118644067796\n",
      "Max reward so far:  72.0\n",
      "==========================================\n",
      "Episode:  59\n",
      "Reward:  13.0\n",
      "Mean Reward 26.4\n",
      "Max reward so far:  72.0\n",
      "==========================================\n",
      "Episode:  60\n",
      "Reward:  16.0\n",
      "Mean Reward 26.229508196721312\n",
      "Max reward so far:  72.0\n",
      "==========================================\n",
      "Episode:  61\n",
      "Reward:  39.0\n",
      "Mean Reward 26.43548387096774\n",
      "Max reward so far:  72.0\n",
      "==========================================\n",
      "Episode:  62\n",
      "Reward:  100.0\n",
      "Mean Reward 27.603174603174605\n",
      "Max reward so far:  100.0\n",
      "==========================================\n",
      "Episode:  63\n",
      "Reward:  16.0\n",
      "Mean Reward 27.421875\n",
      "Max reward so far:  100.0\n",
      "==========================================\n",
      "Episode:  64\n",
      "Reward:  44.0\n",
      "Mean Reward 27.676923076923078\n",
      "Max reward so far:  100.0\n",
      "==========================================\n",
      "Episode:  65\n",
      "Reward:  16.0\n",
      "Mean Reward 27.5\n",
      "Max reward so far:  100.0\n",
      "==========================================\n",
      "Episode:  66\n",
      "Reward:  24.0\n",
      "Mean Reward 27.44776119402985\n",
      "Max reward so far:  100.0\n",
      "==========================================\n",
      "Episode:  67\n",
      "Reward:  28.0\n",
      "Mean Reward 27.455882352941178\n",
      "Max reward so far:  100.0\n",
      "==========================================\n",
      "Episode:  68\n",
      "Reward:  14.0\n",
      "Mean Reward 27.26086956521739\n",
      "Max reward so far:  100.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Episode:  69\n",
      "Reward:  24.0\n",
      "Mean Reward 27.214285714285715\n",
      "Max reward so far:  100.0\n",
      "==========================================\n",
      "Episode:  70\n",
      "Reward:  24.0\n",
      "Mean Reward 27.169014084507044\n",
      "Max reward so far:  100.0\n",
      "==========================================\n",
      "Episode:  71\n",
      "Reward:  35.0\n",
      "Mean Reward 27.27777777777778\n",
      "Max reward so far:  100.0\n",
      "==========================================\n",
      "Episode:  72\n",
      "Reward:  13.0\n",
      "Mean Reward 27.08219178082192\n",
      "Max reward so far:  100.0\n",
      "==========================================\n",
      "Episode:  73\n",
      "Reward:  9.0\n",
      "Mean Reward 26.83783783783784\n",
      "Max reward so far:  100.0\n",
      "==========================================\n",
      "Episode:  74\n",
      "Reward:  50.0\n",
      "Mean Reward 27.14666666666667\n",
      "Max reward so far:  100.0\n",
      "==========================================\n",
      "Episode:  75\n",
      "Reward:  10.0\n",
      "Mean Reward 26.92105263157895\n",
      "Max reward so far:  100.0\n",
      "==========================================\n",
      "Episode:  76\n",
      "Reward:  111.0\n",
      "Mean Reward 28.01298701298701\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  77\n",
      "Reward:  26.0\n",
      "Mean Reward 27.987179487179485\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  78\n",
      "Reward:  18.0\n",
      "Mean Reward 27.860759493670887\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  79\n",
      "Reward:  37.0\n",
      "Mean Reward 27.975\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  80\n",
      "Reward:  31.0\n",
      "Mean Reward 28.012345679012345\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  81\n",
      "Reward:  28.0\n",
      "Mean Reward 28.01219512195122\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  82\n",
      "Reward:  39.0\n",
      "Mean Reward 28.14457831325301\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  83\n",
      "Reward:  19.0\n",
      "Mean Reward 28.035714285714285\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  84\n",
      "Reward:  25.0\n",
      "Mean Reward 28.0\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  85\n",
      "Reward:  38.0\n",
      "Mean Reward 28.11627906976744\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  86\n",
      "Reward:  20.0\n",
      "Mean Reward 28.022988505747126\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  87\n",
      "Reward:  29.0\n",
      "Mean Reward 28.03409090909091\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  88\n",
      "Reward:  20.0\n",
      "Mean Reward 27.9438202247191\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  89\n",
      "Reward:  20.0\n",
      "Mean Reward 27.855555555555554\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  90\n",
      "Reward:  21.0\n",
      "Mean Reward 27.78021978021978\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  91\n",
      "Reward:  22.0\n",
      "Mean Reward 27.717391304347824\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  92\n",
      "Reward:  14.0\n",
      "Mean Reward 27.56989247311828\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  93\n",
      "Reward:  55.0\n",
      "Mean Reward 27.861702127659573\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  94\n",
      "Reward:  15.0\n",
      "Mean Reward 27.726315789473684\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  95\n",
      "Reward:  25.0\n",
      "Mean Reward 27.697916666666668\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  96\n",
      "Reward:  20.0\n",
      "Mean Reward 27.61855670103093\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  97\n",
      "Reward:  12.0\n",
      "Mean Reward 27.459183673469386\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  98\n",
      "Reward:  34.0\n",
      "Mean Reward 27.525252525252526\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  99\n",
      "Reward:  10.0\n",
      "Mean Reward 27.35\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  100\n",
      "Reward:  37.0\n",
      "Mean Reward 27.445544554455445\n",
      "Max reward so far:  111.0\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  101\n",
      "Reward:  70.0\n",
      "Mean Reward 27.862745098039216\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  102\n",
      "Reward:  20.0\n",
      "Mean Reward 27.78640776699029\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  103\n",
      "Reward:  35.0\n",
      "Mean Reward 27.85576923076923\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  104\n",
      "Reward:  52.0\n",
      "Mean Reward 28.085714285714285\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  105\n",
      "Reward:  13.0\n",
      "Mean Reward 27.943396226415093\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  106\n",
      "Reward:  50.0\n",
      "Mean Reward 28.149532710280372\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  107\n",
      "Reward:  25.0\n",
      "Mean Reward 28.12037037037037\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  108\n",
      "Reward:  22.0\n",
      "Mean Reward 28.06422018348624\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  109\n",
      "Reward:  63.0\n",
      "Mean Reward 28.381818181818183\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  110\n",
      "Reward:  12.0\n",
      "Mean Reward 28.234234234234233\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  111\n",
      "Reward:  75.0\n",
      "Mean Reward 28.651785714285715\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  112\n",
      "Reward:  12.0\n",
      "Mean Reward 28.504424778761063\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  113\n",
      "Reward:  24.0\n",
      "Mean Reward 28.464912280701753\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  114\n",
      "Reward:  19.0\n",
      "Mean Reward 28.382608695652173\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  115\n",
      "Reward:  16.0\n",
      "Mean Reward 28.275862068965516\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  116\n",
      "Reward:  13.0\n",
      "Mean Reward 28.145299145299145\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  117\n",
      "Reward:  29.0\n",
      "Mean Reward 28.152542372881356\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  118\n",
      "Reward:  14.0\n",
      "Mean Reward 28.03361344537815\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  119\n",
      "Reward:  11.0\n",
      "Mean Reward 27.891666666666666\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  120\n",
      "Reward:  30.0\n",
      "Mean Reward 27.90909090909091\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  121\n",
      "Reward:  18.0\n",
      "Mean Reward 27.827868852459016\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  122\n",
      "Reward:  31.0\n",
      "Mean Reward 27.853658536585368\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  123\n",
      "Reward:  18.0\n",
      "Mean Reward 27.774193548387096\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  124\n",
      "Reward:  20.0\n",
      "Mean Reward 27.712\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  125\n",
      "Reward:  29.0\n",
      "Mean Reward 27.72222222222222\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  126\n",
      "Reward:  17.0\n",
      "Mean Reward 27.637795275590552\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  127\n",
      "Reward:  31.0\n",
      "Mean Reward 27.6640625\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  128\n",
      "Reward:  32.0\n",
      "Mean Reward 27.697674418604652\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  129\n",
      "Reward:  16.0\n",
      "Mean Reward 27.607692307692307\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  130\n",
      "Reward:  38.0\n",
      "Mean Reward 27.68702290076336\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  131\n",
      "Reward:  15.0\n",
      "Mean Reward 27.59090909090909\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  132\n",
      "Reward:  10.0\n",
      "Mean Reward 27.458646616541355\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  133\n",
      "Reward:  26.0\n",
      "Mean Reward 27.44776119402985\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  134\n",
      "Reward:  62.0\n",
      "Mean Reward 27.703703703703702\n",
      "Max reward so far:  111.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Episode:  135\n",
      "Reward:  38.0\n",
      "Mean Reward 27.779411764705884\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  136\n",
      "Reward:  33.0\n",
      "Mean Reward 27.817518248175183\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  137\n",
      "Reward:  33.0\n",
      "Mean Reward 27.855072463768117\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  138\n",
      "Reward:  20.0\n",
      "Mean Reward 27.798561151079138\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  139\n",
      "Reward:  46.0\n",
      "Mean Reward 27.928571428571427\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  140\n",
      "Reward:  29.0\n",
      "Mean Reward 27.93617021276596\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  141\n",
      "Reward:  28.0\n",
      "Mean Reward 27.93661971830986\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  142\n",
      "Reward:  43.0\n",
      "Mean Reward 28.041958041958043\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  143\n",
      "Reward:  56.0\n",
      "Mean Reward 28.23611111111111\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  144\n",
      "Reward:  14.0\n",
      "Mean Reward 28.137931034482758\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  145\n",
      "Reward:  20.0\n",
      "Mean Reward 28.08219178082192\n",
      "Max reward so far:  111.0\n",
      "==========================================\n",
      "Episode:  146\n",
      "Reward:  127.0\n",
      "Mean Reward 28.755102040816325\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  147\n",
      "Reward:  36.0\n",
      "Mean Reward 28.804054054054053\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  148\n",
      "Reward:  26.0\n",
      "Mean Reward 28.78523489932886\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  149\n",
      "Reward:  52.0\n",
      "Mean Reward 28.94\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  150\n",
      "Reward:  14.0\n",
      "Mean Reward 28.841059602649008\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  151\n",
      "Reward:  107.0\n",
      "Mean Reward 29.355263157894736\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  152\n",
      "Reward:  12.0\n",
      "Mean Reward 29.241830065359476\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  153\n",
      "Reward:  14.0\n",
      "Mean Reward 29.142857142857142\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  154\n",
      "Reward:  21.0\n",
      "Mean Reward 29.09032258064516\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  155\n",
      "Reward:  23.0\n",
      "Mean Reward 29.05128205128205\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  156\n",
      "Reward:  34.0\n",
      "Mean Reward 29.0828025477707\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  157\n",
      "Reward:  29.0\n",
      "Mean Reward 29.082278481012658\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  158\n",
      "Reward:  15.0\n",
      "Mean Reward 28.9937106918239\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  159\n",
      "Reward:  19.0\n",
      "Mean Reward 28.93125\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  160\n",
      "Reward:  21.0\n",
      "Mean Reward 28.88198757763975\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  161\n",
      "Reward:  60.0\n",
      "Mean Reward 29.074074074074073\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  162\n",
      "Reward:  19.0\n",
      "Mean Reward 29.012269938650306\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  163\n",
      "Reward:  24.0\n",
      "Mean Reward 28.98170731707317\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  164\n",
      "Reward:  38.0\n",
      "Mean Reward 29.036363636363635\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  165\n",
      "Reward:  123.0\n",
      "Mean Reward 29.602409638554217\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  166\n",
      "Reward:  49.0\n",
      "Mean Reward 29.718562874251496\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  167\n",
      "Reward:  19.0\n",
      "Mean Reward 29.654761904761905\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  168\n",
      "Reward:  11.0\n",
      "Mean Reward 29.54437869822485\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  169\n",
      "Reward:  66.0\n",
      "Mean Reward 29.758823529411764\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  170\n",
      "Reward:  12.0\n",
      "Mean Reward 29.65497076023392\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  171\n",
      "Reward:  39.0\n",
      "Mean Reward 29.709302325581394\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  172\n",
      "Reward:  41.0\n",
      "Mean Reward 29.77456647398844\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  173\n",
      "Reward:  55.0\n",
      "Mean Reward 29.919540229885058\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  174\n",
      "Reward:  41.0\n",
      "Mean Reward 29.982857142857142\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  175\n",
      "Reward:  23.0\n",
      "Mean Reward 29.943181818181817\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  176\n",
      "Reward:  64.0\n",
      "Mean Reward 30.135593220338983\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  177\n",
      "Reward:  26.0\n",
      "Mean Reward 30.1123595505618\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  178\n",
      "Reward:  36.0\n",
      "Mean Reward 30.145251396648046\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  179\n",
      "Reward:  13.0\n",
      "Mean Reward 30.05\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  180\n",
      "Reward:  14.0\n",
      "Mean Reward 29.96132596685083\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  181\n",
      "Reward:  46.0\n",
      "Mean Reward 30.04945054945055\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  182\n",
      "Reward:  77.0\n",
      "Mean Reward 30.306010928961747\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  183\n",
      "Reward:  40.0\n",
      "Mean Reward 30.358695652173914\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  184\n",
      "Reward:  21.0\n",
      "Mean Reward 30.308108108108108\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  185\n",
      "Reward:  20.0\n",
      "Mean Reward 30.252688172043012\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  186\n",
      "Reward:  32.0\n",
      "Mean Reward 30.262032085561497\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  187\n",
      "Reward:  22.0\n",
      "Mean Reward 30.21808510638298\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  188\n",
      "Reward:  17.0\n",
      "Mean Reward 30.14814814814815\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  189\n",
      "Reward:  12.0\n",
      "Mean Reward 30.05263157894737\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  190\n",
      "Reward:  29.0\n",
      "Mean Reward 30.047120418848166\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  191\n",
      "Reward:  26.0\n",
      "Mean Reward 30.026041666666668\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  192\n",
      "Reward:  78.0\n",
      "Mean Reward 30.27461139896373\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  193\n",
      "Reward:  8.0\n",
      "Mean Reward 30.15979381443299\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  194\n",
      "Reward:  24.0\n",
      "Mean Reward 30.128205128205128\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  195\n",
      "Reward:  10.0\n",
      "Mean Reward 30.025510204081634\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  196\n",
      "Reward:  21.0\n",
      "Mean Reward 29.97969543147208\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  197\n",
      "Reward:  14.0\n",
      "Mean Reward 29.8989898989899\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  198\n",
      "Reward:  25.0\n",
      "Mean Reward 29.87437185929648\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  199\n",
      "Reward:  15.0\n",
      "Mean Reward 29.8\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  200\n",
      "Reward:  17.0\n",
      "Mean Reward 29.736318407960198\n",
      "Max reward so far:  127.0\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  201\n",
      "Reward:  41.0\n",
      "Mean Reward 29.792079207920793\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  202\n",
      "Reward:  30.0\n",
      "Mean Reward 29.79310344827586\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  203\n",
      "Reward:  22.0\n",
      "Mean Reward 29.754901960784313\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  204\n",
      "Reward:  22.0\n",
      "Mean Reward 29.71707317073171\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  205\n",
      "Reward:  30.0\n",
      "Mean Reward 29.718446601941746\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  206\n",
      "Reward:  40.0\n",
      "Mean Reward 29.768115942028984\n",
      "Max reward so far:  127.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Episode:  207\n",
      "Reward:  69.0\n",
      "Mean Reward 29.95673076923077\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  208\n",
      "Reward:  63.0\n",
      "Mean Reward 30.114832535885167\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  209\n",
      "Reward:  13.0\n",
      "Mean Reward 30.033333333333335\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  210\n",
      "Reward:  18.0\n",
      "Mean Reward 29.976303317535546\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  211\n",
      "Reward:  20.0\n",
      "Mean Reward 29.92924528301887\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  212\n",
      "Reward:  34.0\n",
      "Mean Reward 29.948356807511736\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  213\n",
      "Reward:  27.0\n",
      "Mean Reward 29.934579439252335\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  214\n",
      "Reward:  39.0\n",
      "Mean Reward 29.976744186046513\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  215\n",
      "Reward:  74.0\n",
      "Mean Reward 30.180555555555557\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  216\n",
      "Reward:  22.0\n",
      "Mean Reward 30.142857142857142\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  217\n",
      "Reward:  41.0\n",
      "Mean Reward 30.192660550458715\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  218\n",
      "Reward:  39.0\n",
      "Mean Reward 30.232876712328768\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  219\n",
      "Reward:  16.0\n",
      "Mean Reward 30.168181818181818\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  220\n",
      "Reward:  23.0\n",
      "Mean Reward 30.13574660633484\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  221\n",
      "Reward:  45.0\n",
      "Mean Reward 30.2027027027027\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  222\n",
      "Reward:  26.0\n",
      "Mean Reward 30.183856502242154\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  223\n",
      "Reward:  51.0\n",
      "Mean Reward 30.276785714285715\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  224\n",
      "Reward:  109.0\n",
      "Mean Reward 30.626666666666665\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  225\n",
      "Reward:  98.0\n",
      "Mean Reward 30.924778761061948\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  226\n",
      "Reward:  25.0\n",
      "Mean Reward 30.898678414096917\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  227\n",
      "Reward:  18.0\n",
      "Mean Reward 30.842105263157894\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  228\n",
      "Reward:  23.0\n",
      "Mean Reward 30.807860262008735\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  229\n",
      "Reward:  28.0\n",
      "Mean Reward 30.795652173913044\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  230\n",
      "Reward:  51.0\n",
      "Mean Reward 30.883116883116884\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  231\n",
      "Reward:  48.0\n",
      "Mean Reward 30.95689655172414\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  232\n",
      "Reward:  18.0\n",
      "Mean Reward 30.901287553648068\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  233\n",
      "Reward:  24.0\n",
      "Mean Reward 30.871794871794872\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  234\n",
      "Reward:  88.0\n",
      "Mean Reward 31.114893617021277\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  235\n",
      "Reward:  41.0\n",
      "Mean Reward 31.156779661016948\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  236\n",
      "Reward:  71.0\n",
      "Mean Reward 31.324894514767934\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  237\n",
      "Reward:  81.0\n",
      "Mean Reward 31.53361344537815\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  238\n",
      "Reward:  54.0\n",
      "Mean Reward 31.627615062761507\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  239\n",
      "Reward:  58.0\n",
      "Mean Reward 31.7375\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  240\n",
      "Reward:  39.0\n",
      "Mean Reward 31.767634854771785\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  241\n",
      "Reward:  17.0\n",
      "Mean Reward 31.706611570247933\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  242\n",
      "Reward:  18.0\n",
      "Mean Reward 31.650205761316872\n",
      "Max reward so far:  127.0\n",
      "==========================================\n",
      "Episode:  243\n",
      "Reward:  144.0\n",
      "Mean Reward 32.11065573770492\n",
      "Max reward so far:  144.0\n",
      "==========================================\n",
      "Episode:  244\n",
      "Reward:  43.0\n",
      "Mean Reward 32.155102040816324\n",
      "Max reward so far:  144.0\n",
      "==========================================\n",
      "Episode:  245\n",
      "Reward:  44.0\n",
      "Mean Reward 32.203252032520325\n",
      "Max reward so far:  144.0\n",
      "==========================================\n",
      "Episode:  246\n",
      "Reward:  29.0\n",
      "Mean Reward 32.19028340080972\n",
      "Max reward so far:  144.0\n",
      "==========================================\n",
      "Episode:  247\n",
      "Reward:  74.0\n",
      "Mean Reward 32.358870967741936\n",
      "Max reward so far:  144.0\n",
      "==========================================\n",
      "Episode:  248\n",
      "Reward:  189.0\n",
      "Mean Reward 32.98795180722892\n",
      "Max reward so far:  189.0\n",
      "==========================================\n",
      "Episode:  249\n",
      "Reward:  26.0\n",
      "Mean Reward 32.96\n",
      "Max reward so far:  189.0\n",
      "==========================================\n",
      "Episode:  250\n",
      "Reward:  35.0\n",
      "Mean Reward 32.96812749003984\n",
      "Max reward so far:  189.0\n",
      "==========================================\n",
      "Episode:  251\n",
      "Reward:  161.0\n",
      "Mean Reward 33.476190476190474\n",
      "Max reward so far:  189.0\n",
      "==========================================\n",
      "Episode:  252\n",
      "Reward:  63.0\n",
      "Mean Reward 33.59288537549407\n",
      "Max reward so far:  189.0\n",
      "==========================================\n",
      "Episode:  253\n",
      "Reward:  18.0\n",
      "Mean Reward 33.531496062992126\n",
      "Max reward so far:  189.0\n",
      "==========================================\n",
      "Episode:  254\n",
      "Reward:  195.0\n",
      "Mean Reward 34.16470588235294\n",
      "Max reward so far:  195.0\n",
      "==========================================\n",
      "Episode:  255\n",
      "Reward:  155.0\n",
      "Mean Reward 34.63671875\n",
      "Max reward so far:  195.0\n",
      "==========================================\n",
      "Episode:  256\n",
      "Reward:  34.0\n",
      "Mean Reward 34.63424124513619\n",
      "Max reward so far:  195.0\n",
      "==========================================\n",
      "Episode:  257\n",
      "Reward:  62.0\n",
      "Mean Reward 34.74031007751938\n",
      "Max reward so far:  195.0\n",
      "==========================================\n",
      "Episode:  258\n",
      "Reward:  28.0\n",
      "Mean Reward 34.714285714285715\n",
      "Max reward so far:  195.0\n",
      "==========================================\n",
      "Episode:  259\n",
      "Reward:  79.0\n",
      "Mean Reward 34.88461538461539\n",
      "Max reward so far:  195.0\n",
      "==========================================\n",
      "Episode:  260\n",
      "Reward:  104.0\n",
      "Mean Reward 35.14942528735632\n",
      "Max reward so far:  195.0\n",
      "==========================================\n",
      "Episode:  261\n",
      "Reward:  98.0\n",
      "Mean Reward 35.38931297709924\n",
      "Max reward so far:  195.0\n",
      "==========================================\n",
      "Episode:  262\n",
      "Reward:  70.0\n",
      "Mean Reward 35.52091254752852\n",
      "Max reward so far:  195.0\n",
      "==========================================\n",
      "Episode:  263\n",
      "Reward:  118.0\n",
      "Mean Reward 35.833333333333336\n",
      "Max reward so far:  195.0\n",
      "==========================================\n",
      "Episode:  264\n",
      "Reward:  102.0\n",
      "Mean Reward 36.08301886792453\n",
      "Max reward so far:  195.0\n",
      "==========================================\n",
      "Episode:  265\n",
      "Reward:  11.0\n",
      "Mean Reward 35.98872180451128\n",
      "Max reward so far:  195.0\n",
      "==========================================\n",
      "Episode:  266\n",
      "Reward:  115.0\n",
      "Mean Reward 36.28464419475655\n",
      "Max reward so far:  195.0\n",
      "==========================================\n",
      "Episode:  267\n",
      "Reward:  174.0\n",
      "Mean Reward 36.798507462686565\n",
      "Max reward so far:  195.0\n",
      "==========================================\n",
      "Episode:  268\n",
      "Reward:  171.0\n",
      "Mean Reward 37.29739776951673\n",
      "Max reward so far:  195.0\n",
      "==========================================\n",
      "Episode:  269\n",
      "Reward:  138.0\n",
      "Mean Reward 37.67037037037037\n",
      "Max reward so far:  195.0\n",
      "==========================================\n",
      "Episode:  270\n",
      "Reward:  85.0\n",
      "Mean Reward 37.845018450184504\n",
      "Max reward so far:  195.0\n",
      "==========================================\n",
      "Episode:  271\n",
      "Reward:  316.0\n",
      "Mean Reward 38.86764705882353\n",
      "Max reward so far:  316.0\n",
      "==========================================\n",
      "Episode:  272\n",
      "Reward:  260.0\n",
      "Mean Reward 39.67765567765568\n",
      "Max reward so far:  316.0\n",
      "==========================================\n",
      "Episode:  273\n",
      "Reward:  56.0\n",
      "Mean Reward 39.737226277372265\n",
      "Max reward so far:  316.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Episode:  274\n",
      "Reward:  115.0\n",
      "Mean Reward 40.01090909090909\n",
      "Max reward so far:  316.0\n",
      "==========================================\n",
      "Episode:  275\n",
      "Reward:  189.0\n",
      "Mean Reward 40.55072463768116\n",
      "Max reward so far:  316.0\n",
      "==========================================\n",
      "Episode:  276\n",
      "Reward:  152.0\n",
      "Mean Reward 40.95306859205776\n",
      "Max reward so far:  316.0\n",
      "==========================================\n",
      "Episode:  277\n",
      "Reward:  452.0\n",
      "Mean Reward 42.431654676258994\n",
      "Max reward so far:  452.0\n",
      "==========================================\n",
      "Episode:  278\n",
      "Reward:  173.0\n",
      "Mean Reward 42.89964157706093\n",
      "Max reward so far:  452.0\n",
      "==========================================\n",
      "Episode:  279\n",
      "Reward:  46.0\n",
      "Mean Reward 42.910714285714285\n",
      "Max reward so far:  452.0\n",
      "==========================================\n",
      "Episode:  280\n",
      "Reward:  778.0\n",
      "Mean Reward 45.52669039145908\n",
      "Max reward so far:  778.0\n",
      "==========================================\n",
      "Episode:  281\n",
      "Reward:  425.0\n",
      "Mean Reward 46.87234042553192\n",
      "Max reward so far:  778.0\n",
      "==========================================\n",
      "Episode:  282\n",
      "Reward:  149.0\n",
      "Mean Reward 47.23321554770318\n",
      "Max reward so far:  778.0\n",
      "==========================================\n",
      "Episode:  283\n",
      "Reward:  100.0\n",
      "Mean Reward 47.41901408450704\n",
      "Max reward so far:  778.0\n",
      "==========================================\n",
      "Episode:  284\n",
      "Reward:  87.0\n",
      "Mean Reward 47.55789473684211\n",
      "Max reward so far:  778.0\n",
      "==========================================\n",
      "Episode:  285\n",
      "Reward:  72.0\n",
      "Mean Reward 47.64335664335665\n",
      "Max reward so far:  778.0\n",
      "==========================================\n",
      "Episode:  286\n",
      "Reward:  131.0\n",
      "Mean Reward 47.933797909407666\n",
      "Max reward so far:  778.0\n",
      "==========================================\n",
      "Episode:  287\n",
      "Reward:  68.0\n",
      "Mean Reward 48.00347222222222\n",
      "Max reward so far:  778.0\n",
      "==========================================\n",
      "Episode:  288\n",
      "Reward:  87.0\n",
      "Mean Reward 48.13840830449827\n",
      "Max reward so far:  778.0\n",
      "==========================================\n",
      "Episode:  289\n",
      "Reward:  174.0\n",
      "Mean Reward 48.57241379310345\n",
      "Max reward so far:  778.0\n",
      "==========================================\n",
      "Episode:  290\n",
      "Reward:  143.0\n",
      "Mean Reward 48.896907216494846\n",
      "Max reward so far:  778.0\n",
      "==========================================\n",
      "Episode:  291\n",
      "Reward:  139.0\n",
      "Mean Reward 49.205479452054796\n",
      "Max reward so far:  778.0\n",
      "==========================================\n",
      "Episode:  292\n",
      "Reward:  127.0\n",
      "Mean Reward 49.47098976109215\n",
      "Max reward so far:  778.0\n",
      "==========================================\n",
      "Episode:  293\n",
      "Reward:  116.0\n",
      "Mean Reward 49.697278911564624\n",
      "Max reward so far:  778.0\n",
      "==========================================\n",
      "Episode:  294\n",
      "Reward:  39.0\n",
      "Mean Reward 49.66101694915254\n",
      "Max reward so far:  778.0\n",
      "==========================================\n",
      "Episode:  295\n",
      "Reward:  48.0\n",
      "Mean Reward 49.6554054054054\n",
      "Max reward so far:  778.0\n",
      "==========================================\n",
      "Episode:  296\n",
      "Reward:  94.0\n",
      "Mean Reward 49.804713804713806\n",
      "Max reward so far:  778.0\n",
      "==========================================\n",
      "Episode:  297\n",
      "Reward:  131.0\n",
      "Mean Reward 50.077181208053695\n",
      "Max reward so far:  778.0\n",
      "==========================================\n",
      "Episode:  298\n",
      "Reward:  90.0\n",
      "Mean Reward 50.21070234113712\n",
      "Max reward so far:  778.0\n",
      "==========================================\n",
      "Episode:  299\n",
      "Reward:  115.0\n",
      "Mean Reward 50.42666666666667\n",
      "Max reward so far:  778.0\n"
     ]
    }
   ],
   "source": [
    "allRewards = []\n",
    "total_rewards = 0\n",
    "maximumRewardRecorded = 0\n",
    "episode = 0\n",
    "episode_states, episode_actions, episode_rewards = [],[],[]\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for episode in range(max_episodes):\n",
    "        \n",
    "        episode_rewards_sum = 0\n",
    "\n",
    "        # Launch the game\n",
    "        state = env.reset()\n",
    "        \n",
    "        env.render()\n",
    "           \n",
    "        while True:\n",
    "            \n",
    "            # Choose action a, remember WE'RE NOT IN A DETERMINISTIC ENVIRONMENT, WE'RE OUTPUT PROBABILITIES.\n",
    "            action_probability_distribution = sess.run(action_distribution, feed_dict={input_: state.reshape([1,4])})\n",
    "            \n",
    "            action = np.random.choice(range(action_probability_distribution.shape[1]), p=action_probability_distribution.ravel())  # select action w.r.t the actions prob\n",
    "\n",
    "            # Perform a\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "\n",
    "            # Store s, a, r\n",
    "            episode_states.append(state)\n",
    "                        \n",
    "            # For actions because we output only one (the index) we need 2 (1 is for the action taken)\n",
    "            # We need [0., 1.] (if we take right) not just the index\n",
    "            action_ = np.zeros(action_size)\n",
    "            action_[action] = 1\n",
    "            \n",
    "            episode_actions.append(action_)\n",
    "            \n",
    "            episode_rewards.append(reward)\n",
    "            if done:\n",
    "                # Calculate sum reward\n",
    "                episode_rewards_sum = np.sum(episode_rewards)\n",
    "                \n",
    "                allRewards.append(episode_rewards_sum)\n",
    "                \n",
    "                total_rewards = np.sum(allRewards)\n",
    "                \n",
    "                # Mean reward\n",
    "                mean_reward = np.divide(total_rewards, episode+1)\n",
    "                \n",
    "                \n",
    "                maximumRewardRecorded = np.amax(allRewards)\n",
    "                \n",
    "                print(\"==========================================\")\n",
    "                print(\"Episode: \", episode)\n",
    "                print(\"Reward: \", episode_rewards_sum)\n",
    "                print(\"Mean Reward\", mean_reward)\n",
    "                print(\"Max reward so far: \", maximumRewardRecorded)\n",
    "                \n",
    "                # Calculate discounted reward\n",
    "                discounted_episode_rewards = discount_and_normalize_rewards(episode_rewards)\n",
    "                                \n",
    "                # Feedforward, gradient and backpropagation\n",
    "                loss_, _ = sess.run([loss, train_opt], feed_dict={input_: np.vstack(np.array(episode_states)),\n",
    "                                                                 actions: np.vstack(np.array(episode_actions)),\n",
    "                                                                 discounted_episode_rewards_: discounted_episode_rewards \n",
    "                                                                })\n",
    "                \n",
    " \n",
    "                                                                 \n",
    "                # Write TF Summaries\n",
    "                summary = sess.run(write_op, feed_dict={input_: np.vstack(np.array(episode_states)),\n",
    "                                                                 actions: np.vstack(np.array(episode_actions)),\n",
    "                                                                 discounted_episode_rewards_: discounted_episode_rewards,\n",
    "                                                                    mean_reward_: mean_reward\n",
    "                                                                })\n",
    "                \n",
    "               \n",
    "                writer.add_summary(summary, episode)\n",
    "                writer.flush()\n",
    "                \n",
    "            \n",
    "                \n",
    "                # Reset the transition stores\n",
    "                episode_states, episode_actions, episode_rewards = [],[],[]\n",
    "                \n",
    "                break\n",
    "            \n",
    "            state = new_state\n",
    "        \n",
    "        # Save Model\n",
    "        if episode % 100 == 0:\n",
    "            saver.save(sess, \"./models/model.ckpt\")\n",
    "            print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T07:42:11.126990Z",
     "start_time": "2021-03-18T07:42:10.936838Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/model.ckpt\n",
      "****************************************************\n",
      "EPISODE  0\n",
      "Score 20.0\n",
      "****************************************************\n",
      "EPISODE  1\n",
      "Score 18.0\n",
      "****************************************************\n",
      "EPISODE  2\n",
      "Score 28.0\n",
      "****************************************************\n",
      "EPISODE  3\n",
      "Score 19.0\n",
      "****************************************************\n",
      "EPISODE  4\n",
      "Score 13.0\n",
      "****************************************************\n",
      "EPISODE  5\n",
      "Score 46.0\n",
      "****************************************************\n",
      "EPISODE  6\n",
      "Score 55.0\n",
      "****************************************************\n",
      "EPISODE  7\n",
      "Score 16.0\n",
      "****************************************************\n",
      "EPISODE  8\n",
      "Score 17.0\n",
      "****************************************************\n",
      "EPISODE  9\n",
      "Score 9.0\n",
      "Score over time: 24.1\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    env.reset()\n",
    "    rewards = []\n",
    "    \n",
    "    # Load the model\n",
    "    saver.restore(sess, \"./models/model.ckpt\")\n",
    "\n",
    "    for episode in range(10):\n",
    "        state = env.reset()\n",
    "        step = 0\n",
    "        done = False\n",
    "        total_rewards = 0\n",
    "        print(\"****************************************************\")\n",
    "        print(\"EPISODE \", episode)\n",
    "\n",
    "        while True:\n",
    "            \n",
    "\n",
    "            # Choose action a, remember WE'RE NOT IN A DETERMINISTIC ENVIRONMENT, WE'RE OUTPUT PROBABILITIES.\n",
    "            action_probability_distribution = sess.run(action_distribution, feed_dict={input_: state.reshape([1,4])})\n",
    "            #print(action_probability_distribution)\n",
    "            action = np.random.choice(range(action_probability_distribution.shape[1]), p=action_probability_distribution.ravel())  # select action w.r.t the actions prob\n",
    "\n",
    "\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "\n",
    "            total_rewards += reward\n",
    "\n",
    "            if done:\n",
    "                rewards.append(total_rewards)\n",
    "                print (\"Score\", total_rewards)\n",
    "                break\n",
    "            state = new_state\n",
    "    env.close()\n",
    "    print (\"Score over time: \" +  str(sum(rewards)/10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:drl_simonini] *",
   "language": "python",
   "name": "conda-env-drl_simonini-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
