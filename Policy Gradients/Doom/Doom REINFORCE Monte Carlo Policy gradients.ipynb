{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doom-Health: REINFORCE Monte Carlo Policy gradients üïπÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we'll implement an agent <b>that try to survive in Doom environment by using a Policy Gradient architecture.</b> <br>\n",
    "Our agent playing Doom:\n",
    "\n",
    "<img src=\"assets/projectw4.gif\" style=\"max-width: 600px;\" alt=\"Policy Gradient with Doom\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You can follow this notebook with this video tutorial üìπ that will helps you to understand each step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T07:51:07.086591Z",
     "start_time": "2021-03-18T07:51:07.075377Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/explore/miniconda3/envs/drl_simonini/lib/python3.7/site-packages/IPython/core/display.py:717: UserWarning: Consider using IPython.display.IFrame instead\n",
      "  warnings.warn(\"Consider using IPython.display.IFrame instead\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/wLTQRuizVyE?showinfo=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/wLTQRuizVyE?showinfo=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a notebook from [Deep Reinforcement Learning Course with Tensorflow](https://simoninithomas.github.io/Deep_reinforcement_learning_Course/)\n",
    "<img src=\"https://raw.githubusercontent.com/simoninithomas/Deep_reinforcement_learning_Course/master/docs/assets/img/DRLC%20Environments.png\" alt=\"Deep Reinforcement Course\"/>\n",
    "<br>\n",
    "<p>  Deep Reinforcement Learning Course is a free series of articles and videos tutorials üÜï about Deep Reinforcement Learning, where **we'll learn the main algorithms (Q-learning, Deep Q Nets, Dueling Deep Q Nets, Policy Gradients, A2C, Proximal Policy Gradients‚Ä¶), and how to implement them with Tensorflow.**\n",
    "<br><br>\n",
    "    \n",
    "üìúThe articles explain the architectures from the big picture to the mathematical details behind them.\n",
    "<br>\n",
    "üìπ The videos explain how to build the agents with Tensorflow </b></p>\n",
    "<br>\n",
    "This course will give you a **solid foundation for understanding and implementing the future state of the art algorithms**. And, you'll build a strong professional portfolio by creating **agents that learn to play awesome environments**: Doom¬© üëπ, Space invaders üëæ, Outrun, Sonic the Hedgehog¬©, Michael Jackson‚Äôs Moonwalker, agents that will be able to navigate in 3D environments with DeepMindLab (Quake) and able to walk with Mujoco. \n",
    "<br><br>\n",
    "</p> \n",
    "\n",
    "## üìö The complete [Syllabus HERE](https://simoninithomas.github.io/Deep_reinforcement_learning_Course/)\n",
    "\n",
    "\n",
    "## Any questions üë®‚Äçüíª\n",
    "<p> If you have any questions, feel free to ask me: </p>\n",
    "<p> üìß: <a href=\"mailto:hello@simoninithomas.com\">hello@simoninithomas.com</a>  </p>\n",
    "<p> Github: https://github.com/simoninithomas/Deep_reinforcement_learning_Course </p>\n",
    "<p> üåê : https://simoninithomas.github.io/Deep_reinforcement_learning_Course/ </p>\n",
    "<p> Twitter: <a href=\"https://twitter.com/ThomasSimonini\">@ThomasSimonini</a> </p>\n",
    "<p> Don't forget to <b> follow me on <a href=\"https://twitter.com/ThomasSimonini\">twitter</a>, <a href=\"https://github.com/simoninithomas/Deep_reinforcement_learning_Course\">github</a> and <a href=\"https://medium.com/@thomassimonini\">Medium</a> to be alerted of the new articles that I publish </b></p>\n",
    "    \n",
    "## How to help  üôå\n",
    "3 ways:\n",
    "- **Clap our articles and like our videos a lot**:Clapping in Medium means that you really like our articles. And the more claps we have, the more our article is shared Liking our videos help them to be much more visible to the deep learning community.\n",
    "- **Share and speak about our articles and videos**: By sharing our articles and videos you help us to spread the word. \n",
    "- **Improve our notebooks**: if you found a bug or **a better implementation** you can send a pull request.\n",
    "<br>\n",
    "\n",
    "## Important note ü§î\n",
    "<b> You can run it on your computer but it's better to run it on GPU based services</b>, personally I use Microsoft Azure and their Deep Learning Virtual Machine (they offer 170$)\n",
    "https://azuremarketplace.microsoft.com/en-us/marketplace/apps/microsoft-ads.dsvm-deep-learning\n",
    "<br>\n",
    "‚ö†Ô∏è I don't have any business relations with them. I just loved their excellent customer service.\n",
    "\n",
    "If you have some troubles to use Microsoft Azure follow the explainations of this excellent article here (without last the part fast.ai): https://medium.com/@manikantayadunanda/setting-up-deeplearning-machine-and-fast-ai-on-azure-a22eb6bd6429"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites üèóÔ∏è\n",
    "Before diving on the notebook **you need to understand**:\n",
    "- The foundations of Reinforcement learning (MC, TD, Rewards hypothesis...) [Article](https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419)\n",
    "- Policy gradients [Article](https://medium.freecodecamp.org/an-introduction-to-policy-gradients-with-cartpole-and-doom-495b5ef2207f)\n",
    "- We made a [tutorial video](https://youtu.be/wLTQRuizVyE) where we implement a Policy Gradient agent with Tensorflow that learns to play Doom üëπüî´ in a Deathmatch environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import the libraries üìö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T07:51:08.128234Z",
     "start_time": "2021-03-18T07:51:07.088163Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf      # Deep Learning library\n",
    "import numpy as np           # Handle matrices\n",
    "from vizdoom import *        # Doom Environment\n",
    "import random                # Handling random number generation\n",
    "import time                  # Handling time calculation\n",
    "from skimage import transform# Help us to preprocess the frames\n",
    "\n",
    "from collections import deque# Ordered collection with ends\n",
    "import matplotlib.pyplot as plt # Display graphs\n",
    "\n",
    "import warnings # This ignore all the warning messages that are normally printed during the training because of skiimage\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T07:51:08.131483Z",
     "start_time": "2021-03-18T07:51:08.129389Z"
    }
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create our environment üéÆ\n",
    "- Now that we imported the libraries/dependencies, we will create our environment.\n",
    "- Doom environment takes:\n",
    "    - A `configuration file` that **handle all the options** (size of the frame, possible actions...)\n",
    "    - A `scenario file`: that **generates the correct scenario** (in our case basic **but you're invited to try other scenarios**).\n",
    "- Note: We have 3 possible actions `[[0,0,1], [1,0,0], [0,1,0]]` so we don't need to do one hot encoding (thanks to < a href=\"https://stackoverflow.com/users/2237916/silgon\">silgon</a> for figuring out. \n",
    "\n",
    "### Our environment\n",
    "<img src=\"assets/health_doom.jpg\" style=\"max-width:500px;\" alt=\"Doom health\"/>\n",
    "\n",
    "The purpose of this scenario is to teach the agent **how to survive without knowing what makes him survive.** Agent know only that life is precious and death is bad so he must learn what prolongs his existence and that his health is connected with it.\n",
    "\n",
    "Map is a rectangle with green, acidic floor which hurts the player periodically. Initially there are some medkits spread uniformly over the map. A new medkit falls from the skies every now and then. **Medkits heal some portions of player's health - to survive agent needs to pick them up. Episode finishes after player's death or on timeout.**\n",
    "\n",
    "Further configuration:\n",
    "\n",
    "- living_reward = 1\n",
    "- 3 available buttons: turn left, turn right, move forward\n",
    "- 1 available game variable: HEALTH\n",
    "- death penalty = 100\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T07:51:08.140062Z",
     "start_time": "2021-03-18T07:51:08.132478Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here we create our environment\n",
    "\"\"\"\n",
    "def create_environment():\n",
    "    game = DoomGame()\n",
    "    \n",
    "    # Load the correct configuration\n",
    "    game.load_config(\"health_gathering.cfg\")\n",
    "    \n",
    "    # Load the correct scenario (in our case defend_the_center scenario)\n",
    "    game.set_doom_scenario_path(\"health_gathering.wad\")\n",
    "    \n",
    "    game.init()\n",
    "    \n",
    "    # Here our possible actions\n",
    "    # [[1,0,0],[0,1,0],[0,0,1]]\n",
    "    possible_actions  = np.identity(3,dtype=int).tolist()\n",
    "    \n",
    "    return game, possible_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T07:51:08.887536Z",
     "start_time": "2021-03-18T07:51:08.141038Z"
    }
   },
   "outputs": [],
   "source": [
    "game, possible_actions = create_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define the preprocessing functions ‚öôÔ∏è\n",
    "### preprocess_frame üñºÔ∏è\n",
    "Preprocessing is an important step, <b>because we want to reduce the complexity of our states to reduce the computation time needed for training.</b>\n",
    "<br><br>\n",
    "Our steps:\n",
    "- Grayscale each of our frames (because <b> color does not add important information </b>). But this is already done by the config file.\n",
    "- Crop the screen (in our case we remove the roof because it contains no information)\n",
    "- We normalize pixel values\n",
    "- Finally we resize the preprocessed frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T07:51:08.891988Z",
     "start_time": "2021-03-18T07:51:08.888736Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    preprocess_frame:\n",
    "    Take a frame.\n",
    "    Resize it.\n",
    "        __________________\n",
    "        |                 |\n",
    "        |                 |\n",
    "        |                 |\n",
    "        |                 |\n",
    "        |_________________|\n",
    "        \n",
    "        to\n",
    "        _____________\n",
    "        |            |\n",
    "        |            |\n",
    "        |            |\n",
    "        |____________|\n",
    "    Normalize it.\n",
    "    \n",
    "    return preprocessed_frame\n",
    "    \n",
    "    \"\"\"\n",
    "def preprocess_frame(frame):\n",
    "    # Greyscale frame already done in our vizdoom config\n",
    "    # x = np.mean(frame,-1)\n",
    "    \n",
    "    # Crop the screen (remove the roof because it contains no information)\n",
    "    # [Up: Down, Left: right]\n",
    "    cropped_frame = frame[80:,:]\n",
    "    \n",
    "    # Normalize Pixel Values\n",
    "    normalized_frame = cropped_frame/255.0\n",
    "    \n",
    "    # Resize\n",
    "    preprocessed_frame = transform.resize(normalized_frame, [84,84])\n",
    "    \n",
    "    return preprocessed_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stack_frames\n",
    "üëè This part was made possible thanks to help of <a href=\"https://github.com/Miffyli\">Anssi</a><br>\n",
    "\n",
    "As explained in this really <a href=\"https://danieltakeshi.github.io/2016/11/25/frame-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games/\">  good article </a> we stack frames.\n",
    "\n",
    "Stacking frames is really important because it helps us to **give have a sense of motion to our Neural Network.**\n",
    "\n",
    "- First we preprocess frame\n",
    "- Then we append the frame to the deque that automatically **removes the oldest frame**\n",
    "- Finally we **build the stacked state**\n",
    "\n",
    "This is how work stack:\n",
    "- For the first frame, we feed 4 frames\n",
    "- At each timestep, **we add the new frame to deque and then we stack them to form a new stacked frame**\n",
    "- And so on\n",
    "<img src=\"https://raw.githubusercontent.com/simoninithomas/Deep_reinforcement_learning_Course/master/DQN/Space%20Invaders/assets/stack_frames.png\" alt=\"stack\">\n",
    "- If we're done, **we create a new stack with 4 new frames (because we are in a new episode)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T07:51:08.903694Z",
     "start_time": "2021-03-18T07:51:08.893154Z"
    }
   },
   "outputs": [],
   "source": [
    "stack_size = 4 # We stack 4 frames\n",
    "\n",
    "# Initialize deque with zero-images one array for each image\n",
    "stacked_frames  =  deque([np.zeros((84,84), dtype=np.int) for i in range(stack_size)], maxlen=4) \n",
    "\n",
    "def stack_frames(stacked_frames, state, is_new_episode):\n",
    "    # Preprocess frame\n",
    "    frame = preprocess_frame(state)\n",
    "    \n",
    "    if is_new_episode:\n",
    "        # Clear our stacked_frames\n",
    "        stacked_frames = deque([np.zeros((84,84), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "        \n",
    "        # Because we're in a new episode, copy the same frame 4x\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        \n",
    "        # Stack the frames\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "\n",
    "    else:\n",
    "        # Append frame to deque, automatically removes the oldest frame\n",
    "        stacked_frames.append(frame)\n",
    "\n",
    "        # Build the stacked state (first dimension specifies different frames)\n",
    "        stacked_state = np.stack(stacked_frames, axis=2) \n",
    "    \n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### discount_and_normalize_rewards üí∞\n",
    "This function is important, because we are in a Monte Carlo situation. <br>\n",
    "\n",
    "We need to **discount the rewards at the end of the episode**. This function takes, the reward discount it, and **then normalize them** (to avoid a big variability in rewards)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T07:51:08.909711Z",
     "start_time": "2021-03-18T07:51:08.905883Z"
    }
   },
   "outputs": [],
   "source": [
    "def discount_and_normalize_rewards(episode_rewards):\n",
    "    discounted_episode_rewards = np.zeros_like(episode_rewards)\n",
    "    cumulative = 0.0\n",
    "    for i in reversed(range(len(episode_rewards))):\n",
    "        cumulative = cumulative * gamma + episode_rewards[i]\n",
    "        discounted_episode_rewards[i] = cumulative\n",
    "    \n",
    "    mean = np.mean(discounted_episode_rewards)\n",
    "    std = np.std(discounted_episode_rewards)\n",
    "    discounted_episode_rewards = (discounted_episode_rewards - mean) / (std)\n",
    "\n",
    "    return discounted_episode_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Set up our hyperparameters ‚öóÔ∏è\n",
    "In this part we'll set up our different hyperparameters. But when you implement a Neural Network by yourself you will **not implement hyperparamaters at once but progressively**.\n",
    "\n",
    "- First, you begin by defining the neural networks hyperparameters when you implement the model.\n",
    "- Then, you'll add the training hyperparameters when you implement the training algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T07:51:08.917159Z",
     "start_time": "2021-03-18T07:51:08.910998Z"
    }
   },
   "outputs": [],
   "source": [
    "### ENVIRONMENT HYPERPARAMETERS\n",
    "state_size = [84,84,4] # Our input is a stack of 4 frames hence 84x84x4 (Width, height, channels) \n",
    "action_size = game.get_available_buttons_size() # 3 possible actions: turn left, turn right, move forward\n",
    "stack_size = 4 # Defines how many frames are stacked together\n",
    "\n",
    "## TRAINING HYPERPARAMETERS\n",
    "learning_rate = 0.002\n",
    "num_epochs = 500 # Total epochs for training \n",
    "\n",
    "batch_size = 1000 # Each 1 is a timestep (NOT AN EPISODE) # YOU CAN CHANGE TO 5000 if you have GPU\n",
    "gamma = 0.95 # Discounting rate\n",
    "\n",
    "### MODIFY THIS TO FALSE IF YOU JUST WANT TO SEE THE TRAINED AGENT\n",
    "training = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick note: Policy gradient methods like reinforce **are on-policy method which can not be updated from experience replay.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create our Policy Gradient Neural Network model üß†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/doomPG.png\" alt=\"Doom PG\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T07:51:08.928800Z",
     "start_time": "2021-03-18T07:51:08.918334Z"
    }
   },
   "outputs": [],
   "source": [
    "class PGNetwork:\n",
    "    def __init__(self, state_size, action_size, learning_rate, name='PGNetwork'):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        with tf.variable_scope(name):\n",
    "            with tf.name_scope(\"inputs\"):\n",
    "                # We create the placeholders\n",
    "                # *state_size means that we take each elements of state_size in tuple hence is like if we wrote\n",
    "                # [None, 84, 84, 4]\n",
    "                self.inputs_= tf.placeholder(tf.float32, [None, *state_size], name=\"inputs_\")\n",
    "                self.actions = tf.placeholder(tf.int32, [None, action_size], name=\"actions\")\n",
    "                self.discounted_episode_rewards_ = tf.placeholder(tf.float32, [None, ], name=\"discounted_episode_rewards_\")\n",
    "            \n",
    "                \n",
    "                # Add this placeholder for having this variable in tensorboard\n",
    "                self.mean_reward_ = tf.placeholder(tf.float32, name=\"mean_reward\")\n",
    "                \n",
    "            with tf.name_scope(\"conv1\"):\n",
    "                \"\"\"\n",
    "                First convnet:\n",
    "                CNN\n",
    "                BatchNormalization\n",
    "                ELU\n",
    "                \"\"\"\n",
    "                # Input is 84x84x4\n",
    "                self.conv1 = tf.layers.conv2d(inputs = self.inputs_,\n",
    "                                             filters = 32,\n",
    "                                             kernel_size = [8,8],\n",
    "                                             strides = [4,4],\n",
    "                                             padding = \"VALID\",\n",
    "                                              kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                             name = \"conv1\")\n",
    "\n",
    "                self.conv1_batchnorm = tf.layers.batch_normalization(self.conv1,\n",
    "                                                       training = True,\n",
    "                                                       epsilon = 1e-5,\n",
    "                                                         name = 'batch_norm1')\n",
    "\n",
    "                self.conv1_out = tf.nn.elu(self.conv1_batchnorm, name=\"conv1_out\")\n",
    "                ## --> [20, 20, 32]\n",
    "            \n",
    "            with tf.name_scope(\"conv2\"):\n",
    "                \"\"\"\n",
    "                Second convnet:\n",
    "                CNN\n",
    "                BatchNormalization\n",
    "                ELU\n",
    "                \"\"\"\n",
    "                self.conv2 = tf.layers.conv2d(inputs = self.conv1_out,\n",
    "                                     filters = 64,\n",
    "                                     kernel_size = [4,4],\n",
    "                                     strides = [2,2],\n",
    "                                     padding = \"VALID\",\n",
    "                                    kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                     name = \"conv2\")\n",
    "\n",
    "                self.conv2_batchnorm = tf.layers.batch_normalization(self.conv2,\n",
    "                                                       training = True,\n",
    "                                                       epsilon = 1e-5,\n",
    "                                                         name = 'batch_norm2')\n",
    "\n",
    "                self.conv2_out = tf.nn.elu(self.conv2_batchnorm, name=\"conv2_out\")\n",
    "                ## --> [9, 9, 64]\n",
    "            \n",
    "            with tf.name_scope(\"conv3\"):\n",
    "                \"\"\"\n",
    "                Third convnet:\n",
    "                CNN\n",
    "                BatchNormalization\n",
    "                ELU\n",
    "                \"\"\"\n",
    "                self.conv3 = tf.layers.conv2d(inputs = self.conv2_out,\n",
    "                                     filters = 128,\n",
    "                                     kernel_size = [4,4],\n",
    "                                     strides = [2,2],\n",
    "                                     padding = \"VALID\",\n",
    "                                    kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                     name = \"conv3\")\n",
    "\n",
    "                self.conv3_batchnorm = tf.layers.batch_normalization(self.conv3,\n",
    "                                                       training = True,\n",
    "                                                       epsilon = 1e-5,\n",
    "                                                         name = 'batch_norm3')\n",
    "\n",
    "                self.conv3_out = tf.nn.elu(self.conv3_batchnorm, name=\"conv3_out\")\n",
    "                ## --> [3, 3, 128]\n",
    "            \n",
    "            with tf.name_scope(\"flatten\"):\n",
    "                self.flatten = tf.layers.flatten(self.conv3_out)\n",
    "                ## --> [1152]\n",
    "            \n",
    "            with tf.name_scope(\"fc1\"):\n",
    "                self.fc = tf.layers.dense(inputs = self.flatten,\n",
    "                                      units = 512,\n",
    "                                      activation = tf.nn.elu,\n",
    "                                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                    name=\"fc1\")\n",
    "            \n",
    "            with tf.name_scope(\"logits\"):\n",
    "                self.logits = tf.layers.dense(inputs = self.fc, \n",
    "                                               kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                              units = 3, \n",
    "                                            activation=None)\n",
    "            \n",
    "            with tf.name_scope(\"softmax\"):\n",
    "                self.action_distribution = tf.nn.softmax(self.logits)\n",
    "                \n",
    "\n",
    "            with tf.name_scope(\"loss\"):\n",
    "                # tf.nn.softmax_cross_entropy_with_logits computes the cross entropy of the result after applying the softmax function\n",
    "                # If you have single-class labels, where an object can only belong to one class, you might now consider using \n",
    "                # tf.nn.sparse_softmax_cross_entropy_with_logits so that you don't have to convert your labels to a dense one-hot array. \n",
    "                self.neg_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(logits = self.logits, labels = self.actions)\n",
    "                self.loss = tf.reduce_mean(self.neg_log_prob * self.discounted_episode_rewards_) \n",
    "        \n",
    "    \n",
    "            with tf.name_scope(\"train\"):\n",
    "                self.train_opt = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T07:51:09.982076Z",
     "start_time": "2021-03-18T07:51:08.930036Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-10-6c75c112285e>:34: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "WARNING:tensorflow:From /home/explore/miniconda3/envs/drl_simonini/lib/python3.7/site-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From <ipython-input-10-6c75c112285e>:39: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
      "WARNING:tensorflow:From <ipython-input-10-6c75c112285e>:91: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From <ipython-input-10-6c75c112285e>:99: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /home/explore/miniconda3/envs/drl_simonini/lib/python3.7/site-packages/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# Reset the graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Instantiate the PGNetwork\n",
    "PGNetwork = PGNetwork(state_size, action_size, learning_rate)\n",
    "\n",
    "# Initialize Session\n",
    "# sess = tf.Session()\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Set up Tensorboard üìä\n",
    "For more information about tensorboard, please watch this <a href=\"https://www.youtube.com/embed/eBbEDRsCmv4\">excellent 30min tutorial</a> <br><br>\n",
    "To launch tensorboard : `tensorboard --logdir=./tensorboard/pg/test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T07:51:09.988191Z",
     "start_time": "2021-03-18T07:51:09.982993Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setup TensorBoard Writer\n",
    "writer = tf.summary.FileWriter(\"./tensorboard/pg/test\")\n",
    "\n",
    "## Losses\n",
    "tf.summary.scalar(\"Loss\", PGNetwork.loss)\n",
    "\n",
    "## Reward mean\n",
    "tf.summary.scalar(\"Reward_mean\", PGNetwork.mean_reward_ )\n",
    "\n",
    "write_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Train our Agent üèÉ‚Äç‚ôÇÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll create batches.<br>\n",
    "These batches contains episodes **(their number depends on how many rewards we collect**: for instance if we have episodes with only 10 rewards we can put batch_size/10 episodes\n",
    "<br>\n",
    "* Make a batch\n",
    "    * For each step:\n",
    "        * Choose action a\n",
    "        * Perform action a\n",
    "        * Store s, a, r\n",
    "        * **If** done:\n",
    "            * Calculate sum reward\n",
    "            * Calculate gamma Gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T07:51:10.000540Z",
     "start_time": "2021-03-18T07:51:09.989194Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_batch(batch_size, stacked_frames):\n",
    "    # Initialize lists: states, actions, rewards_of_episode, rewards_of_batch, discounted_rewards\n",
    "    states, actions, rewards_of_episode, rewards_of_batch, discounted_rewards = [], [], [], [], []\n",
    "    \n",
    "    # Reward of batch is also a trick to keep track of how many timestep we made.\n",
    "    # We use to to verify at the end of each episode if > batch_size or not.\n",
    "    \n",
    "    # Keep track of how many episodes in our batch (useful when we'll need to calculate the average reward per episode)\n",
    "    episode_num  = 1\n",
    "    \n",
    "    # Launch a new episode\n",
    "    game.new_episode()\n",
    "        \n",
    "    # Get a new state\n",
    "    state = game.get_state().screen_buffer\n",
    "    state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "\n",
    "    while True:\n",
    "        # Run State Through Policy & Calculate Action\n",
    "        action_probability_distribution = sess.run(PGNetwork.action_distribution, \n",
    "                                                   feed_dict={PGNetwork.inputs_: state.reshape(1, *state_size)})\n",
    "        \n",
    "        # REMEMBER THAT WE ARE IN A STOCHASTIC POLICY SO WE DON'T ALWAYS TAKE THE ACTION WITH THE HIGHEST PROBABILITY\n",
    "        # (For instance if the action with the best probability for state S is a1 with 70% chances, there is\n",
    "        #30% chance that we take action a2)\n",
    "        action = np.random.choice(range(action_probability_distribution.shape[1]), \n",
    "                                  p=action_probability_distribution.ravel())  # select action w.r.t the actions prob\n",
    "        action = possible_actions[action]\n",
    "\n",
    "        # Perform action\n",
    "        reward = game.make_action(action)\n",
    "        done = game.is_episode_finished()\n",
    "\n",
    "        # Store results\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards_of_episode.append(reward)\n",
    "        \n",
    "        if done:\n",
    "            # The episode ends so no next state\n",
    "            next_state = np.zeros((84, 84), dtype=np.int)\n",
    "            next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "            \n",
    "            # Append the rewards_of_batch to reward_of_episode\n",
    "            rewards_of_batch.append(rewards_of_episode)\n",
    "            \n",
    "            # Calculate gamma Gt\n",
    "            discounted_rewards.append(discount_and_normalize_rewards(rewards_of_episode))\n",
    "           \n",
    "            # If the number of rewards_of_batch > batch_size stop the minibatch creation\n",
    "            # (Because we have sufficient number of episode mb)\n",
    "            # Remember that we put this condition here, because we want entire episode (Monte Carlo)\n",
    "            # so we can't check that condition for each step but only if an episode is finished\n",
    "            if len(np.concatenate(rewards_of_batch)) > batch_size:\n",
    "                break\n",
    "                \n",
    "            # Reset the transition stores\n",
    "            rewards_of_episode = []\n",
    "            \n",
    "            # Add episode\n",
    "            episode_num += 1\n",
    "            \n",
    "            # Start a new episode\n",
    "            game.new_episode()\n",
    "\n",
    "            # First we need a state\n",
    "            state = game.get_state().screen_buffer\n",
    "\n",
    "            # Stack the frames\n",
    "            state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "         \n",
    "        else:\n",
    "            # If not done, the next_state become the current state\n",
    "            next_state = game.get_state().screen_buffer\n",
    "            next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "            state = next_state\n",
    "                         \n",
    "    return np.stack(np.array(states)), np.stack(np.array(actions)), np.concatenate(rewards_of_batch), np.concatenate(discounted_rewards), episode_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create the Neural Network\n",
    "* Initialize the weights\n",
    "* Init the environment\n",
    "* maxReward = 0 # Keep track of maximum reward\n",
    "* **For** epochs in range(num_epochs):\n",
    "    * Get batches\n",
    "    * Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T08:40:03.556132Z",
     "start_time": "2021-03-18T07:51:10.001685Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Epoch:  1 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 284.0\n",
      "Max reward for a batch so far: 852.0\n",
      "Training Loss: -0.0255582258105278\n",
      "==========================================\n",
      "Epoch:  2 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 856.0\n",
      "Mean Reward of that batch 428.0\n",
      "Average Reward of all training: 356.0\n",
      "Max reward for a batch so far: 856.0\n",
      "Training Loss: 0.018531018868088722\n",
      "==========================================\n",
      "Epoch:  3 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 948.0\n",
      "Mean Reward of that batch 316.0\n",
      "Average Reward of all training: 342.6666666666667\n",
      "Max reward for a batch so far: 948.0\n",
      "Training Loss: -0.012900994159281254\n",
      "==========================================\n",
      "Epoch:  4 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 328.0\n",
      "Max reward for a batch so far: 948.0\n",
      "Training Loss: 7.052553701214492e-05\n",
      "==========================================\n",
      "Epoch:  5 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 980.0\n",
      "Mean Reward of that batch 326.6666666666667\n",
      "Average Reward of all training: 327.73333333333335\n",
      "Max reward for a batch so far: 980.0\n",
      "Training Loss: -0.037577323615550995\n",
      "==========================================\n",
      "Epoch:  6 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1268.0\n",
      "Mean Reward of that batch 422.6666666666667\n",
      "Average Reward of all training: 343.5555555555556\n",
      "Max reward for a batch so far: 1268.0\n",
      "Training Loss: 0.04607958719134331\n",
      "==========================================\n",
      "Epoch:  7 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1204.0\n",
      "Mean Reward of that batch 401.3333333333333\n",
      "Average Reward of all training: 351.80952380952385\n",
      "Max reward for a batch so far: 1268.0\n",
      "Training Loss: 0.00873906072229147\n",
      "==========================================\n",
      "Epoch:  8 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 948.0\n",
      "Mean Reward of that batch 316.0\n",
      "Average Reward of all training: 347.3333333333333\n",
      "Max reward for a batch so far: 1268.0\n",
      "Training Loss: 0.017371835187077522\n",
      "==========================================\n",
      "Epoch:  9 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 948.0\n",
      "Mean Reward of that batch 316.0\n",
      "Average Reward of all training: 343.85185185185185\n",
      "Max reward for a batch so far: 1268.0\n",
      "Training Loss: 0.0009388403850607574\n",
      "==========================================\n",
      "Epoch:  10 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 948.0\n",
      "Mean Reward of that batch 316.0\n",
      "Average Reward of all training: 341.06666666666666\n",
      "Max reward for a batch so far: 1268.0\n",
      "Training Loss: -0.0033764929976314306\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  11 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1048.0\n",
      "Mean Reward of that batch 524.0\n",
      "Average Reward of all training: 357.6969696969697\n",
      "Max reward for a batch so far: 1268.0\n",
      "Training Loss: 0.037602994590997696\n",
      "==========================================\n",
      "Epoch:  12 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1140.0\n",
      "Mean Reward of that batch 380.0\n",
      "Average Reward of all training: 359.5555555555555\n",
      "Max reward for a batch so far: 1268.0\n",
      "Training Loss: 0.010624609887599945\n",
      "==========================================\n",
      "Epoch:  13 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 980.0\n",
      "Mean Reward of that batch 326.6666666666667\n",
      "Average Reward of all training: 357.025641025641\n",
      "Max reward for a batch so far: 1268.0\n",
      "Training Loss: 0.0014465302228927612\n",
      "==========================================\n",
      "Epoch:  14 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 948.0\n",
      "Mean Reward of that batch 316.0\n",
      "Average Reward of all training: 354.0952380952381\n",
      "Max reward for a batch so far: 1268.0\n",
      "Training Loss: -0.02769368700683117\n",
      "==========================================\n",
      "Epoch:  15 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1044.0\n",
      "Mean Reward of that batch 348.0\n",
      "Average Reward of all training: 353.68888888888887\n",
      "Max reward for a batch so far: 1268.0\n",
      "Training Loss: 3.7176268961047754e-05\n",
      "==========================================\n",
      "Epoch:  16 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 948.0\n",
      "Mean Reward of that batch 316.0\n",
      "Average Reward of all training: 351.3333333333333\n",
      "Max reward for a batch so far: 1268.0\n",
      "Training Loss: 0.007237027864903212\n",
      "==========================================\n",
      "Epoch:  17 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1364.0\n",
      "Mean Reward of that batch 454.6666666666667\n",
      "Average Reward of all training: 357.4117647058824\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.024593055248260498\n",
      "==========================================\n",
      "Epoch:  18 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 856.0\n",
      "Mean Reward of that batch 428.0\n",
      "Average Reward of all training: 361.3333333333333\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.023131312802433968\n",
      "==========================================\n",
      "Epoch:  19 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1108.0\n",
      "Mean Reward of that batch 369.3333333333333\n",
      "Average Reward of all training: 361.7543859649123\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.002342495135962963\n",
      "==========================================\n",
      "Epoch:  20 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 948.0\n",
      "Mean Reward of that batch 316.0\n",
      "Average Reward of all training: 359.46666666666664\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.008658256381750107\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  21 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1172.0\n",
      "Mean Reward of that batch 390.6666666666667\n",
      "Average Reward of all training: 360.95238095238096\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.0007061958312988281\n",
      "==========================================\n",
      "Epoch:  22 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1304.0\n",
      "Mean Reward of that batch 652.0\n",
      "Average Reward of all training: 374.1818181818182\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: -0.026170402765274048\n",
      "==========================================\n",
      "Epoch:  23 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1044.0\n",
      "Mean Reward of that batch 348.0\n",
      "Average Reward of all training: 373.04347826086956\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.019602807238698006\n",
      "==========================================\n",
      "Epoch:  24 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 984.0\n",
      "Mean Reward of that batch 492.0\n",
      "Average Reward of all training: 378.0\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: -0.012652480974793434\n",
      "==========================================\n",
      "Epoch:  25 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 888.0\n",
      "Mean Reward of that batch 444.0\n",
      "Average Reward of all training: 380.64\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: -0.013363529928028584\n",
      "==========================================\n",
      "Epoch:  26 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 856.0\n",
      "Mean Reward of that batch 428.0\n",
      "Average Reward of all training: 382.46153846153845\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.014929740689694881\n",
      "==========================================\n",
      "Epoch:  27 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1044.0\n",
      "Mean Reward of that batch 348.0\n",
      "Average Reward of all training: 381.18518518518516\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: -0.004026929382234812\n",
      "==========================================\n",
      "Epoch:  28 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 888.0\n",
      "Mean Reward of that batch 444.0\n",
      "Average Reward of all training: 383.42857142857144\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.029483893886208534\n",
      "==========================================\n",
      "Epoch:  29 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1140.0\n",
      "Mean Reward of that batch 380.0\n",
      "Average Reward of all training: 383.3103448275862\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.0074762399308383465\n",
      "==========================================\n",
      "Epoch:  30 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1108.0\n",
      "Mean Reward of that batch 369.3333333333333\n",
      "Average Reward of all training: 382.8444444444445\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: -0.04484986886382103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n",
      "==========================================\n",
      "Epoch:  31 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 856.0\n",
      "Mean Reward of that batch 428.0\n",
      "Average Reward of all training: 384.3010752688172\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: -0.02592526376247406\n",
      "==========================================\n",
      "Epoch:  32 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1300.0\n",
      "Mean Reward of that batch 433.3333333333333\n",
      "Average Reward of all training: 385.8333333333333\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: -0.0021361398976296186\n",
      "==========================================\n",
      "Epoch:  33 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1364.0\n",
      "Mean Reward of that batch 454.6666666666667\n",
      "Average Reward of all training: 387.91919191919186\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: -0.03599080815911293\n",
      "==========================================\n",
      "Epoch:  34 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 856.0\n",
      "Mean Reward of that batch 428.0\n",
      "Average Reward of all training: 389.09803921568624\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.04902006313204765\n",
      "==========================================\n",
      "Epoch:  35 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 888.0\n",
      "Mean Reward of that batch 444.0\n",
      "Average Reward of all training: 390.66666666666663\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.009712308645248413\n",
      "==========================================\n",
      "Epoch:  36 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1080.0\n",
      "Mean Reward of that batch 540.0\n",
      "Average Reward of all training: 394.8148148148148\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.030545245856046677\n",
      "==========================================\n",
      "Epoch:  37 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 391.81981981981977\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: -0.05363204702734947\n",
      "==========================================\n",
      "Epoch:  38 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 956.0\n",
      "Mean Reward of that batch 956.0\n",
      "Average Reward of all training: 406.66666666666663\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: -0.07427946478128433\n",
      "==========================================\n",
      "Epoch:  39 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1044.0\n",
      "Mean Reward of that batch 348.0\n",
      "Average Reward of all training: 405.16239316239313\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: -0.013963639736175537\n",
      "==========================================\n",
      "Epoch:  40 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1236.0\n",
      "Mean Reward of that batch 412.0\n",
      "Average Reward of all training: 405.3333333333333\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.05099085345864296\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  41 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1048.0\n",
      "Mean Reward of that batch 524.0\n",
      "Average Reward of all training: 408.2276422764227\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.013991029001772404\n",
      "==========================================\n",
      "Epoch:  42 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1044.0\n",
      "Mean Reward of that batch 348.0\n",
      "Average Reward of all training: 406.7936507936508\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: -0.015193194150924683\n",
      "==========================================\n",
      "Epoch:  43 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1044.0\n",
      "Mean Reward of that batch 348.0\n",
      "Average Reward of all training: 405.42635658914725\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: -0.02224854566156864\n",
      "==========================================\n",
      "Epoch:  44 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1112.0\n",
      "Mean Reward of that batch 556.0\n",
      "Average Reward of all training: 408.8484848484848\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: -0.0774216279387474\n",
      "==========================================\n",
      "Epoch:  45 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 888.0\n",
      "Mean Reward of that batch 444.0\n",
      "Average Reward of all training: 409.6296296296296\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: -0.001757224672473967\n",
      "==========================================\n",
      "Epoch:  46 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 884.0\n",
      "Mean Reward of that batch 294.6666666666667\n",
      "Average Reward of all training: 407.1304347826087\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.0106890257447958\n",
      "==========================================\n",
      "Epoch:  47 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1044.0\n",
      "Mean Reward of that batch 348.0\n",
      "Average Reward of all training: 405.8723404255319\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.021794838830828667\n",
      "==========================================\n",
      "Epoch:  48 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 403.3333333333333\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: -0.0736260712146759\n",
      "==========================================\n",
      "Epoch:  49 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 948.0\n",
      "Mean Reward of that batch 316.0\n",
      "Average Reward of all training: 401.55102040816325\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.0180110614746809\n",
      "==========================================\n",
      "Epoch:  50 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1140.0\n",
      "Mean Reward of that batch 380.0\n",
      "Average Reward of all training: 401.12\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.008717622607946396\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  51 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1044.0\n",
      "Mean Reward of that batch 348.0\n",
      "Average Reward of all training: 400.078431372549\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: -0.05107198283076286\n",
      "==========================================\n",
      "Epoch:  52 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1044.0\n",
      "Mean Reward of that batch 348.0\n",
      "Average Reward of all training: 399.0769230769231\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.00404617702588439\n",
      "==========================================\n",
      "Epoch:  53 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 396.9056603773585\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: -0.025096096098423004\n",
      "==========================================\n",
      "Epoch:  54 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 856.0\n",
      "Mean Reward of that batch 428.0\n",
      "Average Reward of all training: 397.48148148148147\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.015382004901766777\n",
      "==========================================\n",
      "Epoch:  55 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1044.0\n",
      "Mean Reward of that batch 348.0\n",
      "Average Reward of all training: 396.58181818181816\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: -0.006979822181165218\n",
      "==========================================\n",
      "Epoch:  56 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 856.0\n",
      "Mean Reward of that batch 428.0\n",
      "Average Reward of all training: 397.14285714285717\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: -0.03535859286785126\n",
      "==========================================\n",
      "Epoch:  57 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 395.1578947368421\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: -0.06814131140708923\n",
      "==========================================\n",
      "Epoch:  58 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 952.0\n",
      "Mean Reward of that batch 476.0\n",
      "Average Reward of all training: 396.55172413793105\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.020260745659470558\n",
      "==========================================\n",
      "Epoch:  59 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 394.64406779661016\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.00755375437438488\n",
      "==========================================\n",
      "Epoch:  60 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1044.0\n",
      "Mean Reward of that batch 348.0\n",
      "Average Reward of all training: 393.8666666666667\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: -0.05414089560508728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n",
      "==========================================\n",
      "Epoch:  61 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 948.0\n",
      "Mean Reward of that batch 316.0\n",
      "Average Reward of all training: 392.59016393442624\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: -0.01256487239152193\n",
      "==========================================\n",
      "Epoch:  62 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1044.0\n",
      "Mean Reward of that batch 348.0\n",
      "Average Reward of all training: 391.8709677419355\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: -0.05892781913280487\n",
      "==========================================\n",
      "Epoch:  63 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 390.1587301587302\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: -0.024828657507896423\n",
      "==========================================\n",
      "Epoch:  64 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 388.5\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.004466771148145199\n",
      "==========================================\n",
      "Epoch:  65 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 386.89230769230767\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.011572950519621372\n",
      "==========================================\n",
      "Epoch:  66 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 948.0\n",
      "Mean Reward of that batch 316.0\n",
      "Average Reward of all training: 385.8181818181818\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.0025759844575077295\n",
      "==========================================\n",
      "Epoch:  67 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 384.2985074626866\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.0014878138899803162\n",
      "==========================================\n",
      "Epoch:  68 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 948.0\n",
      "Mean Reward of that batch 316.0\n",
      "Average Reward of all training: 383.29411764705884\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.010997157543897629\n",
      "==========================================\n",
      "Epoch:  69 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 381.8550724637681\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: -0.04251366853713989\n",
      "==========================================\n",
      "Epoch:  70 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 380.45714285714286\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.0031254515051841736\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  71 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 948.0\n",
      "Mean Reward of that batch 316.0\n",
      "Average Reward of all training: 379.5492957746479\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.004832224454730749\n",
      "==========================================\n",
      "Epoch:  72 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 378.22222222222223\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.0018768643494695425\n",
      "==========================================\n",
      "Epoch:  73 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1044.0\n",
      "Mean Reward of that batch 348.0\n",
      "Average Reward of all training: 377.8082191780822\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.009740270674228668\n",
      "==========================================\n",
      "Epoch:  74 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 376.5405405405405\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: -0.0023672578390687704\n",
      "==========================================\n",
      "Epoch:  75 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 375.3066666666667\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.008303935639560223\n",
      "==========================================\n",
      "Epoch:  76 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 374.10526315789474\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: -0.0007634263020008802\n",
      "==========================================\n",
      "Epoch:  77 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 372.93506493506493\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.0003433393139857799\n",
      "==========================================\n",
      "Epoch:  78 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 371.79487179487177\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: -0.010523547418415546\n",
      "==========================================\n",
      "Epoch:  79 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 370.6835443037975\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: -0.0005830365116707981\n",
      "==========================================\n",
      "Epoch:  80 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 369.6\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.005474801640957594\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  81 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 368.5432098765432\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.004606372211128473\n",
      "==========================================\n",
      "Epoch:  82 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 367.5121951219512\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.0020582505967468023\n",
      "==========================================\n",
      "Epoch:  83 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 948.0\n",
      "Mean Reward of that batch 316.0\n",
      "Average Reward of all training: 366.89156626506025\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: -0.04034877568483353\n",
      "==========================================\n",
      "Epoch:  84 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 365.9047619047619\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.0031187194399535656\n",
      "==========================================\n",
      "Epoch:  85 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 364.94117647058823\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.00019402460020501167\n",
      "==========================================\n",
      "Epoch:  86 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 364.0\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.0017624936299398541\n",
      "==========================================\n",
      "Epoch:  87 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 363.08045977011494\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.002384086139500141\n",
      "==========================================\n",
      "Epoch:  88 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 362.1818181818182\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: -0.03638722002506256\n",
      "==========================================\n",
      "Epoch:  89 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 361.30337078651684\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: -0.0022020693868398666\n",
      "==========================================\n",
      "Epoch:  90 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 360.44444444444446\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 6.886947812745348e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n",
      "==========================================\n",
      "Epoch:  91 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 359.6043956043956\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 3.8483209209516644e-05\n",
      "==========================================\n",
      "Epoch:  92 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 358.7826086956522\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.003353644162416458\n",
      "==========================================\n",
      "Epoch:  93 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 357.97849462365593\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.0023643886670470238\n",
      "==========================================\n",
      "Epoch:  94 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 357.1914893617021\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.003739916253834963\n",
      "==========================================\n",
      "Epoch:  95 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 356.42105263157896\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 2.432976725685876e-05\n",
      "==========================================\n",
      "Epoch:  96 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 355.6666666666667\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.0011280725011602044\n",
      "==========================================\n",
      "Epoch:  97 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 354.92783505154637\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: -5.544790838030167e-06\n",
      "==========================================\n",
      "Epoch:  98 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 354.2040816326531\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: -0.00013010516704525799\n",
      "==========================================\n",
      "Epoch:  99 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 353.4949494949495\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.002927382942289114\n",
      "==========================================\n",
      "Epoch:  100 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 352.8\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: -4.950692891725339e-05\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  101 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 352.1188118811881\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.0007380145834758878\n",
      "==========================================\n",
      "Epoch:  102 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 948.0\n",
      "Mean Reward of that batch 316.0\n",
      "Average Reward of all training: 351.7647058823529\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.0037511594127863646\n",
      "==========================================\n",
      "Epoch:  103 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 351.1067961165049\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: -0.014827821403741837\n",
      "==========================================\n",
      "Epoch:  104 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 350.46153846153845\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.002307578921318054\n",
      "==========================================\n",
      "Epoch:  105 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 349.8285714285714\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.0026455428451299667\n",
      "==========================================\n",
      "Epoch:  106 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 349.20754716981133\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.004477604757994413\n",
      "==========================================\n",
      "Epoch:  107 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 348.5981308411215\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.005996595602482557\n",
      "==========================================\n",
      "Epoch:  108 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 348.0\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.0008707667584531009\n",
      "==========================================\n",
      "Epoch:  109 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 347.41284403669727\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.001517555327154696\n",
      "==========================================\n",
      "Epoch:  110 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 346.8363636363636\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.00010422142804600298\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  111 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 346.27027027027026\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.0003216975019313395\n",
      "==========================================\n",
      "Epoch:  112 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 345.7142857142857\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 9.655336907599121e-05\n",
      "==========================================\n",
      "Epoch:  113 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 345.16814159292034\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.003680186113342643\n",
      "==========================================\n",
      "Epoch:  114 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 344.63157894736844\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.0030753437895327806\n",
      "==========================================\n",
      "Epoch:  115 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 344.10434782608695\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 4.4005297240801156e-05\n",
      "==========================================\n",
      "Epoch:  116 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 343.58620689655174\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.0004539564251899719\n",
      "==========================================\n",
      "Epoch:  117 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 343.0769230769231\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: -8.479609823552892e-05\n",
      "==========================================\n",
      "Epoch:  118 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 342.5762711864407\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.0027406751178205013\n",
      "==========================================\n",
      "Epoch:  119 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 342.0840336134454\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.004040671978145838\n",
      "==========================================\n",
      "Epoch:  120 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 341.6\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.0008608487551100552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n",
      "==========================================\n",
      "Epoch:  121 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 341.12396694214874\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.0007127790595404804\n",
      "==========================================\n",
      "Epoch:  122 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 340.655737704918\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.000269963318714872\n",
      "==========================================\n",
      "Epoch:  123 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 340.1951219512195\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: -1.0152544746233616e-05\n",
      "==========================================\n",
      "Epoch:  124 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 339.741935483871\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: -0.0011865678243339062\n",
      "==========================================\n",
      "Epoch:  125 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 339.296\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.00021236800239421427\n",
      "==========================================\n",
      "Epoch:  126 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 884.0\n",
      "Mean Reward of that batch 294.6666666666667\n",
      "Average Reward of all training: 338.94179894179894\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.0013570593437179923\n",
      "==========================================\n",
      "Epoch:  127 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 338.50918635170603\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.001865670899860561\n",
      "==========================================\n",
      "Epoch:  128 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 338.0833333333333\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: -7.492924487451091e-05\n",
      "==========================================\n",
      "Epoch:  129 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 948.0\n",
      "Mean Reward of that batch 316.0\n",
      "Average Reward of all training: 337.91214470284234\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.0025764082092791796\n",
      "==========================================\n",
      "Epoch:  130 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 337.4974358974359\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: -0.010548688471317291\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  131 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 337.089058524173\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.001789746806025505\n",
      "==========================================\n",
      "Epoch:  132 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 336.68686868686865\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.008743999525904655\n",
      "==========================================\n",
      "Epoch:  133 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 916.0\n",
      "Mean Reward of that batch 305.3333333333333\n",
      "Average Reward of all training: 336.45112781954884\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.007673086132854223\n",
      "==========================================\n",
      "Epoch:  134 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 336.05970149253733\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.0023419721983373165\n",
      "==========================================\n",
      "Epoch:  135 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 335.6740740740741\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 1.5608015928592067e-06\n",
      "==========================================\n",
      "Epoch:  136 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 335.29411764705884\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: -1.592959051777143e-05\n",
      "==========================================\n",
      "Epoch:  137 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 334.91970802919707\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.0030694848392158747\n",
      "==========================================\n",
      "Epoch:  138 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 334.5507246376812\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.0016715321689844131\n",
      "==========================================\n",
      "Epoch:  139 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 334.1870503597122\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.0014018616639077663\n",
      "==========================================\n",
      "Epoch:  140 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 333.8285714285714\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.006220770999789238\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  141 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 333.4751773049645\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 1.0824292985489592e-05\n",
      "==========================================\n",
      "Epoch:  142 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 333.1267605633803\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: -7.3824244282150175e-06\n",
      "==========================================\n",
      "Epoch:  143 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 332.7832167832168\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: -0.00012511963723227382\n",
      "==========================================\n",
      "Epoch:  144 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 332.44444444444446\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: -5.541168775380356e-06\n",
      "==========================================\n",
      "Epoch:  145 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 332.11034482758623\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.0018428917974233627\n",
      "==========================================\n",
      "Epoch:  146 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 331.7808219178082\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.00015138820162974298\n",
      "==========================================\n",
      "Epoch:  147 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 331.4557823129252\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 3.726731847564224e-06\n",
      "==========================================\n",
      "Epoch:  148 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 331.13513513513516\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 1.5512172467424534e-05\n",
      "==========================================\n",
      "Epoch:  149 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 330.81879194630875\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: -0.01886792480945587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Epoch:  150 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 330.50666666666666\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: -0.00018599422764964402\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  151 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 330.19867549668874\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: 0.0005941266426816583\n",
      "==========================================\n",
      "Epoch:  152 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 329.89473684210526\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: -0.002134735696017742\n",
      "==========================================\n",
      "Epoch:  153 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1044.0\n",
      "Mean Reward of that batch 348.0\n",
      "Average Reward of all training: 330.01307189542484\n",
      "Max reward for a batch so far: 1364.0\n",
      "Training Loss: -0.0365651473402977\n",
      "==========================================\n",
      "Epoch:  154 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1656.0\n",
      "Mean Reward of that batch 828.0\n",
      "Average Reward of all training: 333.24675324675326\n",
      "Max reward for a batch so far: 1656.0\n",
      "Training Loss: -0.18381386995315552\n",
      "==========================================\n",
      "Epoch:  155 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1112.0\n",
      "Mean Reward of that batch 556.0\n",
      "Average Reward of all training: 334.68387096774194\n",
      "Max reward for a batch so far: 1656.0\n",
      "Training Loss: 0.2033749520778656\n",
      "==========================================\n",
      "Epoch:  156 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 1660.0\n",
      "Mean Reward of that batch 1660.0\n",
      "Average Reward of all training: 343.1794871794872\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: -0.10551191121339798\n",
      "==========================================\n",
      "Epoch:  157 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 948.0\n",
      "Mean Reward of that batch 316.0\n",
      "Average Reward of all training: 343.0063694267516\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: -0.09029730409383774\n",
      "==========================================\n",
      "Epoch:  158 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 948.0\n",
      "Mean Reward of that batch 316.0\n",
      "Average Reward of all training: 342.8354430379747\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 0.05042684078216553\n",
      "==========================================\n",
      "Epoch:  159 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1012.0\n",
      "Mean Reward of that batch 337.3333333333333\n",
      "Average Reward of all training: 342.80083857442344\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 0.014858041889965534\n",
      "==========================================\n",
      "Epoch:  160 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 342.43333333333334\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 1.2439728607205325e-06\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  161 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 948.0\n",
      "Mean Reward of that batch 316.0\n",
      "Average Reward of all training: 342.2691511387164\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 0.003950435668230057\n",
      "==========================================\n",
      "Epoch:  162 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1076.0\n",
      "Mean Reward of that batch 358.6666666666667\n",
      "Average Reward of all training: 342.3703703703704\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 0.0006708969012834132\n",
      "==========================================\n",
      "Epoch:  163 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 920.0\n",
      "Mean Reward of that batch 460.0\n",
      "Average Reward of all training: 343.0920245398773\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 0.0031414255499839783\n",
      "==========================================\n",
      "Epoch:  164 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 948.0\n",
      "Mean Reward of that batch 316.0\n",
      "Average Reward of all training: 342.9268292682927\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 0.0002598035498522222\n",
      "==========================================\n",
      "Epoch:  165 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 342.56969696969696\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 0.00014943104179110378\n",
      "==========================================\n",
      "Epoch:  166 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 342.2168674698795\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 1.3916019725446205e-10\n",
      "==========================================\n",
      "Epoch:  167 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1044.0\n",
      "Mean Reward of that batch 348.0\n",
      "Average Reward of all training: 342.251497005988\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: -0.012028086930513382\n",
      "==========================================\n",
      "Epoch:  168 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1364.0\n",
      "Mean Reward of that batch 454.6666666666667\n",
      "Average Reward of all training: 342.9206349206349\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 0.002267161849886179\n",
      "==========================================\n",
      "Epoch:  169 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 342.5719921104536\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 0.006533726584166288\n",
      "==========================================\n",
      "Epoch:  170 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1076.0\n",
      "Mean Reward of that batch 358.6666666666667\n",
      "Average Reward of all training: 342.66666666666663\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 0.0012213517911732197\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  171 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 952.0\n",
      "Mean Reward of that batch 476.0\n",
      "Average Reward of all training: 343.4463937621832\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 0.0014737551100552082\n",
      "==========================================\n",
      "Epoch:  172 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 343.1007751937984\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 0.0010081464424729347\n",
      "==========================================\n",
      "Epoch:  173 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1012.0\n",
      "Mean Reward of that batch 337.3333333333333\n",
      "Average Reward of all training: 343.0674373795761\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 0.00034506749943830073\n",
      "==========================================\n",
      "Epoch:  174 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1012.0\n",
      "Mean Reward of that batch 337.3333333333333\n",
      "Average Reward of all training: 343.0344827586207\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 0.0001391610858263448\n",
      "==========================================\n",
      "Epoch:  175 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 948.0\n",
      "Mean Reward of that batch 316.0\n",
      "Average Reward of all training: 342.88\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 0.0014852463500574231\n",
      "==========================================\n",
      "Epoch:  176 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 342.54545454545456\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 0.0008675318094901741\n",
      "==========================================\n",
      "Epoch:  177 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 856.0\n",
      "Mean Reward of that batch 428.0\n",
      "Average Reward of all training: 343.0282485875706\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: -0.010543741285800934\n",
      "==========================================\n",
      "Epoch:  178 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1044.0\n",
      "Mean Reward of that batch 348.0\n",
      "Average Reward of all training: 343.0561797752809\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 0.00137244607321918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Epoch:  179 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 342.7262569832402\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 0.009746907278895378\n",
      "==========================================\n",
      "Epoch:  180 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1012.0\n",
      "Mean Reward of that batch 337.3333333333333\n",
      "Average Reward of all training: 342.6962962962963\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 0.0009592304704710841\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  181 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1044.0\n",
      "Mean Reward of that batch 348.0\n",
      "Average Reward of all training: 342.7255985267035\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 0.0003625496756285429\n",
      "==========================================\n",
      "Epoch:  182 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1044.0\n",
      "Mean Reward of that batch 348.0\n",
      "Average Reward of all training: 342.75457875457874\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 0.0009318235097452998\n",
      "==========================================\n",
      "Epoch:  183 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 948.0\n",
      "Mean Reward of that batch 316.0\n",
      "Average Reward of all training: 342.6083788706739\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 0.0006854781531728804\n",
      "==========================================\n",
      "Epoch:  184 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 884.0\n",
      "Mean Reward of that batch 294.6666666666667\n",
      "Average Reward of all training: 342.3478260869565\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 8.534716471331194e-05\n",
      "==========================================\n",
      "Epoch:  185 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1144.0\n",
      "Mean Reward of that batch 572.0\n",
      "Average Reward of all training: 343.5891891891892\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 0.0010702521540224552\n",
      "==========================================\n",
      "Epoch:  186 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 343.26881720430106\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: -1.2621100722753908e-05\n",
      "==========================================\n",
      "Epoch:  187 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1044.0\n",
      "Mean Reward of that batch 348.0\n",
      "Average Reward of all training: 343.29411764705884\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 0.002684882376343012\n",
      "==========================================\n",
      "Epoch:  188 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 824.0\n",
      "Mean Reward of that batch 412.0\n",
      "Average Reward of all training: 343.6595744680851\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 1.9739205527002923e-05\n",
      "==========================================\n",
      "Epoch:  189 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1044.0\n",
      "Mean Reward of that batch 348.0\n",
      "Average Reward of all training: 343.6825396825397\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 7.018140422587749e-06\n",
      "==========================================\n",
      "Epoch:  190 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 856.0\n",
      "Mean Reward of that batch 428.0\n",
      "Average Reward of all training: 344.12631578947367\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 0.005702430382370949\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  191 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1044.0\n",
      "Mean Reward of that batch 348.0\n",
      "Average Reward of all training: 344.14659685863876\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 0.1487254947423935\n",
      "==========================================\n",
      "Epoch:  192 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1012.0\n",
      "Mean Reward of that batch 337.3333333333333\n",
      "Average Reward of all training: 344.1111111111111\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: -0.6646478772163391\n",
      "==========================================\n",
      "Epoch:  193 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 343.7996545768566\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: -0.115913525223732\n",
      "==========================================\n",
      "Epoch:  194 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 343.49140893470786\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 0.712252676486969\n",
      "==========================================\n",
      "Epoch:  195 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1268.0\n",
      "Mean Reward of that batch 422.6666666666667\n",
      "Average Reward of all training: 343.8974358974359\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 0.07816996425390244\n",
      "==========================================\n",
      "Epoch:  196 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 343.59183673469386\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: -0.16645079851150513\n",
      "==========================================\n",
      "Epoch:  197 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 948.0\n",
      "Mean Reward of that batch 316.0\n",
      "Average Reward of all training: 343.4517766497462\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 0.08740033954381943\n",
      "==========================================\n",
      "Epoch:  198 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 948.0\n",
      "Mean Reward of that batch 316.0\n",
      "Average Reward of all training: 343.3131313131313\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: -0.6842159628868103\n",
      "==========================================\n",
      "Epoch:  199 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 343.0150753768844\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: -0.008350253105163574\n",
      "==========================================\n",
      "Epoch:  200 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 342.72\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 0.01720958575606346\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  201 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1144.0\n",
      "Mean Reward of that batch 572.0\n",
      "Average Reward of all training: 343.8606965174129\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: -1.5771615505218506\n",
      "==========================================\n",
      "Epoch:  202 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1204.0\n",
      "Mean Reward of that batch 401.3333333333333\n",
      "Average Reward of all training: 344.1452145214522\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: -3.6522200107574463\n",
      "==========================================\n",
      "Epoch:  203 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1268.0\n",
      "Mean Reward of that batch 422.6666666666667\n",
      "Average Reward of all training: 344.5320197044335\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 0.6869392395019531\n",
      "==========================================\n",
      "Epoch:  204 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1368.0\n",
      "Mean Reward of that batch 684.0\n",
      "Average Reward of all training: 346.19607843137254\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: -1.060346007347107\n",
      "==========================================\n",
      "Epoch:  205 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 856.0\n",
      "Mean Reward of that batch 428.0\n",
      "Average Reward of all training: 346.59512195121954\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 0.9001044631004333\n",
      "==========================================\n",
      "Epoch:  206 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1048.0\n",
      "Mean Reward of that batch 524.0\n",
      "Average Reward of all training: 347.45631067961165\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: -0.036396123468875885\n",
      "==========================================\n",
      "Epoch:  207 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1208.0\n",
      "Mean Reward of that batch 604.0\n",
      "Average Reward of all training: 348.69565217391306\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: -16.04584312438965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Epoch:  208 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1272.0\n",
      "Mean Reward of that batch 636.0\n",
      "Average Reward of all training: 350.0769230769231\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: -29.357769012451172\n",
      "==========================================\n",
      "Epoch:  209 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1080.0\n",
      "Mean Reward of that batch 540.0\n",
      "Average Reward of all training: 350.9856459330144\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: -13.561851501464844\n",
      "==========================================\n",
      "Epoch:  210 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 948.0\n",
      "Mean Reward of that batch 316.0\n",
      "Average Reward of all training: 350.81904761904764\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 0.15033523738384247\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  211 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1044.0\n",
      "Mean Reward of that batch 348.0\n",
      "Average Reward of all training: 350.8056872037915\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 1.4360153675079346\n",
      "==========================================\n",
      "Epoch:  212 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1304.0\n",
      "Mean Reward of that batch 652.0\n",
      "Average Reward of all training: 352.22641509433964\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: -28.910579681396484\n",
      "==========================================\n",
      "Epoch:  213 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1048.0\n",
      "Mean Reward of that batch 524.0\n",
      "Average Reward of all training: 353.03286384976525\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: -0.568841814994812\n",
      "==========================================\n",
      "Epoch:  214 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 352.7102803738318\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 1.1298311948776245\n",
      "==========================================\n",
      "Epoch:  215 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 352.3906976744186\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 0.0004393770359456539\n",
      "==========================================\n",
      "Epoch:  216 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 352.0740740740741\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 8.570688078179955e-05\n",
      "==========================================\n",
      "Epoch:  217 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 351.7603686635945\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 0.0016777339624240994\n",
      "==========================================\n",
      "Epoch:  218 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1076.0\n",
      "Mean Reward of that batch 358.6666666666667\n",
      "Average Reward of all training: 351.79204892966357\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 1.3847366571426392\n",
      "==========================================\n",
      "Epoch:  219 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 351.48249619482493\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 0.0211610309779644\n",
      "==========================================\n",
      "Epoch:  220 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 351.17575757575753\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 0.16613639891147614\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  221 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1044.0\n",
      "Mean Reward of that batch 348.0\n",
      "Average Reward of all training: 351.1613876319758\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 1.0826654434204102\n",
      "==========================================\n",
      "Epoch:  222 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 350.8588588588588\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 4.2100302380276844e-05\n",
      "==========================================\n",
      "Epoch:  223 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 350.55904334828097\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 8.149625500664115e-06\n",
      "==========================================\n",
      "Epoch:  224 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 350.2619047619047\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 0.0010892227292060852\n",
      "==========================================\n",
      "Epoch:  225 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 349.96740740740734\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 2.08970122912433e-05\n",
      "==========================================\n",
      "Epoch:  226 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 349.67551622418875\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 0.0001569792366353795\n",
      "==========================================\n",
      "Epoch:  227 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 349.38619676945666\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 0.4295251667499542\n",
      "==========================================\n",
      "Epoch:  228 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 349.09941520467834\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 0.00012329388118814677\n",
      "==========================================\n",
      "Epoch:  229 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 348.8151382823871\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 0.000401829689508304\n",
      "==========================================\n",
      "Epoch:  230 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 348.5333333333333\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 0.14293719828128815\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  231 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 348.2539682539682\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 0.5225432515144348\n",
      "==========================================\n",
      "Epoch:  232 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1044.0\n",
      "Mean Reward of that batch 348.0\n",
      "Average Reward of all training: 348.2528735632184\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: -5.682658672332764\n",
      "==========================================\n",
      "Epoch:  233 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 347.97711015736763\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 0.2537683844566345\n",
      "==========================================\n",
      "Epoch:  234 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 347.70370370370364\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 0.00024203024804592133\n",
      "==========================================\n",
      "Epoch:  235 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 347.43262411347513\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 0.0016187017317861319\n",
      "==========================================\n",
      "Epoch:  236 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 347.1638418079096\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 0.3738124668598175\n",
      "==========================================\n",
      "Epoch:  237 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 346.8973277074542\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 3.831157664535567e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Epoch:  238 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1140.0\n",
      "Mean Reward of that batch 380.0\n",
      "Average Reward of all training: 347.0364145658263\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: -0.3495739698410034\n",
      "==========================================\n",
      "Epoch:  239 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1140.0\n",
      "Mean Reward of that batch 380.0\n",
      "Average Reward of all training: 347.1743375174337\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 0.9074496626853943\n",
      "==========================================\n",
      "Epoch:  240 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1272.0\n",
      "Mean Reward of that batch 636.0\n",
      "Average Reward of all training: 348.37777777777774\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 1.345657467842102\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  241 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 856.0\n",
      "Mean Reward of that batch 428.0\n",
      "Average Reward of all training: 348.7081604426002\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 0.7941066026687622\n",
      "==========================================\n",
      "Epoch:  242 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 952.0\n",
      "Mean Reward of that batch 476.0\n",
      "Average Reward of all training: 349.2341597796143\n",
      "Max reward for a batch so far: 1660.0\n",
      "Training Loss: 0.34624549746513367\n",
      "==========================================\n",
      "Epoch:  243 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 1724.0\n",
      "Mean Reward of that batch 1724.0\n",
      "Average Reward of all training: 354.8916323731138\n",
      "Max reward for a batch so far: 1724.0\n",
      "Training Loss: 0.4218294322490692\n",
      "==========================================\n",
      "Epoch:  244 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1368.0\n",
      "Mean Reward of that batch 684.0\n",
      "Average Reward of all training: 356.2404371584699\n",
      "Max reward for a batch so far: 1724.0\n",
      "Training Loss: 0.21295148134231567\n",
      "==========================================\n",
      "Epoch:  245 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1304.0\n",
      "Mean Reward of that batch 652.0\n",
      "Average Reward of all training: 357.447619047619\n",
      "Max reward for a batch so far: 1724.0\n",
      "Training Loss: -1.8251186609268188\n",
      "==========================================\n",
      "Epoch:  246 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1172.0\n",
      "Mean Reward of that batch 390.6666666666667\n",
      "Average Reward of all training: 357.58265582655827\n",
      "Max reward for a batch so far: 1724.0\n",
      "Training Loss: 0.3672999441623688\n",
      "==========================================\n",
      "Epoch:  247 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 824.0\n",
      "Mean Reward of that batch 412.0\n",
      "Average Reward of all training: 357.80296896086367\n",
      "Max reward for a batch so far: 1724.0\n",
      "Training Loss: 0.34753361344337463\n",
      "==========================================\n",
      "Epoch:  248 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 920.0\n",
      "Mean Reward of that batch 460.0\n",
      "Average Reward of all training: 358.21505376344084\n",
      "Max reward for a batch so far: 1724.0\n",
      "Training Loss: 0.3343348503112793\n",
      "==========================================\n",
      "Epoch:  249 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 365.2101740294512\n",
      "Max reward for a batch so far: 2100.0\n",
      "Training Loss: 2.9892876148223877\n",
      "==========================================\n",
      "Epoch:  250 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 1724.0\n",
      "Mean Reward of that batch 1724.0\n",
      "Average Reward of all training: 370.6453333333334\n",
      "Max reward for a batch so far: 2100.0\n",
      "Training Loss: 1.5250093936920166\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  251 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 377.53519256308107\n",
      "Max reward for a batch so far: 2100.0\n",
      "Training Loss: 0.5755990743637085\n",
      "==========================================\n",
      "Epoch:  252 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 984.0\n",
      "Mean Reward of that batch 492.0\n",
      "Average Reward of all training: 377.98941798941803\n",
      "Max reward for a batch so far: 2100.0\n",
      "Training Loss: -5.593899250030518\n",
      "==========================================\n",
      "Epoch:  253 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 952.0\n",
      "Mean Reward of that batch 476.0\n",
      "Average Reward of all training: 378.37681159420293\n",
      "Max reward for a batch so far: 2100.0\n",
      "Training Loss: 1.1762861013412476\n",
      "==========================================\n",
      "Epoch:  254 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 385.15485564304464\n",
      "Max reward for a batch so far: 2100.0\n",
      "Training Loss: 5.650079727172852\n",
      "==========================================\n",
      "Epoch:  255 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 391.8797385620915\n",
      "Max reward for a batch so far: 2100.0\n",
      "Training Loss: 0.31033971905708313\n",
      "==========================================\n",
      "Epoch:  256 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 2828.0\n",
      "Mean Reward of that batch 942.6666666666666\n",
      "Average Reward of all training: 394.03125\n",
      "Max reward for a batch so far: 2828.0\n",
      "Training Loss: -8.959751539805438e-06\n",
      "==========================================\n",
      "Epoch:  257 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 2992.0\n",
      "Mean Reward of that batch 1496.0\n",
      "Average Reward of all training: 398.3190661478599\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -0.7591780424118042\n",
      "==========================================\n",
      "Epoch:  258 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1848.0\n",
      "Mean Reward of that batch 924.0\n",
      "Average Reward of all training: 400.3565891472868\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 1.5513347387313843\n",
      "==========================================\n",
      "Epoch:  259 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 2800.0\n",
      "Mean Reward of that batch 1400.0\n",
      "Average Reward of all training: 404.2162162162162\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.26756179332733154\n",
      "==========================================\n",
      "Epoch:  260 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 956.0\n",
      "Mean Reward of that batch 956.0\n",
      "Average Reward of all training: 406.33846153846156\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.9058411121368408\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  261 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 924.0\n",
      "Mean Reward of that batch 924.0\n",
      "Average Reward of all training: 408.32183908045977\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -3.4268686771392822\n",
      "==========================================\n",
      "Epoch:  262 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 414.77862595419845\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -0.01624768227338791\n",
      "==========================================\n",
      "Epoch:  263 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 421.18631178707227\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -0.40324726700782776\n",
      "==========================================\n",
      "Epoch:  264 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 427.54545454545456\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.8797522187232971\n",
      "==========================================\n",
      "Epoch:  265 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 433.8566037735849\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.5253486037254333\n",
      "==========================================\n",
      "Epoch:  266 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 440.1203007518797\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -0.2992689609527588\n",
      "==========================================\n",
      "Epoch:  267 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 446.3370786516854\n",
      "Max reward for a batch so far: 2992.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.3226238191127777\n",
      "==========================================\n",
      "Epoch:  268 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 452.5074626865672\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 1.5657435655593872\n",
      "==========================================\n",
      "Epoch:  269 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 2512.0\n",
      "Mean Reward of that batch 1256.0\n",
      "Average Reward of all training: 455.4944237918216\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -0.2981680929660797\n",
      "==========================================\n",
      "Epoch:  270 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 461.5851851851852\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.5213154554367065\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  271 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 467.6309963099631\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.2572267949581146\n",
      "==========================================\n",
      "Epoch:  272 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 956.0\n",
      "Mean Reward of that batch 956.0\n",
      "Average Reward of all training: 469.4264705882353\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -0.9729174971580505\n",
      "==========================================\n",
      "Epoch:  273 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 2672.0\n",
      "Mean Reward of that batch 1336.0\n",
      "Average Reward of all training: 472.6007326007326\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.26580938696861267\n",
      "==========================================\n",
      "Epoch:  274 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 478.54014598540147\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -0.7521397471427917\n",
      "==========================================\n",
      "Epoch:  275 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 484.43636363636364\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 1.486610770225525\n",
      "==========================================\n",
      "Epoch:  276 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1080.0\n",
      "Mean Reward of that batch 540.0\n",
      "Average Reward of all training: 484.6376811594203\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 1.320475459098816\n",
      "==========================================\n",
      "Epoch:  277 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 490.46931407942236\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.23301710188388824\n",
      "==========================================\n",
      "Epoch:  278 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 920.0\n",
      "Mean Reward of that batch 460.0\n",
      "Average Reward of all training: 490.35971223021585\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -0.772918164730072\n",
      "==========================================\n",
      "Epoch:  279 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1496.0\n",
      "Mean Reward of that batch 748.0\n",
      "Average Reward of all training: 491.2831541218638\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 1.5544358491897583\n",
      "==========================================\n",
      "Epoch:  280 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1044.0\n",
      "Mean Reward of that batch 348.0\n",
      "Average Reward of all training: 490.77142857142854\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 2.9118800163269043\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  281 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 952.0\n",
      "Mean Reward of that batch 476.0\n",
      "Average Reward of all training: 490.7188612099644\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -1.3005284070968628\n",
      "==========================================\n",
      "Epoch:  282 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 980.0\n",
      "Mean Reward of that batch 326.6666666666667\n",
      "Average Reward of all training: 490.13711583924356\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -2.524233341217041\n",
      "==========================================\n",
      "Epoch:  283 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 824.0\n",
      "Mean Reward of that batch 412.0\n",
      "Average Reward of all training: 489.86101295641936\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -0.7944427728652954\n",
      "==========================================\n",
      "Epoch:  284 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 495.5305164319249\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -0.36743810772895813\n",
      "==========================================\n",
      "Epoch:  285 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 501.1602339181287\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.9627988934516907\n",
      "==========================================\n",
      "Epoch:  286 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 948.0\n",
      "Mean Reward of that batch 316.0\n",
      "Average Reward of all training: 500.51282051282055\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -1.3563551902770996\n",
      "==========================================\n",
      "Epoch:  287 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1592.0\n",
      "Mean Reward of that batch 796.0\n",
      "Average Reward of all training: 501.5423925667829\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -8.061953849392012e-05\n",
      "==========================================\n",
      "Epoch:  288 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 2764.0\n",
      "Mean Reward of that batch 921.3333333333334\n",
      "Average Reward of all training: 503.0\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -0.2421010136604309\n",
      "==========================================\n",
      "Epoch:  289 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 2384.0\n",
      "Mean Reward of that batch 1192.0\n",
      "Average Reward of all training: 505.3840830449827\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -1.2199651002883911\n",
      "==========================================\n",
      "Epoch:  290 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 510.88275862068963\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 2.4243240356445312\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  291 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 2640.0\n",
      "Mean Reward of that batch 1320.0\n",
      "Average Reward of all training: 513.6632302405499\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 1.7037849426269531\n",
      "==========================================\n",
      "Epoch:  292 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 1372.0\n",
      "Mean Reward of that batch 1372.0\n",
      "Average Reward of all training: 516.6027397260274\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 1.6732778549194336\n",
      "==========================================\n",
      "Epoch:  293 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 515.8088737201365\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -3.431936025619507\n",
      "==========================================\n",
      "Epoch:  294 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 2736.0\n",
      "Mean Reward of that batch 1368.0\n",
      "Average Reward of all training: 518.7074829931972\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 1.3461263179779053\n",
      "==========================================\n",
      "Epoch:  295 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 2732.0\n",
      "Mean Reward of that batch 910.6666666666666\n",
      "Average Reward of all training: 520.0361581920905\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -0.2246021330356598\n",
      "==========================================\n",
      "Epoch:  296 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 525.3738738738739\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -1.5251485109329224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Epoch:  297 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 2360.0\n",
      "Mean Reward of that batch 1180.0\n",
      "Average Reward of all training: 527.5780022446688\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 1.7797234058380127\n",
      "==========================================\n",
      "Epoch:  298 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 532.8545861297539\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 1.218950867652893\n",
      "==========================================\n",
      "Epoch:  299 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1816.0\n",
      "Mean Reward of that batch 908.0\n",
      "Average Reward of all training: 534.1092530657747\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.6010787487030029\n",
      "==========================================\n",
      "Epoch:  300 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 2448.0\n",
      "Mean Reward of that batch 1224.0\n",
      "Average Reward of all training: 536.4088888888889\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.32410287857055664\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  301 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 541.6035437430786\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.14300848543643951\n",
      "==========================================\n",
      "Epoch:  302 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1076.0\n",
      "Mean Reward of that batch 358.6666666666667\n",
      "Average Reward of all training: 540.9977924944811\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 2.0320584774017334\n",
      "==========================================\n",
      "Epoch:  303 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1236.0\n",
      "Mean Reward of that batch 412.0\n",
      "Average Reward of all training: 540.5720572057205\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -0.6073765754699707\n",
      "==========================================\n",
      "Epoch:  304 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1428.0\n",
      "Mean Reward of that batch 476.0\n",
      "Average Reward of all training: 540.3596491228069\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.6914912462234497\n",
      "==========================================\n",
      "Epoch:  305 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1976.0\n",
      "Mean Reward of that batch 988.0\n",
      "Average Reward of all training: 541.8273224043716\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.17495238780975342\n",
      "==========================================\n",
      "Epoch:  306 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1204.0\n",
      "Mean Reward of that batch 401.3333333333333\n",
      "Average Reward of all training: 541.3681917211329\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -0.30561092495918274\n",
      "==========================================\n",
      "Epoch:  307 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 546.4451682953311\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.24859720468521118\n",
      "==========================================\n",
      "Epoch:  308 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 856.0\n",
      "Mean Reward of that batch 428.0\n",
      "Average Reward of all training: 546.060606060606\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -2.816970109939575\n",
      "==========================================\n",
      "Epoch:  309 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1208.0\n",
      "Mean Reward of that batch 604.0\n",
      "Average Reward of all training: 546.2481121898597\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 3.463629722595215\n",
      "==========================================\n",
      "Epoch:  310 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 551.2602150537634\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.24500642716884613\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  311 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 556.2400857449088\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -1.0806193351745605\n",
      "==========================================\n",
      "Epoch:  312 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 2768.0\n",
      "Mean Reward of that batch 1384.0\n",
      "Average Reward of all training: 558.8931623931624\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 2.1629157066345215\n",
      "==========================================\n",
      "Epoch:  313 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 563.8168264110756\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -0.17400863766670227\n",
      "==========================================\n",
      "Epoch:  314 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 568.7091295116772\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.748177170753479\n",
      "==========================================\n",
      "Epoch:  315 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 2764.0\n",
      "Mean Reward of that batch 921.3333333333334\n",
      "Average Reward of all training: 569.8285714285714\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.5826787352561951\n",
      "==========================================\n",
      "Epoch:  316 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 574.6708860759494\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.5050575137138367\n",
      "==========================================\n",
      "Epoch:  317 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 579.4826498422713\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.23474234342575073\n",
      "==========================================\n",
      "Epoch:  318 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1368.0\n",
      "Mean Reward of that batch 684.0\n",
      "Average Reward of all training: 579.811320754717\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.29247236251831055\n",
      "==========================================\n",
      "Epoch:  319 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 1276.0\n",
      "Mean Reward of that batch 1276.0\n",
      "Average Reward of all training: 581.9937304075235\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -0.356099933385849\n",
      "==========================================\n",
      "Epoch:  320 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1300.0\n",
      "Mean Reward of that batch 433.3333333333333\n",
      "Average Reward of all training: 581.5291666666667\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.7126191854476929\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  321 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 856.0\n",
      "Mean Reward of that batch 428.0\n",
      "Average Reward of all training: 581.0508826583593\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.2554926574230194\n",
      "==========================================\n",
      "Epoch:  322 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1400.0\n",
      "Mean Reward of that batch 700.0\n",
      "Average Reward of all training: 581.4202898550725\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.22706155478954315\n",
      "==========================================\n",
      "Epoch:  323 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 824.0\n",
      "Mean Reward of that batch 412.0\n",
      "Average Reward of all training: 580.8957688338494\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.004420433193445206\n",
      "==========================================\n",
      "Epoch:  324 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1620.0\n",
      "Mean Reward of that batch 540.0\n",
      "Average Reward of all training: 580.769547325103\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -0.4316583275794983\n",
      "==========================================\n",
      "Epoch:  325 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1528.0\n",
      "Mean Reward of that batch 764.0\n",
      "Average Reward of all training: 581.3333333333334\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.003588941413909197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Epoch:  326 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 856.0\n",
      "Mean Reward of that batch 428.0\n",
      "Average Reward of all training: 580.8629856850716\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.6142831444740295\n",
      "==========================================\n",
      "Epoch:  327 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 888.0\n",
      "Mean Reward of that batch 444.0\n",
      "Average Reward of all training: 580.4444444444445\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -0.8242550492286682\n",
      "==========================================\n",
      "Epoch:  328 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 585.0772357723578\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -1.3364346027374268\n",
      "==========================================\n",
      "Epoch:  329 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1048.0\n",
      "Mean Reward of that batch 524.0\n",
      "Average Reward of all training: 584.8915906788247\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -11.67420768737793\n",
      "==========================================\n",
      "Epoch:  330 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 2384.0\n",
      "Mean Reward of that batch 1192.0\n",
      "Average Reward of all training: 586.7313131313132\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 11.046177864074707\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  331 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 591.3031218529708\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -5.481982707977295\n",
      "==========================================\n",
      "Epoch:  332 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1272.0\n",
      "Mean Reward of that batch 636.0\n",
      "Average Reward of all training: 591.4377510040161\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -32.1780891418457\n",
      "==========================================\n",
      "Epoch:  333 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 920.0\n",
      "Mean Reward of that batch 460.0\n",
      "Average Reward of all training: 591.0430430430431\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -24.665786743164062\n",
      "==========================================\n",
      "Epoch:  334 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 952.0\n",
      "Mean Reward of that batch 476.0\n",
      "Average Reward of all training: 590.6986027944112\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 26.36948013305664\n",
      "==========================================\n",
      "Epoch:  335 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1332.0\n",
      "Mean Reward of that batch 444.0\n",
      "Average Reward of all training: 590.260696517413\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 1.885300636291504\n",
      "==========================================\n",
      "Epoch:  336 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1656.0\n",
      "Mean Reward of that batch 828.0\n",
      "Average Reward of all training: 590.9682539682539\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -37.79151916503906\n",
      "==========================================\n",
      "Epoch:  337 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 1084.0\n",
      "Mean Reward of that batch 1084.0\n",
      "Average Reward of all training: 592.4312561819979\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 3.3827643394470215\n",
      "==========================================\n",
      "Epoch:  338 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 2072.0\n",
      "Mean Reward of that batch 1036.0\n",
      "Average Reward of all training: 593.7435897435897\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -35.545387268066406\n",
      "==========================================\n",
      "Epoch:  339 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 888.0\n",
      "Mean Reward of that batch 444.0\n",
      "Average Reward of all training: 593.3018682399213\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 22.616310119628906\n",
      "==========================================\n",
      "Epoch:  340 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 980.0\n",
      "Mean Reward of that batch 326.6666666666667\n",
      "Average Reward of all training: 592.5176470588235\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 10.688678741455078\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  341 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 824.0\n",
      "Mean Reward of that batch 412.0\n",
      "Average Reward of all training: 591.9882697947214\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -8.063691139221191\n",
      "==========================================\n",
      "Epoch:  342 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 591.0877192982456\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 4.85814211970137e-07\n",
      "==========================================\n",
      "Epoch:  343 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 590.1924198250729\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 1.0464608669281006\n",
      "==========================================\n",
      "Epoch:  344 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 589.3023255813954\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -0.009279957972466946\n",
      "==========================================\n",
      "Epoch:  345 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 948.0\n",
      "Mean Reward of that batch 316.0\n",
      "Average Reward of all training: 588.5101449275362\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 1.4853954315185547\n",
      "==========================================\n",
      "Epoch:  346 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 948.0\n",
      "Mean Reward of that batch 316.0\n",
      "Average Reward of all training: 587.7225433526012\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 1.7790814638137817\n",
      "==========================================\n",
      "Epoch:  347 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 586.8472622478387\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -0.1548648625612259\n",
      "==========================================\n",
      "Epoch:  348 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1140.0\n",
      "Mean Reward of that batch 380.0\n",
      "Average Reward of all training: 586.2528735632184\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 2.9857308864593506\n",
      "==========================================\n",
      "Epoch:  349 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1140.0\n",
      "Mean Reward of that batch 380.0\n",
      "Average Reward of all training: 585.6618911174785\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 1.417559266090393\n",
      "==========================================\n",
      "Epoch:  350 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1172.0\n",
      "Mean Reward of that batch 390.6666666666667\n",
      "Average Reward of all training: 585.1047619047619\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -1.9575568437576294\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  351 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1076.0\n",
      "Mean Reward of that batch 358.6666666666667\n",
      "Average Reward of all training: 584.4596391263058\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -0.4069218337535858\n",
      "==========================================\n",
      "Epoch:  352 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 583.6060606060606\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.00046518724411726\n",
      "==========================================\n",
      "Epoch:  353 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 582.7573182247404\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -1.4651254415512085\n",
      "==========================================\n",
      "Epoch:  354 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 581.9133709981168\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 7.008085844972811e-08\n",
      "==========================================\n",
      "Epoch:  355 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 581.0741784037559\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 3.562269557733089e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Epoch:  356 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 580.2397003745318\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 8.386073750443757e-05\n",
      "==========================================\n",
      "Epoch:  357 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 579.4098972922502\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 5.9347858183400604e-08\n",
      "==========================================\n",
      "Epoch:  358 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 578.584729981378\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 5.350742071641434e-07\n",
      "==========================================\n",
      "Epoch:  359 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 577.7641597028784\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 8.175316906999797e-06\n",
      "==========================================\n",
      "Epoch:  360 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 576.9481481481482\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 2.2907336187927285e-06\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  361 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 576.1366574330564\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 5.245774445938878e-07\n",
      "==========================================\n",
      "Epoch:  362 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 575.329650092081\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 2.86283512451746e-08\n",
      "==========================================\n",
      "Epoch:  363 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 574.5270890725436\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 8.99451788427541e-06\n",
      "==========================================\n",
      "Epoch:  364 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 573.7289377289378\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.21092970669269562\n",
      "==========================================\n",
      "Epoch:  365 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 916.0\n",
      "Mean Reward of that batch 305.3333333333333\n",
      "Average Reward of all training: 572.9936073059362\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.37289124727249146\n",
      "==========================================\n",
      "Epoch:  366 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 572.2040072859745\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.4768688976764679\n",
      "==========================================\n",
      "Epoch:  367 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 571.418710263397\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -1.9066284949076362e-05\n",
      "==========================================\n",
      "Epoch:  368 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 570.6376811594204\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.24880139529705048\n",
      "==========================================\n",
      "Epoch:  369 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 916.0\n",
      "Mean Reward of that batch 305.3333333333333\n",
      "Average Reward of all training: 569.9186991869918\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 1.0879907608032227\n",
      "==========================================\n",
      "Epoch:  370 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 569.145945945946\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.05575929209589958\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  371 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 568.377358490566\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.007880214601755142\n",
      "==========================================\n",
      "Epoch:  372 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1044.0\n",
      "Mean Reward of that batch 348.0\n",
      "Average Reward of all training: 567.7849462365591\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.16787928342819214\n",
      "==========================================\n",
      "Epoch:  373 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 567.0241286863271\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -0.299964964389801\n",
      "==========================================\n",
      "Epoch:  374 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1076.0\n",
      "Mean Reward of that batch 358.6666666666667\n",
      "Average Reward of all training: 566.4670231729056\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -1.1203625202178955\n",
      "==========================================\n",
      "Epoch:  375 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 2296.0\n",
      "Mean Reward of that batch 1148.0\n",
      "Average Reward of all training: 568.0177777777778\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.919122040271759\n",
      "==========================================\n",
      "Epoch:  376 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1880.0\n",
      "Mean Reward of that batch 940.0\n",
      "Average Reward of all training: 569.0070921985816\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 1.7711684703826904\n",
      "==========================================\n",
      "Epoch:  377 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 573.0680813439435\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 3.569495916366577\n",
      "==========================================\n",
      "Epoch:  378 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 577.1075837742505\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 3.755354881286621\n",
      "==========================================\n",
      "Epoch:  379 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 956.0\n",
      "Mean Reward of that batch 956.0\n",
      "Average Reward of all training: 578.1072999120493\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 1.3767982721328735\n",
      "==========================================\n",
      "Epoch:  380 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1016.0\n",
      "Mean Reward of that batch 508.0\n",
      "Average Reward of all training: 577.922807017544\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.964896559715271\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  381 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 824.0\n",
      "Mean Reward of that batch 412.0\n",
      "Average Reward of all training: 577.4873140857393\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.6939737796783447\n",
      "==========================================\n",
      "Epoch:  382 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 824.0\n",
      "Mean Reward of that batch 412.0\n",
      "Average Reward of all training: 577.0541012216405\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -0.3106306493282318\n",
      "==========================================\n",
      "Epoch:  383 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1076.0\n",
      "Mean Reward of that batch 358.6666666666667\n",
      "Average Reward of all training: 576.4838990426458\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.5803658366203308\n",
      "==========================================\n",
      "Epoch:  384 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 952.0\n",
      "Mean Reward of that batch 476.0\n",
      "Average Reward of all training: 576.2222222222222\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -2.349764823913574\n",
      "==========================================\n",
      "Epoch:  385 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 580.1800865800865\n",
      "Max reward for a batch so far: 2992.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2.9192068576812744\n",
      "==========================================\n",
      "Epoch:  386 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1176.0\n",
      "Mean Reward of that batch 588.0\n",
      "Average Reward of all training: 580.2003454231433\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.9428285360336304\n",
      "==========================================\n",
      "Epoch:  387 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 584.1274763135227\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.5186979174613953\n",
      "==========================================\n",
      "Epoch:  388 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1720.0\n",
      "Mean Reward of that batch 860.0\n",
      "Average Reward of all training: 584.8384879725086\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.5921421647071838\n",
      "==========================================\n",
      "Epoch:  389 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 1020.0\n",
      "Mean Reward of that batch 1020.0\n",
      "Average Reward of all training: 585.9571550985432\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.4741549491882324\n",
      "==========================================\n",
      "Epoch:  390 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1460.0\n",
      "Mean Reward of that batch 486.6666666666667\n",
      "Average Reward of all training: 585.7025641025641\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -1.1715861558914185\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  391 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 952.0\n",
      "Mean Reward of that batch 476.0\n",
      "Average Reward of all training: 585.4219948849105\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -0.9387434720993042\n",
      "==========================================\n",
      "Epoch:  392 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 589.2857142857143\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.7500036954879761\n",
      "==========================================\n",
      "Epoch:  393 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 593.1297709923664\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -0.2928745448589325\n",
      "==========================================\n",
      "Epoch:  394 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 596.9543147208121\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.3334020972251892\n",
      "==========================================\n",
      "Epoch:  395 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 600.7594936708861\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -0.7972166538238525\n",
      "==========================================\n",
      "Epoch:  396 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 604.5454545454545\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.672915518283844\n",
      "==========================================\n",
      "Epoch:  397 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 2480.0\n",
      "Mean Reward of that batch 1240.0\n",
      "Average Reward of all training: 606.1460957178841\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -0.35449162125587463\n",
      "==========================================\n",
      "Epoch:  398 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 609.8994974874372\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.6904246807098389\n",
      "==========================================\n",
      "Epoch:  399 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 613.6340852130326\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -0.06779805570840836\n",
      "==========================================\n",
      "Epoch:  400 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 1660.0\n",
      "Mean Reward of that batch 1660.0\n",
      "Average Reward of all training: 616.25\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.1386878788471222\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  401 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 1148.0\n",
      "Mean Reward of that batch 1148.0\n",
      "Average Reward of all training: 617.576059850374\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.9703139066696167\n",
      "==========================================\n",
      "Epoch:  402 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 621.2636815920398\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -0.6343308687210083\n",
      "==========================================\n",
      "Epoch:  403 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 1628.0\n",
      "Mean Reward of that batch 1628.0\n",
      "Average Reward of all training: 623.7617866004963\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -1.4523301124572754\n",
      "==========================================\n",
      "Epoch:  404 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 627.4158415841584\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.9289736747741699\n",
      "==========================================\n",
      "Epoch:  405 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 631.0518518518519\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.5469210147857666\n",
      "==========================================\n",
      "Epoch:  406 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 634.6699507389162\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -0.31856194138526917\n",
      "==========================================\n",
      "Epoch:  407 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1624.0\n",
      "Mean Reward of that batch 812.0\n",
      "Average Reward of all training: 635.1056511056511\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -0.831666111946106\n",
      "==========================================\n",
      "Epoch:  408 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 2544.0\n",
      "Mean Reward of that batch 1272.0\n",
      "Average Reward of all training: 636.6666666666666\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 1.1181925535202026\n",
      "==========================================\n",
      "Epoch:  409 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 640.2444987775061\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.10663190484046936\n",
      "==========================================\n",
      "Epoch:  410 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 643.8048780487804\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -0.005038316827267408\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  411 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 956.0\n",
      "Mean Reward of that batch 956.0\n",
      "Average Reward of all training: 644.5644768856448\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.1532886028289795\n",
      "==========================================\n",
      "Epoch:  412 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 1052.0\n",
      "Mean Reward of that batch 1052.0\n",
      "Average Reward of all training: 645.5533980582525\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -0.14896856248378754\n",
      "==========================================\n",
      "Epoch:  413 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 649.0750605326876\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.815304696559906\n",
      "==========================================\n",
      "Epoch:  414 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 652.5797101449275\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.39498934149742126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Epoch:  415 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 656.0674698795181\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -0.1991281509399414\n",
      "==========================================\n",
      "Epoch:  416 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1176.0\n",
      "Mean Reward of that batch 588.0\n",
      "Average Reward of all training: 655.9038461538462\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -0.30927813053131104\n",
      "==========================================\n",
      "Epoch:  417 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1048.0\n",
      "Mean Reward of that batch 524.0\n",
      "Average Reward of all training: 655.5875299760191\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.12828890979290009\n",
      "==========================================\n",
      "Epoch:  418 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1112.0\n",
      "Mean Reward of that batch 556.0\n",
      "Average Reward of all training: 655.3492822966507\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.8644256591796875\n",
      "==========================================\n",
      "Epoch:  419 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1080.0\n",
      "Mean Reward of that batch 540.0\n",
      "Average Reward of all training: 655.073985680191\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.08199572563171387\n",
      "==========================================\n",
      "Epoch:  420 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1076.0\n",
      "Mean Reward of that batch 358.6666666666667\n",
      "Average Reward of all training: 654.3682539682541\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.06416546553373337\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  421 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 2384.0\n",
      "Mean Reward of that batch 1192.0\n",
      "Average Reward of all training: 655.6452889944577\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.42602047324180603\n",
      "==========================================\n",
      "Epoch:  422 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1016.0\n",
      "Mean Reward of that batch 508.0\n",
      "Average Reward of all training: 655.2954186413903\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.204884335398674\n",
      "==========================================\n",
      "Epoch:  423 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 980.0\n",
      "Mean Reward of that batch 326.6666666666667\n",
      "Average Reward of all training: 654.5185185185186\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.18851739168167114\n",
      "==========================================\n",
      "Epoch:  424 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1144.0\n",
      "Mean Reward of that batch 572.0\n",
      "Average Reward of all training: 654.3238993710692\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.141847625374794\n",
      "==========================================\n",
      "Epoch:  425 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 920.0\n",
      "Mean Reward of that batch 460.0\n",
      "Average Reward of all training: 653.8666666666667\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -0.1688382476568222\n",
      "==========================================\n",
      "Epoch:  426 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1528.0\n",
      "Mean Reward of that batch 764.0\n",
      "Average Reward of all training: 654.1251956181533\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.01926962286233902\n",
      "==========================================\n",
      "Epoch:  427 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1624.0\n",
      "Mean Reward of that batch 812.0\n",
      "Average Reward of all training: 654.4949258391881\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -1.2309317588806152\n",
      "==========================================\n",
      "Epoch:  428 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 657.8722741433021\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.8426879644393921\n",
      "==========================================\n",
      "Epoch:  429 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1080.0\n",
      "Mean Reward of that batch 540.0\n",
      "Average Reward of all training: 657.5975135975135\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.5793298482894897\n",
      "==========================================\n",
      "Epoch:  430 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1012.0\n",
      "Mean Reward of that batch 337.3333333333333\n",
      "Average Reward of all training: 656.8527131782945\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.37573006749153137\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  431 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1016.0\n",
      "Mean Reward of that batch 508.0\n",
      "Average Reward of all training: 656.5073472544469\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.341717004776001\n",
      "==========================================\n",
      "Epoch:  432 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 824.0\n",
      "Mean Reward of that batch 412.0\n",
      "Average Reward of all training: 655.9413580246915\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -0.21643607318401337\n",
      "==========================================\n",
      "Epoch:  433 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1300.0\n",
      "Mean Reward of that batch 433.3333333333333\n",
      "Average Reward of all training: 655.4272517321016\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -0.15172994136810303\n",
      "==========================================\n",
      "Epoch:  434 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1176.0\n",
      "Mean Reward of that batch 588.0\n",
      "Average Reward of all training: 655.2718894009216\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.058059707283973694\n",
      "==========================================\n",
      "Epoch:  435 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1112.0\n",
      "Mean Reward of that batch 556.0\n",
      "Average Reward of all training: 655.0436781609195\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -0.2603318393230438\n",
      "==========================================\n",
      "Epoch:  436 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1176.0\n",
      "Mean Reward of that batch 588.0\n",
      "Average Reward of all training: 654.8899082568807\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.07266508787870407\n",
      "==========================================\n",
      "Epoch:  437 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 1468.0\n",
      "Mean Reward of that batch 1468.0\n",
      "Average Reward of all training: 656.7505720823799\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.10902496427297592\n",
      "==========================================\n",
      "Epoch:  438 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1336.0\n",
      "Mean Reward of that batch 668.0\n",
      "Average Reward of all training: 656.7762557077625\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -0.049620121717453\n",
      "==========================================\n",
      "Epoch:  439 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 856.0\n",
      "Mean Reward of that batch 428.0\n",
      "Average Reward of all training: 656.255125284738\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -0.44107314944267273\n",
      "==========================================\n",
      "Epoch:  440 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1304.0\n",
      "Mean Reward of that batch 652.0\n",
      "Average Reward of all training: 656.2454545454545\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -0.07325110584497452\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  441 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1112.0\n",
      "Mean Reward of that batch 556.0\n",
      "Average Reward of all training: 656.0181405895692\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 1.67763090133667\n",
      "==========================================\n",
      "Epoch:  442 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 2480.0\n",
      "Mean Reward of that batch 1240.0\n",
      "Average Reward of all training: 657.3393665158371\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.25166788697242737\n",
      "==========================================\n",
      "Epoch:  443 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 952.0\n",
      "Mean Reward of that batch 476.0\n",
      "Average Reward of all training: 656.9300225733634\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.06402397155761719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Epoch:  444 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 952.0\n",
      "Mean Reward of that batch 476.0\n",
      "Average Reward of all training: 656.5225225225225\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -0.4506241977214813\n",
      "==========================================\n",
      "Epoch:  445 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 659.7662921348315\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.46574607491493225\n",
      "==========================================\n",
      "Epoch:  446 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1076.0\n",
      "Mean Reward of that batch 358.6666666666667\n",
      "Average Reward of all training: 659.0911808669657\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.5621063113212585\n",
      "==========================================\n",
      "Epoch:  447 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 2480.0\n",
      "Mean Reward of that batch 1240.0\n",
      "Average Reward of all training: 660.3907531692766\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.2893485724925995\n",
      "==========================================\n",
      "Epoch:  448 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1108.0\n",
      "Mean Reward of that batch 369.3333333333333\n",
      "Average Reward of all training: 659.7410714285714\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -0.0681607648730278\n",
      "==========================================\n",
      "Epoch:  449 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 662.9487750556793\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -0.5824145078659058\n",
      "==========================================\n",
      "Epoch:  450 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 956.0\n",
      "Mean Reward of that batch 956.0\n",
      "Average Reward of all training: 663.6\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.20519137382507324\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  451 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 2796.0\n",
      "Mean Reward of that batch 932.0\n",
      "Average Reward of all training: 664.1951219512196\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.15406891703605652\n",
      "==========================================\n",
      "Epoch:  452 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 667.3716814159292\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.44093796610832214\n",
      "==========================================\n",
      "Epoch:  453 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1012.0\n",
      "Mean Reward of that batch 337.3333333333333\n",
      "Average Reward of all training: 666.6431199411332\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -1.1028028726577759\n",
      "==========================================\n",
      "Epoch:  454 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 669.8002936857563\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.47394829988479614\n",
      "==========================================\n",
      "Epoch:  455 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 672.9435897435899\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 1.1050819158554077\n",
      "==========================================\n",
      "Epoch:  456 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 676.0730994152048\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.006704842671751976\n",
      "==========================================\n",
      "Epoch:  457 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 1724.0\n",
      "Mean Reward of that batch 1724.0\n",
      "Average Reward of all training: 678.366156090445\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -5.545647144317627\n",
      "==========================================\n",
      "Epoch:  458 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 681.4701601164484\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.0814681276679039\n",
      "==========================================\n",
      "Epoch:  459 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 684.5606390704431\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.35720255970954895\n",
      "==========================================\n",
      "Epoch:  460 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 687.6376811594204\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 1.564353585243225\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  461 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 690.7013738250182\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.9191281795501709\n",
      "==========================================\n",
      "Epoch:  462 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 2480.0\n",
      "Mean Reward of that batch 1240.0\n",
      "Average Reward of all training: 691.8903318903319\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -2.974869728088379\n",
      "==========================================\n",
      "Epoch:  463 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 694.9316054715623\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 0.9150894284248352\n",
      "==========================================\n",
      "Epoch:  464 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 697.9597701149426\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -0.22750639915466309\n",
      "==========================================\n",
      "Epoch:  465 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 700.9749103942653\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -12.095503807067871\n",
      "==========================================\n",
      "Epoch:  466 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 703.9771101573677\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -32.879310607910156\n",
      "==========================================\n",
      "Epoch:  467 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 1148.0\n",
      "Mean Reward of that batch 1148.0\n",
      "Average Reward of all training: 704.9279086366881\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -51.15296173095703\n",
      "==========================================\n",
      "Epoch:  468 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1300.0\n",
      "Mean Reward of that batch 433.3333333333333\n",
      "Average Reward of all training: 704.3475783475783\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -21.320600509643555\n",
      "==========================================\n",
      "Epoch:  469 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 707.323383084577\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 14.911664009094238\n",
      "==========================================\n",
      "Epoch:  470 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 2480.0\n",
      "Mean Reward of that batch 1240.0\n",
      "Average Reward of all training: 708.4567375886523\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -45.68087387084961\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  471 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 711.4111818825194\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 7.955699443817139\n",
      "==========================================\n",
      "Epoch:  472 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 714.3531073446327\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 18.094106674194336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Epoch:  473 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1304.0\n",
      "Mean Reward of that batch 652.0\n",
      "Average Reward of all training: 714.2212825933756\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -11.225401878356934\n",
      "==========================================\n",
      "Epoch:  474 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 2100.0\n",
      "Mean Reward of that batch 2100.0\n",
      "Average Reward of all training: 717.1448663853727\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 14.166805267333984\n",
      "==========================================\n",
      "Epoch:  475 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1112.0\n",
      "Mean Reward of that batch 556.0\n",
      "Average Reward of all training: 716.8056140350876\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -169.5421600341797\n",
      "==========================================\n",
      "Epoch:  476 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 1436.0\n",
      "Mean Reward of that batch 1436.0\n",
      "Average Reward of all training: 718.3165266106441\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -93.10000610351562\n",
      "==========================================\n",
      "Epoch:  477 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 2448.0\n",
      "Mean Reward of that batch 1224.0\n",
      "Average Reward of all training: 719.3766596785464\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -12.431292533874512\n",
      "==========================================\n",
      "Epoch:  478 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1300.0\n",
      "Mean Reward of that batch 433.3333333333333\n",
      "Average Reward of all training: 718.7782426778243\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 24.256591796875\n",
      "==========================================\n",
      "Epoch:  479 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1080.0\n",
      "Mean Reward of that batch 540.0\n",
      "Average Reward of all training: 718.4050104384133\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -213.29391479492188\n",
      "==========================================\n",
      "Epoch:  480 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1044.0\n",
      "Mean Reward of that batch 348.0\n",
      "Average Reward of all training: 717.6333333333333\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 3.7075514793395996\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  481 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 856.0\n",
      "Mean Reward of that batch 428.0\n",
      "Average Reward of all training: 717.031185031185\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -16.01093864440918\n",
      "==========================================\n",
      "Epoch:  482 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1492.0\n",
      "Mean Reward of that batch 497.3333333333333\n",
      "Average Reward of all training: 716.5753803596127\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -234.50184631347656\n",
      "==========================================\n",
      "Epoch:  483 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 2384.0\n",
      "Mean Reward of that batch 1192.0\n",
      "Average Reward of all training: 717.5596963423048\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 2.0568225383758545\n",
      "==========================================\n",
      "Epoch:  484 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1240.0\n",
      "Mean Reward of that batch 620.0\n",
      "Average Reward of all training: 717.3581267217629\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 24.42275047302246\n",
      "==========================================\n",
      "Epoch:  485 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1240.0\n",
      "Mean Reward of that batch 620.0\n",
      "Average Reward of all training: 717.157388316151\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -460.6434020996094\n",
      "==========================================\n",
      "Epoch:  486 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1336.0\n",
      "Mean Reward of that batch 668.0\n",
      "Average Reward of all training: 717.0562414266117\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -318.3074645996094\n",
      "==========================================\n",
      "Epoch:  487 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1240.0\n",
      "Mean Reward of that batch 620.0\n",
      "Average Reward of all training: 716.8569472963721\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -52.5317268371582\n",
      "==========================================\n",
      "Epoch:  488 / 500\n",
      "-----------\n",
      "Number of training episodes: 1\n",
      "Total reward: 1724.0\n",
      "Mean Reward of that batch 1724.0\n",
      "Average Reward of all training: 718.9207650273222\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 26.98534393310547\n",
      "==========================================\n",
      "Epoch:  489 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1684.0\n",
      "Mean Reward of that batch 561.3333333333334\n",
      "Average Reward of all training: 718.5985003408316\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -125.52093505859375\n",
      "==========================================\n",
      "Epoch:  490 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1016.0\n",
      "Mean Reward of that batch 508.0\n",
      "Average Reward of all training: 718.1687074829931\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 37.85546875\n",
      "Model saved\n",
      "==========================================\n",
      "Epoch:  491 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 984.0\n",
      "Mean Reward of that batch 492.0\n",
      "Average Reward of all training: 717.7080787508486\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 7.191271781921387\n",
      "==========================================\n",
      "Epoch:  492 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1240.0\n",
      "Mean Reward of that batch 620.0\n",
      "Average Reward of all training: 717.5094850948509\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 36.11251449584961\n",
      "==========================================\n",
      "Epoch:  493 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1140.0\n",
      "Mean Reward of that batch 380.0\n",
      "Average Reward of all training: 716.8248816768086\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -9.239424705505371\n",
      "==========================================\n",
      "Epoch:  494 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1300.0\n",
      "Mean Reward of that batch 433.3333333333333\n",
      "Average Reward of all training: 716.251012145749\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -3.428056240081787\n",
      "==========================================\n",
      "Epoch:  495 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1176.0\n",
      "Mean Reward of that batch 588.0\n",
      "Average Reward of all training: 715.9919191919192\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: -203.54617309570312\n",
      "==========================================\n",
      "Epoch:  496 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1332.0\n",
      "Mean Reward of that batch 444.0\n",
      "Average Reward of all training: 715.4435483870967\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 9.368308067321777\n",
      "==========================================\n",
      "Epoch:  497 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 824.0\n",
      "Mean Reward of that batch 412.0\n",
      "Average Reward of all training: 714.8329979879275\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 7.143482685089111\n",
      "==========================================\n",
      "Epoch:  498 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 852.0\n",
      "Mean Reward of that batch 284.0\n",
      "Average Reward of all training: 713.9678714859436\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 6.767094135284424\n",
      "==========================================\n",
      "Epoch:  499 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 1044.0\n",
      "Mean Reward of that batch 348.0\n",
      "Average Reward of all training: 713.2344689378756\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 8.986259460449219\n",
      "==========================================\n",
      "Epoch:  500 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 888.0\n",
      "Mean Reward of that batch 444.0\n",
      "Average Reward of all training: 712.6959999999999\n",
      "Max reward for a batch so far: 2992.0\n",
      "Training Loss: 10.273327827453613\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "# Keep track of all rewards total for each batch\n",
    "allRewards = []\n",
    "\n",
    "total_rewards = 0\n",
    "maximumRewardRecorded = 0\n",
    "mean_reward_total = []\n",
    "epoch = 1\n",
    "average_reward = []\n",
    "\n",
    "# Saver\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "if training:\n",
    "    # Load the model\n",
    "    #saver.restore(sess, \"./models/model.ckpt\")\n",
    "\n",
    "    while epoch < num_epochs + 1:\n",
    "        # Gather training data\n",
    "        states_mb, actions_mb, rewards_of_batch, discounted_rewards_mb, nb_episodes_mb = make_batch(batch_size, stacked_frames)\n",
    "\n",
    "        ### These part is used for analytics\n",
    "        # Calculate the total reward ot the batch\n",
    "        total_reward_of_that_batch = np.sum(rewards_of_batch)\n",
    "        allRewards.append(total_reward_of_that_batch)\n",
    "\n",
    "        # Calculate the mean reward of the batch\n",
    "        # Total rewards of batch / nb episodes in that batch\n",
    "        mean_reward_of_that_batch = np.divide(total_reward_of_that_batch, nb_episodes_mb)\n",
    "        mean_reward_total.append(mean_reward_of_that_batch)\n",
    "\n",
    "        # Calculate the average reward of all training\n",
    "        # mean_reward_of_that_batch / epoch\n",
    "        average_reward_of_all_training = np.divide(np.sum(mean_reward_total), epoch)\n",
    "\n",
    "        # Calculate maximum reward recorded \n",
    "        maximumRewardRecorded = np.amax(allRewards)\n",
    "\n",
    "        print(\"==========================================\")\n",
    "        print(\"Epoch: \", epoch, \"/\", num_epochs)\n",
    "        print(\"-----------\")\n",
    "        print(\"Number of training episodes: {}\".format(nb_episodes_mb))\n",
    "        print(\"Total reward: {}\".format(total_reward_of_that_batch, nb_episodes_mb))\n",
    "        print(\"Mean Reward of that batch {}\".format(mean_reward_of_that_batch))\n",
    "        print(\"Average Reward of all training: {}\".format(average_reward_of_all_training))\n",
    "        print(\"Max reward for a batch so far: {}\".format(maximumRewardRecorded))\n",
    "\n",
    "        # Feedforward, gradient and backpropagation\n",
    "        loss_, _ = sess.run([PGNetwork.loss, PGNetwork.train_opt], feed_dict={PGNetwork.inputs_: states_mb.reshape((len(states_mb), 84,84,4)),\n",
    "                                                            PGNetwork.actions: actions_mb,\n",
    "                                                                     PGNetwork.discounted_episode_rewards_: discounted_rewards_mb \n",
    "                                                                    })\n",
    "\n",
    "        print(\"Training Loss: {}\".format(loss_))\n",
    "\n",
    "        # Write TF Summaries\n",
    "        summary = sess.run(write_op, feed_dict={PGNetwork.inputs_: states_mb.reshape((len(states_mb), 84,84,4)),\n",
    "                                                            PGNetwork.actions: actions_mb,\n",
    "                                                                     PGNetwork.discounted_episode_rewards_: discounted_rewards_mb,\n",
    "                                                                    PGNetwork.mean_reward_: mean_reward_of_that_batch\n",
    "                                                                    })\n",
    "\n",
    "        #summary = sess.run(write_op, feed_dict={x: s_.reshape(len(s_),84,84,1), y:a_, d_r: d_r_, r: r_, n: n_})\n",
    "        writer.add_summary(summary, epoch)\n",
    "        writer.flush()\n",
    "\n",
    "        # Save Model\n",
    "        if epoch % 10 == 0:\n",
    "            saver.save(sess, \"./models/model.ckpt\")\n",
    "            print(\"Model saved\")\n",
    "        epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Watch our Agent play üëÄ\n",
    "Now that we trained our agent, we can test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T08:40:29.505166Z",
     "start_time": "2021-03-18T08:40:03.558934Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/model.ckpt\n",
      "Score for episode  0  : 476.0\n",
      "Score for episode  1  : 284.0\n",
      "Score for episode  2  : 476.0\n",
      "Score for episode  3  : 2100.0\n",
      "Score for episode  4  : 284.0\n",
      "Score for episode  5  : 444.0\n",
      "Score for episode  6  : 284.0\n",
      "Score for episode  7  : 796.0\n",
      "Score for episode  8  : 284.0\n",
      "Score for episode  9  : 380.0\n"
     ]
    }
   ],
   "source": [
    "# Saver\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    game = DoomGame()\n",
    "\n",
    "    # Load the correct configuration \n",
    "    game.load_config(\"health_gathering.cfg\")\n",
    "    \n",
    "    # Load the correct scenario (in our case basic scenario)\n",
    "    game.set_doom_scenario_path(\"health_gathering.wad\")\n",
    "    \n",
    "    # Load the model\n",
    "    saver.restore(sess, \"./models/model.ckpt\")\n",
    "    game.init()\n",
    "    \n",
    "    for i in range(10):\n",
    "        \n",
    "        # Launch a new episode\n",
    "        game.new_episode()\n",
    "\n",
    "        # Get a new state\n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "\n",
    "        while not game.is_episode_finished():\n",
    "        \n",
    "            # Run State Through Policy & Calculate Action\n",
    "            action_probability_distribution = sess.run(PGNetwork.action_distribution, \n",
    "                                                       feed_dict={PGNetwork.inputs_: state.reshape(1, *state_size)})\n",
    "\n",
    "            # REMEMBER THAT WE ARE IN A STOCHASTIC POLICY SO WE DON'T ALWAYS TAKE THE ACTION WITH THE HIGHEST PROBABILITY\n",
    "            # (For instance if the action with the best probability for state S is a1 with 70% chances, there is\n",
    "            #30% chance that we take action a2)\n",
    "            action = np.random.choice(range(action_probability_distribution.shape[1]), \n",
    "                                      p=action_probability_distribution.ravel())  # select action w.r.t the actions prob\n",
    "            action = possible_actions[action]\n",
    "\n",
    "            # Perform action\n",
    "            reward = game.make_action(action)\n",
    "            done = game.is_episode_finished()\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            else:\n",
    "                # If not done, the next_state become the current state\n",
    "                next_state = game.get_state().screen_buffer\n",
    "                next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                state = next_state\n",
    "        \n",
    "\n",
    "        print(\"Score for episode \", i, \" :\", game.get_total_reward())\n",
    "    game.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:drl_simonini] *",
   "language": "python",
   "name": "conda-env-drl_simonini-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
